{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac585ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "XGBOOST HYPERPARAMETER OPTIMIZATION (Native API)\n",
      "================================================================================\n",
      "⚠ No GPU detected, using CPU\n",
      "\n",
      "================================================================================\n",
      "LOADING PREPROCESSED DATA\n",
      "================================================================================\n",
      "✓ Train shape: (100, 8)\n",
      "✓ Target shape: (100,)\n",
      "✓ Test shape: (10, 8)\n",
      "✓ Device: cpu\n",
      "\n",
      "============================================================\n",
      "OPTIMIZATION SETTINGS\n",
      "============================================================\n",
      "Mode: FAST\n",
      "Trials: 1\n",
      "Folds: 5\n",
      "\n",
      "================================================================================\n",
      "STARTING BAYESIAN OPTIMIZATION\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8c3a5b66694683b85de21ca9045322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS\n",
      "================================================================================\n",
      "Best RMSE: 10.087971\n",
      "Optimization time: 1.3s\n",
      "\n",
      "Best parameters:\n",
      "  alpha               : 1.7835460015641595\n",
      "  colsample_bytree    : 0.7482791797686166\n",
      "  gamma               : 0.6458941130666561\n",
      "  lambda              : 0.875174422525385\n",
      "  learning_rate       : 0.03538453590906552\n",
      "  max_depth           : 9\n",
      "  min_child_weight    : 7\n",
      "  num_boost_round     : 2928\n",
      "  subsample           : 0.7907091140489139\n",
      "\n",
      "✓ Parameters saved to: xgboost_params.pkl\n",
      "✓ num_boost_round: 2928\n",
      "✓ History saved to: xgboost_history.csv\n",
      "✓ Study saved to: xgboost_study.pkl\n",
      "✓ Summary saved to: xgboost_summary.pkl\n",
      "\n",
      "================================================================================\n",
      "TRAINING FINAL MODEL ON FULL DATASET\n",
      "================================================================================\n",
      "Training final model with 2928 rounds...\n",
      "\n",
      "✓ Final model saved to: xgboost_final_model.json\n",
      "✓ Final model saved to: xgboost_final_model.pkl\n",
      "\n",
      "================================================================================\n",
      "GENERATING PREDICTIONS\n",
      "================================================================================\n",
      "✓ Submission file saved to: submission.csv\n",
      "\n",
      "Submission statistics:\n",
      "  Min prediction: 51.43\n",
      "  Max prediction: 100.00\n",
      "  Mean prediction: 74.17\n",
      "  Median prediction: 72.97\n",
      "  Std prediction: 14.40\n",
      "  Shape: (10, 2)\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "Top 10 most important features:\n",
      "                feature  importance\n",
      "        formula_x_study  241.625900\n",
      "                formula  131.227341\n",
      "       formula_residual   92.743027\n",
      "     study_from_optimal   51.103954\n",
      "            study_hours   50.965996\n",
      "            sleep_hours   13.540645\n",
      "       class_attendance   11.666246\n",
      "attendance_from_optimal   10.257032\n",
      "\n",
      "✓ Feature importance saved to: xgboost_feature_importance.csv\n",
      "\n",
      "================================================================================\n",
      "XGBOOST OPTIMIZATION COMPLETE!\n",
      "================================================================================\n",
      "✓ Best CV RMSE: 10.087971\n",
      "✓ Final model trained on 100 samples\n",
      "✓ Model has 2928 trees\n",
      "✓ Predictions generated for 10 test samples\n",
      "✓ All results saved\n",
      "\n",
      "Files created:\n",
      "  • xgboost_params.pkl\n",
      "  • xgboost_num_boost_round.pkl\n",
      "  • xgboost_history.csv\n",
      "  • xgboost_study.pkl\n",
      "  • xgboost_summary.pkl\n",
      "  • xgboost_final_model.json\n",
      "  • xgboost_final_model.pkl\n",
      "  • xgboost_feature_importance.csv\n",
      "  • submission.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =============================================================================\n",
    "# GPU CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"XGBOOST HYPERPARAMETER OPTIMIZATION (Native API)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    USE_GPU = True\n",
    "    DEVICE = 'cuda:0'\n",
    "else:\n",
    "    print(\"⚠ No GPU detected, using CPU\")\n",
    "    USE_GPU = False\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = joblib.load('../../data/preprocessed/X_train.pkl')\n",
    "y_train = joblib.load('../../data/preprocessed/y_train.pkl')\n",
    "X_test = joblib.load('../../data/preprocessed/X_test.pkl')\n",
    "metadata = joblib.load('../../data/preprocessed/metadata.pkl')\n",
    "\n",
    "print(f\"✓ Train shape: {X_train.shape}\")\n",
    "print(f\"✓ Target shape: {y_train.shape}\")\n",
    "print(f\"✓ Test shape: {X_test.shape}\")\n",
    "print(f\"✓ Device: {DEVICE}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZATION SETTINGS\n",
    "# =============================================================================\n",
    "FAST_MODE = True  # Set to False for more thorough search\n",
    "N_TRIALS = 1 if FAST_MODE else 400\n",
    "N_FOLDS = 5\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OPTIMIZATION SETTINGS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {'FAST' if FAST_MODE else 'THOROUGH'}\")\n",
    "print(f\"Trials: {N_TRIALS}\")\n",
    "print(f\"Folds: {N_FOLDS}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "def objective_xgb(trial, X, y):\n",
    "    \"\"\"Objective function for XGBoost optimization (Native API)\"\"\"\n",
    "    \n",
    "    # Base parameters (always included)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'tree_method': 'hist',\n",
    "        'device': DEVICE,\n",
    "        'predictor': 'gpu_predictor' if USE_GPU else 'cpu_predictor',\n",
    "        'seed': RANDOM_SEED,\n",
    "    }\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    params.update({\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0, 2.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0, 2.0),\n",
    "    })\n",
    "    \n",
    "    # Remove None values\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "    \n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 1000, 3000)\n",
    "\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        \n",
    "        # Use numpy indexing for y (no .iloc)\n",
    "        if isinstance(y, pd.Series):\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        else:  # numpy array\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create DMatrix (optimized data structure for XGBoost)\n",
    "        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            evals=[(dval, 'eval')],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Predict and clip to valid range\n",
    "        preds = np.clip(model.predict(dval), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# =============================================================================\n",
    "# RUN OPTIMIZATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    lambda trial: objective_xgb(trial, X_train, y_train),\n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "optimization_time = time.time() - start_time\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best RMSE: {study.best_value:.6f}\")\n",
    "print(f\"Optimization time: {optimization_time:.1f}s\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in sorted(study.best_params.items()):\n",
    "    print(f\"  {param:20s}: {value}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "# Prepare final parameters\n",
    "best_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist',\n",
    "    'device': DEVICE,\n",
    "    'predictor': 'gpu_predictor' if USE_GPU else 'cpu_predictor',\n",
    "    'seed': RANDOM_SEED,\n",
    "}\n",
    "best_params.update(study.best_params)\n",
    "\n",
    "# Extract num_boost_round separately (it's not a model param)\n",
    "num_boost_round = best_params.pop('num_boost_round')\n",
    "\n",
    "# Remove None values\n",
    "best_params = {k: v for k, v in best_params.items() if v is not None}\n",
    "\n",
    "joblib.dump(best_params, 'xgboost_params.pkl')\n",
    "joblib.dump(num_boost_round, 'xgboost_num_boost_round.pkl')\n",
    "print(\"\\n✓ Parameters saved to: xgboost_params.pkl\")\n",
    "print(f\"✓ num_boost_round: {num_boost_round}\")\n",
    "\n",
    "# Save optimization history\n",
    "history = pd.DataFrame({\n",
    "    'trial': [t.number for t in study.trials],\n",
    "    'value': [t.value for t in study.trials],\n",
    "    'params': [str(t.params) for t in study.trials]\n",
    "})\n",
    "history.to_csv('xgboost_history.csv', index=False)\n",
    "print(\"✓ History saved to: xgboost_history.csv\")\n",
    "\n",
    "# Save study object\n",
    "joblib.dump(study, 'xgboost_study.pkl')\n",
    "print(\"✓ Study saved to: xgboost_study.pkl\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'model': 'XGBoost (Native API)',\n",
    "    'best_rmse': study.best_value,\n",
    "    'n_trials': N_TRIALS,\n",
    "    'n_folds': N_FOLDS,\n",
    "    'optimization_time': optimization_time,\n",
    "    'device': DEVICE,\n",
    "    'use_gpu': USE_GPU,\n",
    "    'best_params': best_params,\n",
    "    'num_boost_round': num_boost_round\n",
    "}\n",
    "joblib.dump(summary, 'xgboost_summary.pkl')\n",
    "print(\"✓ Summary saved to: xgboost_summary.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN FINAL MODEL ON FULL TRAINING DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL ON FULL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create DMatrix for full training data\n",
    "dtrain_full = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "print(f\"Training final model with {num_boost_round} rounds...\")\n",
    "final_model = xgb.train(\n",
    "    best_params,\n",
    "    dtrain_full,\n",
    "    num_boost_round=num_boost_round,\n",
    "    verbose_eval=100  # Show progress every 100 rounds\n",
    ")\n",
    "\n",
    "# Save final model in multiple formats\n",
    "final_model.save_model('xgboost_final_model.json')\n",
    "joblib.dump(final_model, 'xgboost_final_model.pkl')\n",
    "print(\"\\n✓ Final model saved to: xgboost_final_model.json\")\n",
    "print(\"✓ Final model saved to: xgboost_final_model.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE PREDICTIONS AND SUBMISSION FILE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create DMatrix for test data\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = final_model.predict(dtest)\n",
    "\n",
    "# Clip predictions to valid range [0, 100]\n",
    "test_predictions = np.clip(test_predictions, 0, 100)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(test_predictions)),\n",
    "    'exam_score': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✓ Submission file saved to: submission.csv\")\n",
    "\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"  Min prediction: {test_predictions.min():.2f}\")\n",
    "print(f\"  Max prediction: {test_predictions.max():.2f}\")\n",
    "print(f\"  Mean prediction: {test_predictions.mean():.2f}\")\n",
    "print(f\"  Median prediction: {np.median(test_predictions):.2f}\")\n",
    "print(f\"  Std prediction: {test_predictions.std():.2f}\")\n",
    "print(f\"  Shape: {submission.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance (using 'gain' as importance type)\n",
    "importance_dict = final_model.get_score(importance_type='gain')\n",
    "\n",
    "if importance_dict:\n",
    "    # Create DataFrame\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': list(importance_dict.keys()),\n",
    "        'importance': list(importance_dict.values())\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv('xgboost_feature_importance.csv', index=False)\n",
    "    print(\"\\n✓ Feature importance saved to: xgboost_feature_importance.csv\")\n",
    "else:\n",
    "    print(\"⚠ No feature importance available (model might have no splits)\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"XGBOOST OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Best CV RMSE: {study.best_value:.6f}\")\n",
    "print(f\"✓ Final model trained on {len(X_train)} samples\")\n",
    "print(f\"✓ Model has {num_boost_round} trees\")\n",
    "print(f\"✓ Predictions generated for {len(X_test)} test samples\")\n",
    "print(f\"✓ All results saved\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  • xgboost_params.pkl\")\n",
    "print(\"  • xgboost_num_boost_round.pkl\")\n",
    "print(\"  • xgboost_history.csv\")\n",
    "print(\"  • xgboost_study.pkl\")\n",
    "print(\"  • xgboost_summary.pkl\")\n",
    "print(\"  • xgboost_final_model.json\")\n",
    "print(\"  • xgboost_final_model.pkl\")\n",
    "print(\"  • xgboost_feature_importance.csv\")\n",
    "print(\"  • submission.csv\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7f712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "students-scores",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
