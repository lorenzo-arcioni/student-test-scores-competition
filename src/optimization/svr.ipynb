{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SVR HYPERPARAMETER OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "LOADING PREPROCESSED DATA\n",
      "================================================================================\n",
      "✓ Train shape: (630000, 9)\n",
      "✓ Target shape: (630000,)\n",
      "\n",
      "============================================================\n",
      "OPTIMIZATION SETTINGS\n",
      "============================================================\n",
      "Mode: FAST\n",
      "Trials: 5\n",
      "Folds: 5\n",
      "\n",
      "================================================================================\n",
      "STARTING BAYESIAN OPTIMIZATION\n",
      "================================================================================\n",
      "Note: Using data sampling for speed (SVR is computationally intensive)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fd7dcdebed47e88dd9aabde061980c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"SVR HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = joblib.load('../../data/preprocessed/X_train.pkl')\n",
    "y_train = joblib.load('../../data/preprocessed/y_train.pkl')\n",
    "X_test = joblib.load('../../data/preprocessed/X_test.pkl')\n",
    "metadata = joblib.load('../../data/preprocessed/metadata.pkl')\n",
    "\n",
    "print(f\"✓ Train shape: {X_train.shape}\")\n",
    "print(f\"✓ Target shape: {y_train.shape}\")\n",
    "print(f\"✓ Test shape: {X_test.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZATION SETTINGS\n",
    "# =============================================================================\n",
    "FAST_MODE = False  # Set to False for more thorough search\n",
    "N_TRIALS = 15 if FAST_MODE else 100\n",
    "N_FOLDS = 5\n",
    "N_JOBS_OPTUNA = 2  # SVR doesn't parallelize well, use fewer jobs\n",
    "SAMPLE_SIZE = 20000  # Subsample for speed (SVR is O(n²) to O(n³))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OPTIMIZATION SETTINGS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {'FAST' if FAST_MODE else 'THOROUGH'}\")\n",
    "print(f\"Trials: {N_TRIALS}\")\n",
    "print(f\"Folds: {N_FOLDS}\")\n",
    "print(f\"Parallel jobs: {N_JOBS_OPTUNA}\")\n",
    "print(f\"Sample size per fold: {SAMPLE_SIZE}\")\n",
    "print(f\"⚠️  Note: Using subsampling due to SVR's computational complexity\")\n",
    "\n",
    "# =============================================================================\n",
    "# OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "def objective_svr(trial, X, y):\n",
    "    \"\"\"Objective function for SVR optimization\"\"\"\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    params = {\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly']),\n",
    "        'C': trial.suggest_float('C', 0.1, 100, log=True),\n",
    "        'epsilon': trial.suggest_float('epsilon', 0.01, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    # Kernel-specific parameters\n",
    "    if params['kernel'] == 'rbf':\n",
    "        params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    elif params['kernel'] == 'poly':\n",
    "        params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "        params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "        params['coef0'] = trial.suggest_float('coef0', 0.0, 10.0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        \n",
    "        # Handle both pandas Series and numpy arrays\n",
    "        if isinstance(y, pd.Series):\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        else:\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Scale the data (CRITICAL for SVR)\n",
    "        X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # Subsample for speed (SVR is O(n²) to O(n³))\n",
    "        if len(X_tr) > SAMPLE_SIZE:\n",
    "            sample_idx = np.random.choice(len(X_tr), SAMPLE_SIZE, replace=False)\n",
    "            X_tr_scaled = X_tr_scaled[sample_idx]\n",
    "            y_tr = y_tr.iloc[sample_idx] if isinstance(y_tr, pd.Series) else y_tr[sample_idx]\n",
    "\n",
    "        # Train SVR\n",
    "        model = SVR(**params)\n",
    "        \n",
    "        try:\n",
    "            model.fit(X_tr_scaled, y_tr)\n",
    "            preds = np.clip(model.predict(X_val_scaled), 0, 100)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "            scores.append(rmse)\n",
    "        except Exception as e:\n",
    "            # If model fails, return high penalty\n",
    "            return 1e6\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# =============================================================================\n",
    "# RUN OPTIMIZATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    lambda trial: objective_svr(trial, X_train, y_train),\n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True,\n",
    "    n_jobs=N_JOBS_OPTUNA\n",
    ")\n",
    "\n",
    "optimization_time = time.time() - start_time\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best RMSE: {study.best_value:.6f}\")\n",
    "print(f\"Optimization time: {optimization_time:.1f}s\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in sorted(study.best_params.items()):\n",
    "    print(f\"  {param:20s}: {value}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "# Save best parameters\n",
    "best_params = study.best_params.copy()\n",
    "\n",
    "joblib.dump(best_params, 'svr_params.pkl')\n",
    "print(\"\\n✓ Parameters saved to: svr_params.pkl\")\n",
    "\n",
    "# Save optimization history\n",
    "history = pd.DataFrame({\n",
    "    'trial': [t.number for t in study.trials],\n",
    "    'value': [t.value for t in study.trials],\n",
    "    'params': [str(t.params) for t in study.trials]\n",
    "})\n",
    "history.to_csv('svr_history.csv', index=False)\n",
    "print(\"✓ History saved to: svr_history.csv\")\n",
    "\n",
    "# Save study object\n",
    "joblib.dump(study, 'svr_study.pkl')\n",
    "print(\"✓ Study saved to: svr_study.pkl\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'model': 'SVR',\n",
    "    'best_rmse': study.best_value,\n",
    "    'n_trials': N_TRIALS,\n",
    "    'n_folds': N_FOLDS,\n",
    "    'optimization_time': optimization_time,\n",
    "    'sample_size': SAMPLE_SIZE,\n",
    "    'best_params': best_params\n",
    "}\n",
    "joblib.dump(summary, 'svr_summary.pkl')\n",
    "print(\"✓ Summary saved to: svr_summary.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN FINAL MODEL ON FULL TRAINING DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL ON FULL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scale the entire training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Optionally subsample for final training (if dataset is very large)\n",
    "use_full_data = len(X_train) <= 50000\n",
    "if use_full_data:\n",
    "    print(f\"Training on full dataset ({len(X_train)} samples)...\")\n",
    "    X_final = X_train_scaled\n",
    "    y_final = y_train\n",
    "else:\n",
    "    final_sample_size = 50000\n",
    "    print(f\"⚠️  Dataset too large for SVR, subsampling to {final_sample_size} samples...\")\n",
    "    sample_idx = np.random.choice(len(X_train), final_sample_size, replace=False)\n",
    "    X_final = X_train_scaled[sample_idx]\n",
    "    y_final = y_train.iloc[sample_idx] if isinstance(y_train, pd.Series) else y_train[sample_idx]\n",
    "\n",
    "final_model = SVR(**best_params)\n",
    "print(\"Training final model...\")\n",
    "final_model.fit(X_final, y_final)\n",
    "\n",
    "# Save final model and scaler\n",
    "joblib.dump(final_model, 'svr_final_model.pkl')\n",
    "joblib.dump(scaler, 'svr_scaler.pkl')\n",
    "print(\"\\n✓ Final model saved to: svr_final_model.pkl\")\n",
    "print(\"✓ Scaler saved to: svr_scaler.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE PREDICTIONS AND SUBMISSION FILE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scale test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = final_model.predict(X_test_scaled)\n",
    "\n",
    "# Clip predictions to valid range [0, 100]\n",
    "test_predictions = np.clip(test_predictions, 0, 100)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(X_train), len(test_predictions) + len(X_train)),\n",
    "    'test_score': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✓ Submission file saved to: submission.csv\")\n",
    "\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"  Min prediction: {test_predictions.min():.2f}\")\n",
    "print(f\"  Max prediction: {test_predictions.max():.2f}\")\n",
    "print(f\"  Mean prediction: {test_predictions.mean():.2f}\")\n",
    "print(f\"  Median prediction: {np.median(test_predictions):.2f}\")\n",
    "print(f\"  Std prediction: {test_predictions.std():.2f}\")\n",
    "print(f\"  Shape: {submission.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL STATISTICS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Kernel: {best_params['kernel']}\")\n",
    "print(f\"C (regularization): {best_params['C']:.4f}\")\n",
    "print(f\"Epsilon (tube width): {best_params['epsilon']:.4f}\")\n",
    "if 'gamma' in best_params:\n",
    "    print(f\"Gamma: {best_params['gamma']}\")\n",
    "if 'degree' in best_params:\n",
    "    print(f\"Polynomial degree: {best_params['degree']}\")\n",
    "print(f\"Number of support vectors: {len(final_model.support_vectors_)}\")\n",
    "print(f\"Support vector ratio: {len(final_model.support_vectors_) / len(X_final) * 100:.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Optimization history\n",
    "ax1 = axes[0, 0]\n",
    "history = pd.read_csv('svr_history.csv')\n",
    "ax1.plot(history['trial'], history['value'], alpha=0.6, marker='o', markersize=4)\n",
    "ax1.plot(history['trial'], history['value'].cummin(), \n",
    "         linewidth=2.5, label='Best score', color='red')\n",
    "ax1.set_xlabel('Trial', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('RMSE', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Optimization Progress', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Kernel comparison (if multiple kernels were tried)\n",
    "ax2 = axes[0, 1]\n",
    "kernel_counts = {}\n",
    "kernel_scores = {}\n",
    "for t in study.trials:\n",
    "    if 'kernel' in t.params:\n",
    "        kernel = t.params['kernel']\n",
    "        kernel_counts[kernel] = kernel_counts.get(kernel, 0) + 1\n",
    "        if kernel not in kernel_scores:\n",
    "            kernel_scores[kernel] = []\n",
    "        kernel_scores[kernel].append(t.value)\n",
    "\n",
    "if kernel_scores:\n",
    "    kernels = list(kernel_scores.keys())\n",
    "    avg_scores = [np.mean(kernel_scores[k]) for k in kernels]\n",
    "    \n",
    "    bars = ax2.bar(kernels, avg_scores, color=['skyblue', 'lightcoral', 'lightgreen'][:len(kernels)],\n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "    ax2.set_ylabel('Average RMSE', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Kernel Performance Comparison', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, val in zip(bars, avg_scores):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Single kernel used', ha='center', va='center',\n",
    "            transform=ax2.transAxes, fontsize=12)\n",
    "\n",
    "# 3. Support vectors visualization (sample)\n",
    "ax3 = axes[1, 0]\n",
    "sv_ratio = len(final_model.support_vectors_) / len(X_final) * 100\n",
    "labels = ['Support Vectors', 'Regular Points']\n",
    "sizes = [sv_ratio, 100 - sv_ratio]\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "explode = (0.1, 0)\n",
    "\n",
    "ax3.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "ax3.set_title(f'Support Vector Distribution\\n({len(final_model.support_vectors_)} SVs)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. Prediction distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(test_predictions, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax4.axvline(test_predictions.mean(), color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Mean: {test_predictions.mean():.2f}')\n",
    "ax4.axvline(np.median(test_predictions), color='green', linestyle='--',\n",
    "           linewidth=2, label=f'Median: {np.median(test_predictions):.2f}')\n",
    "ax4.set_xlabel('Predicted Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Test Predictions Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('svr_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Analysis plot saved to: svr_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SVR OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Best CV RMSE: {study.best_value:.6f}\")\n",
    "print(f\"✓ Final model trained on {len(X_final)} samples\")\n",
    "print(f\"✓ Number of support vectors: {len(final_model.support_vectors_)}\")\n",
    "print(f\"✓ Predictions generated for {len(X_test)} test samples\")\n",
    "print(f\"✓ All results saved\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  • svr_params.pkl\")\n",
    "print(\"  • svr_history.csv\")\n",
    "print(\"  • svr_study.pkl\")\n",
    "print(\"  • svr_summary.pkl\")\n",
    "print(\"  • svr_final_model.pkl\")\n",
    "print(\"  • svr_scaler.pkl\")\n",
    "print(\"  • svr_analysis.png\")\n",
    "print(\"  • submission.csv\")\n",
    "print(\"\\n⚠️  Important Notes:\")\n",
    "print(\"  - SVR requires scaled data (StandardScaler used)\")\n",
    "print(\"  - Subsampling was used due to computational complexity\")\n",
    "print(\"  - For large datasets, consider using LinearSVR or other models\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5486c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "students-scores",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
