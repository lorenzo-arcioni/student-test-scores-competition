{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b9cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LIGHTGBM HYPERPARAMETER OPTIMIZATION\n",
      "================================================================================\n",
      "⚠ No GPU detected, using CPU\n",
      "\n",
      "================================================================================\n",
      "LOADING PREPROCESSED DATA\n",
      "================================================================================\n",
      "✓ Train shape: (630000, 9)\n",
      "✓ Target shape: (630000,)\n",
      "✓ GPU available: False\n",
      "\n",
      "============================================================\n",
      "OPTIMIZATION SETTINGS\n",
      "============================================================\n",
      "Mode: FAST\n",
      "Trials: 20\n",
      "Folds: 4\n",
      "\n",
      "================================================================================\n",
      "STARTING BAYESIAN OPTIMIZATION\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52ecaa99513439da8be6e0a69432abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[993]\tvalid_0's rmse: 8.82734\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's rmse: 8.83552\n",
      "[W 2026-01-10 15:47:42,096] Trial 0 failed with parameters: {'learning_rate': 0.04370861069626263} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lorenzo/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_10565/1394054503.py\", line 122, in <lambda>\n",
      "    lambda trial: objective_lgb(trial, X_train, y_train),\n",
      "                  ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10565/1394054503.py\", line 101, in objective_lgb\n",
      "    preds = np.clip(model.predict(X_val), 0, 100)\n",
      "                    ~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/home/lorenzo/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/lightgbm/basic.py\", line 4767, in predict\n",
      "    return predictor.predict(\n",
      "           ~~~~~~~~~~~~~~~~~^\n",
      "        data=data,\n",
      "        ^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        validate_features=validate_features,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/lorenzo/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/lightgbm/basic.py\", line 1204, in predict\n",
      "    preds, nrow = self.__pred_for_np2d(\n",
      "                  ~~~~~~~~~~~~~~~~~~~~^\n",
      "        mat=data,\n",
      "        ^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        predict_type=predict_type,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/lorenzo/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/lightgbm/basic.py\", line 1361, in __pred_for_np2d\n",
      "    return self.__inner_predict_np2d(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        mat=mat,\n",
      "        ^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        preds=None,\n",
      "        ^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/lorenzo/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/lightgbm/basic.py\", line 1308, in __inner_predict_np2d\n",
      "    _LIB.LGBM_BoosterPredictForMat(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._handle,\n",
      "        ^^^^^^^^^^^^^\n",
      "    ...<10 lines>...\n",
      "        preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2026-01-10 15:47:42,099] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    114\u001b[39m start_time = time.time()\n\u001b[32m    116\u001b[39m study = optuna.create_study(\n\u001b[32m    117\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    118\u001b[39m     sampler=optuna.samplers.TPESampler(seed=\u001b[32m42\u001b[39m)\n\u001b[32m    119\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_lgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    126\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m optimization_time = time.time() - start_time\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# RESULTS\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    114\u001b[39m start_time = time.time()\n\u001b[32m    116\u001b[39m study = optuna.create_study(\n\u001b[32m    117\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    118\u001b[39m     sampler=optuna.samplers.TPESampler(seed=\u001b[32m42\u001b[39m)\n\u001b[32m    119\u001b[39m )\n\u001b[32m    121\u001b[39m study.optimize(\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_lgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    123\u001b[39m     n_trials=N_TRIALS,\n\u001b[32m    124\u001b[39m     show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    125\u001b[39m     n_jobs=\u001b[32m1\u001b[39m\n\u001b[32m    126\u001b[39m )\n\u001b[32m    128\u001b[39m optimization_time = time.time() - start_time\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# RESULTS\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mobjective_lgb\u001b[39m\u001b[34m(trial, X, y)\u001b[39m\n\u001b[32m     93\u001b[39m val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\u001b[32m     95\u001b[39m model = lgb.train(\n\u001b[32m     96\u001b[39m     params, train_data, num_boost_round=\u001b[32m1000\u001b[39m,\n\u001b[32m     97\u001b[39m     valid_sets=[val_data],\n\u001b[32m     98\u001b[39m     callbacks=[lgb.early_stopping(\u001b[32m50\u001b[39m), lgb.log_evaluation(\u001b[32m0\u001b[39m)]\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m preds = np.clip(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m, \u001b[32m0\u001b[39m, \u001b[32m100\u001b[39m)\n\u001b[32m    102\u001b[39m rmse = np.sqrt(mean_squared_error(y_val, preds))\n\u001b[32m    103\u001b[39m scores.append(rmse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/lightgbm/basic.py:4767\u001b[39m, in \u001b[36mBooster.predict\u001b[39m\u001b[34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[39m\n\u001b[32m   4765\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4766\u001b[39m         num_iteration = -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4767\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4776\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/lightgbm/basic.py:1204\u001b[39m, in \u001b[36m_InnerPredictor.predict\u001b[39m\u001b[34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[39m\n\u001b[32m   1197\u001b[39m     preds, nrow = \u001b[38;5;28mself\u001b[39m.__pred_for_csc(\n\u001b[32m   1198\u001b[39m         csc=data,\n\u001b[32m   1199\u001b[39m         start_iteration=start_iteration,\n\u001b[32m   1200\u001b[39m         num_iteration=num_iteration,\n\u001b[32m   1201\u001b[39m         predict_type=predict_type,\n\u001b[32m   1202\u001b[39m     )\n\u001b[32m   1203\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np.ndarray):\n\u001b[32m-> \u001b[39m\u001b[32m1204\u001b[39m     preds, nrow = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data):\n\u001b[32m   1211\u001b[39m     preds, nrow = \u001b[38;5;28mself\u001b[39m.__pred_for_pyarrow_table(\n\u001b[32m   1212\u001b[39m         table=data,\n\u001b[32m   1213\u001b[39m         start_iteration=start_iteration,\n\u001b[32m   1214\u001b[39m         num_iteration=num_iteration,\n\u001b[32m   1215\u001b[39m         predict_type=predict_type,\n\u001b[32m   1216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/lightgbm/basic.py:1361\u001b[39m, in \u001b[36m_InnerPredictor.__pred_for_np2d\u001b[39m\u001b[34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[39m\n\u001b[32m   1359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__inner_predict_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/datascience_projects/students-scores/.venv/lib/python3.13/site-packages/lightgbm/basic.py:1308\u001b[39m, in \u001b[36m_InnerPredictor.__inner_predict_np2d\u001b[39m\u001b[34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[39m\n\u001b[32m   1305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWrong length of pre-allocated predict array\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1306\u001b[39m out_num_preds = ctypes.c_int64(\u001b[32m0\u001b[39m)\n\u001b[32m   1307\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m1308\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m )\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_preds != out_num_preds.value:\n\u001b[32m   1324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWrong length for predict results\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =============================================================================\n",
    "# GPU CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"LIGHTGBM HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    USE_GPU = True\n",
    "else:\n",
    "    print(\"⚠ No GPU detected, using CPU\")\n",
    "    USE_GPU = False\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = joblib.load('../../data/preprocessed/X_train.pkl')\n",
    "y_train = joblib.load('../../data/preprocessed/y_train.pkl')\n",
    "X_test = joblib.load('../../data/preprocessed/X_test.pkl')\n",
    "metadata = joblib.load('../../data/preprocessed/metadata.pkl')\n",
    "\n",
    "print(f\"✓ Train shape: {X_train.shape}\")\n",
    "print(f\"✓ Target shape: {y_train.shape}\")\n",
    "print(f\"✓ Test shape: {X_test.shape}\")\n",
    "print(f\"✓ GPU available: {USE_GPU}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZATION SETTINGS\n",
    "# =============================================================================\n",
    "FAST_MODE = False  # Set to False for more thorough search\n",
    "N_TRIALS = 15 if FAST_MODE else 400\n",
    "N_FOLDS = 5\n",
    "N_JOBS_OPTUNA = 4  # Parallel trials\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OPTIMIZATION SETTINGS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {'FAST' if FAST_MODE else 'THOROUGH'}\")\n",
    "print(f\"Trials: {N_TRIALS}\")\n",
    "print(f\"Folds: {N_FOLDS}\")\n",
    "print(f\"Parallel jobs: {N_JOBS_OPTUNA}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "def objective_lgb(trial, X, y):\n",
    "    \"\"\"Objective function for LightGBM optimization\"\"\"\n",
    "    \n",
    "    # Base parameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'device': 'gpu' if USE_GPU else 'cpu',\n",
    "        'seed': RANDOM_SEED,\n",
    "        'force_col_wise': True,  # Faster on CPU\n",
    "    }\n",
    "    \n",
    "    # GPU-specific parameters\n",
    "    if USE_GPU:\n",
    "        params['gpu_use_dp'] = False  # Use single precision on GPU\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    params.update({\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0, 10.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0, 10.0),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 5.0),\n",
    "    })\n",
    "    \n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 100, 3000)\n",
    "    \n",
    "    # Remove None values\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        \n",
    "        # Handle both pandas Series and numpy arrays\n",
    "        if isinstance(y, pd.Series):\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        else:\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Create LightGBM datasets\n",
    "        train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "        # Train with early stopping\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=num_boost_round,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "                lgb.log_evaluation(period=0)  # Suppress output\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Predict and clip\n",
    "        preds = np.clip(model.predict(X_val), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# =============================================================================\n",
    "# RUN OPTIMIZATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    lambda trial: objective_lgb(trial, X_train, y_train),\n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True,\n",
    "    n_jobs=N_JOBS_OPTUNA\n",
    ")\n",
    "\n",
    "optimization_time = time.time() - start_time\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best RMSE: {study.best_value:.6f}\")\n",
    "print(f\"Optimization time: {optimization_time:.1f}s\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in sorted(study.best_params.items()):\n",
    "    print(f\"  {param:20s}: {value}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "# Prepare final parameters\n",
    "best_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1,\n",
    "    'device': 'gpu' if USE_GPU else 'cpu',\n",
    "    'seed': RANDOM_SEED,\n",
    "    'force_col_wise': True,\n",
    "}\n",
    "\n",
    "if USE_GPU:\n",
    "    best_params['gpu_use_dp'] = False\n",
    "\n",
    "best_params.update(study.best_params)\n",
    "\n",
    "# Extract num_boost_round separately\n",
    "num_boost_round = best_params.pop('num_boost_round')\n",
    "\n",
    "# Remove None values\n",
    "best_params = {k: v for k, v in best_params.items() if v is not None}\n",
    "\n",
    "joblib.dump(best_params, 'lightgbm_params.pkl')\n",
    "joblib.dump(num_boost_round, 'lightgbm_num_boost_round.pkl')\n",
    "print(\"\\n✓ Parameters saved to: lightgbm_params.pkl\")\n",
    "print(f\"✓ num_boost_round: {num_boost_round}\")\n",
    "\n",
    "# Save optimization history\n",
    "history = pd.DataFrame({\n",
    "    'trial': [t.number for t in study.trials],\n",
    "    'value': [t.value for t in study.trials],\n",
    "    'params': [str(t.params) for t in study.trials]\n",
    "})\n",
    "history.to_csv('lightgbm_history.csv', index=False)\n",
    "print(\"✓ History saved to: lightgbm_history.csv\")\n",
    "\n",
    "# Save study object\n",
    "joblib.dump(study, 'lightgbm_study.pkl')\n",
    "print(\"✓ Study saved to: lightgbm_study.pkl\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'model': 'LightGBM',\n",
    "    'best_rmse': study.best_value,\n",
    "    'n_trials': N_TRIALS,\n",
    "    'n_folds': N_FOLDS,\n",
    "    'optimization_time': optimization_time,\n",
    "    'use_gpu': USE_GPU,\n",
    "    'best_params': best_params,\n",
    "    'num_boost_round': num_boost_round\n",
    "}\n",
    "joblib.dump(summary, 'lightgbm_summary.pkl')\n",
    "print(\"✓ Summary saved to: lightgbm_summary.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN FINAL MODEL ON FULL TRAINING DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL ON FULL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create LightGBM dataset for full training data\n",
    "train_data_full = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "print(f\"Training final model with {num_boost_round} rounds...\")\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    train_data_full,\n",
    "    num_boost_round=num_boost_round,\n",
    "    callbacks=[lgb.log_evaluation(period=100)]  # Show progress every 100 rounds\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_model.save_model('lightgbm_final_model.txt')\n",
    "joblib.dump(final_model, 'lightgbm_final_model.pkl')\n",
    "print(\"\\n✓ Final model saved to: lightgbm_final_model.txt\")\n",
    "print(\"✓ Final model saved to: lightgbm_final_model.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE PREDICTIONS AND SUBMISSION FILE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "# Clip predictions to valid range [0, 100]\n",
    "test_predictions = np.clip(test_predictions, 0, 100)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(X_train), len(test_predictions) + len(X_train)),\n",
    "    'test_score': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✓ Submission file saved to: submission.csv\")\n",
    "\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"  Min prediction: {test_predictions.min():.2f}\")\n",
    "print(f\"  Max prediction: {test_predictions.max():.2f}\")\n",
    "print(f\"  Mean prediction: {test_predictions.mean():.2f}\")\n",
    "print(f\"  Median prediction: {np.median(test_predictions):.2f}\")\n",
    "print(f\"  Std prediction: {test_predictions.std():.2f}\")\n",
    "print(f\"  Shape: {submission.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance (using 'gain' as importance type)\n",
    "importance_gain = final_model.feature_importance(importance_type='gain')\n",
    "importance_split = final_model.feature_importance(importance_type='split')\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance_gain': importance_gain,\n",
    "    'importance_split': importance_split\n",
    "}).sort_values('importance_gain', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features (by gain):\")\n",
    "print(feature_importance.head(10)[['feature', 'importance_gain']].to_string(index=False))\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('lightgbm_feature_importance.csv', index=False)\n",
    "print(\"\\n✓ Feature importance saved to: lightgbm_feature_importance.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL STATISTICS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of boosting rounds: {num_boost_round}\")\n",
    "print(f\"Number of trees: {final_model.num_trees()}\")\n",
    "print(f\"Number of features: {final_model.num_feature()}\")\n",
    "print(f\"Learning rate: {best_params['learning_rate']:.4f}\")\n",
    "print(f\"Num leaves: {best_params['num_leaves']}\")\n",
    "print(f\"Max depth: {best_params['max_depth']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Optimization history\n",
    "ax1 = axes[0, 0]\n",
    "history = pd.read_csv('lightgbm_history.csv')\n",
    "ax1.plot(history['trial'], history['value'], alpha=0.6, marker='o', markersize=4)\n",
    "ax1.plot(history['trial'], history['value'].cummin(), \n",
    "         linewidth=2.5, label='Best score', color='red')\n",
    "ax1.set_xlabel('Trial', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('RMSE', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Optimization Progress', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature importance (top 15 by gain)\n",
    "ax2 = axes[0, 1]\n",
    "top_features = feature_importance.head(15)\n",
    "y_pos = np.arange(len(top_features))\n",
    "ax2.barh(y_pos, top_features['importance_gain'], color='lightgreen', edgecolor='black')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(top_features['feature'], fontsize=9)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel('Importance (Gain)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Top 15 Feature Importances', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Feature importance comparison (gain vs split)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(feature_importance['importance_split'], \n",
    "           feature_importance['importance_gain'],\n",
    "           alpha=0.6, s=50)\n",
    "ax3.set_xlabel('Importance (Split)', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Importance (Gain)', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Feature Importance: Gain vs Split', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr = np.corrcoef(feature_importance['importance_split'], \n",
    "                   feature_importance['importance_gain'])[0, 1]\n",
    "ax3.text(0.05, 0.95, f'Correlation: {corr:.3f}',\n",
    "        transform=ax3.transAxes, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 4. Prediction distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(test_predictions, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax4.axvline(test_predictions.mean(), color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Mean: {test_predictions.mean():.2f}')\n",
    "ax4.axvline(np.median(test_predictions), color='green', linestyle='--',\n",
    "           linewidth=2, label=f'Median: {np.median(test_predictions):.2f}')\n",
    "ax4.set_xlabel('Predicted Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Test Predictions Distribution', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lightgbm_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Analysis plot saved to: lightgbm_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIGHTGBM OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Best CV RMSE: {study.best_value:.6f}\")\n",
    "print(f\"✓ Final model trained on {len(X_train)} samples\")\n",
    "print(f\"✓ Model has {final_model.num_trees()} trees\")\n",
    "print(f\"✓ Predictions generated for {len(X_test)} test samples\")\n",
    "print(f\"✓ All results saved\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  • lightgbm_params.pkl\")\n",
    "print(\"  • lightgbm_num_boost_round.pkl\")\n",
    "print(\"  • lightgbm_history.csv\")\n",
    "print(\"  • lightgbm_study.pkl\")\n",
    "print(\"  • lightgbm_summary.pkl\")\n",
    "print(\"  • lightgbm_final_model.txt\")\n",
    "print(\"  • lightgbm_final_model.pkl\")\n",
    "print(\"  • lightgbm_feature_importance.csv\")\n",
    "print(\"  • lightgbm_analysis.png\")\n",
    "print(\"  • submission.csv\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b68ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "students-scores",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
