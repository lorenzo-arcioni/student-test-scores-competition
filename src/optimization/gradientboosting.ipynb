{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b90dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import optuna\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"GRADIENT BOOSTING HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = joblib.load('../../data/preprocessed/X_train.pkl')\n",
    "y_train = joblib.load('../../data/preprocessed/y_train.pkl')\n",
    "X_test = joblib.load('../../data/preprocessed/X_test.pkl')\n",
    "metadata = joblib.load('../../data/preprocessed/metadata.pkl')\n",
    "\n",
    "print(f\"✓ Train shape: {X_train.shape}\")\n",
    "print(f\"✓ Target shape: {y_train.shape}\")\n",
    "print(f\"✓ Test shape: {X_test.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZATION SETTINGS\n",
    "# =============================================================================\n",
    "FAST_MODE = False  # Set to False for more thorough search\n",
    "N_TRIALS = 15 if FAST_MODE else 200\n",
    "N_FOLDS = 5\n",
    "N_JOBS_OPTUNA = 1  # Parallel trials\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OPTIMIZATION SETTINGS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mode: {'FAST' if FAST_MODE else 'THOROUGH'}\")\n",
    "print(f\"Trials: {N_TRIALS}\")\n",
    "print(f\"Folds: {N_FOLDS}\")\n",
    "print(f\"Parallel jobs: {N_JOBS_OPTUNA}\")\n",
    "\n",
    "# =============================================================================\n",
    "# OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "def objective_gbr(trial, X, y):\n",
    "    \"\"\"Objective function for GradientBoosting optimization\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Trial {trial.number}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Parametri testati: {params}\")\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'random_state': RANDOM_SEED,\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        \n",
    "        # Handle both pandas Series and numpy arrays\n",
    "        if isinstance(y, pd.Series):\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        else:\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        preds = np.clip(model.predict(X_val), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "        \n",
    "        print(f\"  Fold {fold_idx + 1}: RMSE = {rmse:.4f}\")\n",
    "    \n",
    "    optimization_time = time.time() - start_time\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"RMSE medio: {mean_score:.4f}\")\n",
    "    print(f\"Elapsed Time: {optimization_time:.1f}s\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return mean_score\n",
    "\n",
    "# =============================================================================\n",
    "# RUN OPTIMIZATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    lambda trial: objective_gbr(trial, X_train, y_train),\n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True,\n",
    "    #n_jobs=N_JOBS_OPTUNA\n",
    ")\n",
    "\n",
    "optimization_time = time.time() - start_time\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best RMSE: {study.best_value:.6f}\")\n",
    "print(f\"Optimization time: {optimization_time:.1f}s\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in sorted(study.best_params.items()):\n",
    "    print(f\"  {param:20s}: {value}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "# Save best parameters\n",
    "best_params = study.best_params.copy()\n",
    "best_params['random_state'] = RANDOM_SEED\n",
    "\n",
    "joblib.dump(best_params, 'gradientboosting_params.pkl')\n",
    "print(\"\\n✓ Parameters saved to: gradientboosting_params.pkl\")\n",
    "\n",
    "# Save optimization history\n",
    "history = pd.DataFrame({\n",
    "    'trial': [t.number for t in study.trials],\n",
    "    'value': [t.value for t in study.trials],\n",
    "    'params': [str(t.params) for t in study.trials]\n",
    "})\n",
    "history.to_csv('gradientboosting_history.csv', index=False)\n",
    "print(\"✓ History saved to: gradientboosting_history.csv\")\n",
    "\n",
    "# Save study object\n",
    "joblib.dump(study, 'gradientboosting_study.pkl')\n",
    "print(\"✓ Study saved to: gradientboosting_study.pkl\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'model': 'GradientBoosting',\n",
    "    'best_rmse': study.best_value,\n",
    "    'n_trials': N_TRIALS,\n",
    "    'n_folds': N_FOLDS,\n",
    "    'optimization_time': optimization_time,\n",
    "    'best_params': best_params\n",
    "}\n",
    "joblib.dump(summary, 'gradientboosting_summary.pkl')\n",
    "print(\"✓ Summary saved to: gradientboosting_summary.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN FINAL MODEL ON FULL TRAINING DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL ON FULL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_model = GradientBoostingRegressor(**best_params)\n",
    "final_model.set_params(verbose=1)  # Show training progress\n",
    "\n",
    "print(\"Training final model...\")\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Save final model\n",
    "joblib.dump(final_model, 'gradientboosting_final_model.pkl')\n",
    "print(\"\\n✓ Final model saved to: gradientboosting_final_model.pkl\")\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE PREDICTIONS AND SUBMISSION FILE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "# Clip predictions to valid range [0, 100]\n",
    "test_predictions = np.clip(test_predictions, 0, 100)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(X_train), len(test_predictions) + len(X_train)),\n",
    "    'test_score': test_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✓ Submission file saved to: submission.csv\")\n",
    "\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"  Min prediction: {test_predictions.min():.2f}\")\n",
    "print(f\"  Max prediction: {test_predictions.max():.2f}\")\n",
    "print(f\"  Mean prediction: {test_predictions.mean():.2f}\")\n",
    "print(f\"  Median prediction: {np.median(test_predictions):.2f}\")\n",
    "print(f\"  Std prediction: {test_predictions.std():.2f}\")\n",
    "print(f\"  Shape: {submission.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('gradientboosting_feature_importance.csv', index=False)\n",
    "print(\"\\n✓ Feature importance saved to: gradientboosting_feature_importance.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING PROGRESS ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING PROGRESS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get training scores (on training data)\n",
    "train_score = final_model.train_score_\n",
    "\n",
    "print(f\"Initial training score (first iteration): {train_score[0]:.6f}\")\n",
    "print(f\"Final training score: {train_score[-1]:.6f}\")\n",
    "print(f\"Total improvement: {train_score[0] - train_score[-1]:.6f}\")\n",
    "\n",
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training score over iterations\n",
    "ax1 = axes[0]\n",
    "ax1.plot(range(1, len(train_score) + 1), train_score, linewidth=2)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Training Score (Deviance)')\n",
    "ax1.set_title('Training Progress')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance (top 15)\n",
    "ax2 = axes[1]\n",
    "top_features = feature_importance.head(15)\n",
    "y_pos = np.arange(len(top_features))\n",
    "ax2.barh(y_pos, top_features['importance'], color='skyblue', edgecolor='black')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(top_features['feature'], fontsize=9)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_title('Top 15 Feature Importances')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gradientboosting_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Training analysis plot saved to: gradientboosting_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL STATISTICS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of estimators: {best_params['n_estimators']}\")\n",
    "print(f\"Learning rate: {best_params['learning_rate']:.4f}\")\n",
    "print(f\"Max depth: {best_params['max_depth']}\")\n",
    "print(f\"Subsample ratio: {best_params['subsample']:.2f}\")\n",
    "print(f\"Total number of leaves across all trees: {sum(est[0].tree_.n_leaves for est in final_model.estimators_)}\")\n",
    "print(f\"Average leaves per tree: {sum(est[0].tree_.n_leaves for est in final_model.estimators_) / len(final_model.estimators_):.1f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRADIENT BOOSTING OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Best CV RMSE: {study.best_value:.6f}\")\n",
    "print(f\"✓ Final model trained on {len(X_train)} samples\")\n",
    "print(f\"✓ Model has {best_params['n_estimators']} estimators\")\n",
    "print(f\"✓ Predictions generated for {len(X_test)} test samples\")\n",
    "print(f\"✓ All results saved\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  • gradientboosting_params.pkl\")\n",
    "print(\"  • gradientboosting_history.csv\")\n",
    "print(\"  • gradientboosting_study.pkl\")\n",
    "print(\"  • gradientboosting_summary.pkl\")\n",
    "print(\"  • gradientboosting_final_model.pkl\")\n",
    "print(\"  • gradientboosting_feature_importance.csv\")\n",
    "print(\"  • gradientboosting_analysis.png\")\n",
    "print(\"  • submission.csv\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
