{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "507f8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# =============================================================================\n",
    "# GPU CONFIGURATION\n",
    "# =============================================================================\n",
    "# import torch\n",
    "# print(\"=\"*80)\n",
    "# print(\"GPU CONFIGURATION\")\n",
    "# print(\"=\"*80)\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"✓ CUDA version: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "#     print(f\"✓ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "#     USE_GPU = True\n",
    "# else:\n",
    "#     print(\"⚠ No GPU detected, using CPU\")\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0024ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "Train: (500, 13), Test: (100, 12), Original: (200, 13)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. LOAD DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_df = pd.read_csv(\"../data/train.csv\")[:500]\n",
    "test_df = pd.read_csv(\"../data/test.csv\")[:100]\n",
    "original_df = pd.read_csv(\"../data/Exam_Score_Prediction.csv\")[:200]\n",
    "\n",
    "TARGET = \"exam_score\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}, Original: {original_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e643a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADVERSARIAL VALIDATION - Detecting Distribution Shift\n",
      "================================================================================\n",
      "Adversarial AUC: 0.4861\n",
      "✓ Distribution shift is minimal\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. ADVERSARIAL VALIDATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVERSARIAL VALIDATION - Detecting Distribution Shift\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def adversarial_validation(train, test, features):\n",
    "    \"\"\"Identify train-test distribution differences\"\"\"\n",
    "    train_adv = train[features].copy()\n",
    "    test_adv = test[features].copy()\n",
    "\n",
    "    train_adv['is_test'] = 0\n",
    "    test_adv['is_test'] = 1\n",
    "\n",
    "    combined = pd.concat([train_adv, test_adv], axis=0, ignore_index=True)\n",
    "\n",
    "    for col in combined.select_dtypes(include=['object']).columns:\n",
    "        if col != 'is_test':\n",
    "            combined[col] = combined[col].astype('category').cat.codes\n",
    "\n",
    "    X = combined.drop('is_test', axis=1)\n",
    "    y = combined['is_test']\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y, oof_preds)\n",
    "    print(f\"Adversarial AUC: {auc:.4f}\")\n",
    "\n",
    "    if auc > 0.6:\n",
    "        print(\"⚠️  Significant distribution shift detected!\")\n",
    "        model = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "        model.fit(X, y)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"\\nTop features causing shift:\")\n",
    "        print(importance_df.head(10))\n",
    "    else:\n",
    "        print(\"✓ Distribution shift is minimal\")\n",
    "\n",
    "    return auc\n",
    "\n",
    "base_features = [c for c in train_df.columns if c not in [ID_COL, TARGET]]\n",
    "adv_auc = adversarial_validation(train_df, test_df, base_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfdde88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>course</th>\n",
       "      <th>study_hours</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>internet_access</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>study_method</th>\n",
       "      <th>facility_rating</th>\n",
       "      <th>exam_difficulty</th>\n",
       "      <th>exam_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>female</td>\n",
       "      <td>b.sc</td>\n",
       "      <td>7.91</td>\n",
       "      <td>98.8</td>\n",
       "      <td>no</td>\n",
       "      <td>4.9</td>\n",
       "      <td>average</td>\n",
       "      <td>online videos</td>\n",
       "      <td>low</td>\n",
       "      <td>easy</td>\n",
       "      <td>78.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>other</td>\n",
       "      <td>diploma</td>\n",
       "      <td>4.95</td>\n",
       "      <td>94.8</td>\n",
       "      <td>yes</td>\n",
       "      <td>4.7</td>\n",
       "      <td>poor</td>\n",
       "      <td>self-study</td>\n",
       "      <td>medium</td>\n",
       "      <td>moderate</td>\n",
       "      <td>46.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>female</td>\n",
       "      <td>b.sc</td>\n",
       "      <td>4.68</td>\n",
       "      <td>92.6</td>\n",
       "      <td>yes</td>\n",
       "      <td>5.8</td>\n",
       "      <td>poor</td>\n",
       "      <td>coaching</td>\n",
       "      <td>high</td>\n",
       "      <td>moderate</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>male</td>\n",
       "      <td>b.sc</td>\n",
       "      <td>2.00</td>\n",
       "      <td>49.5</td>\n",
       "      <td>yes</td>\n",
       "      <td>8.3</td>\n",
       "      <td>average</td>\n",
       "      <td>group study</td>\n",
       "      <td>high</td>\n",
       "      <td>moderate</td>\n",
       "      <td>63.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "      <td>bca</td>\n",
       "      <td>7.65</td>\n",
       "      <td>86.9</td>\n",
       "      <td>yes</td>\n",
       "      <td>9.6</td>\n",
       "      <td>good</td>\n",
       "      <td>self-study</td>\n",
       "      <td>high</td>\n",
       "      <td>easy</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age  gender   course  study_hours  class_attendance internet_access  \\\n",
       "0   0   21  female     b.sc         7.91              98.8              no   \n",
       "1   1   18   other  diploma         4.95              94.8             yes   \n",
       "2   2   20  female     b.sc         4.68              92.6             yes   \n",
       "3   3   19    male     b.sc         2.00              49.5             yes   \n",
       "4   4   23    male      bca         7.65              86.9             yes   \n",
       "\n",
       "   sleep_hours sleep_quality   study_method facility_rating exam_difficulty  \\\n",
       "0          4.9       average  online videos             low            easy   \n",
       "1          4.7          poor     self-study          medium        moderate   \n",
       "2          5.8          poor       coaching            high        moderate   \n",
       "3          8.3       average    group study            high        moderate   \n",
       "4          9.6          good     self-study            high            easy   \n",
       "\n",
       "   exam_score  \n",
       "0        78.3  \n",
       "1        46.7  \n",
       "2        99.0  \n",
       "3        63.9  \n",
       "4       100.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ee53cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZED FEATURE ENGINEERING (EDA-DRIVEN)\n",
      "================================================================================\n",
      "\n",
      "Applying optimized feature engineering...\n",
      "  Creating Tier 1: Critical features...\n",
      "  Creating Tier 2: High-impact categorical features...\n",
      "  Creating Tier 3: Categorical × numeric interactions...\n",
      "  Creating Tier 4: Domain-specific flags...\n",
      "  Creating Tier 5: Intelligent binning...\n",
      "  Creating Tier 6: Selective transformations...\n",
      "  Creating Tier 7: Distance from optimal...\n",
      "  Creating Tier 8: Meaningful ratios...\n",
      "✓ Feature engineering complete!\n",
      "  Creating Tier 1: Critical features...\n",
      "  Creating Tier 2: High-impact categorical features...\n",
      "  Creating Tier 3: Categorical × numeric interactions...\n",
      "  Creating Tier 4: Domain-specific flags...\n",
      "  Creating Tier 5: Intelligent binning...\n",
      "  Creating Tier 6: Selective transformations...\n",
      "  Creating Tier 7: Distance from optimal...\n",
      "  Creating Tier 8: Meaningful ratios...\n",
      "✓ Feature engineering complete!\n",
      "  Creating Tier 1: Critical features...\n",
      "  Creating Tier 2: High-impact categorical features...\n",
      "  Creating Tier 3: Categorical × numeric interactions...\n",
      "  Creating Tier 4: Domain-specific flags...\n",
      "  Creating Tier 5: Intelligent binning...\n",
      "  Creating Tier 6: Selective transformations...\n",
      "  Creating Tier 7: Distance from optimal...\n",
      "  Creating Tier 8: Meaningful ratios...\n",
      "✓ Feature engineering complete!\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "High-impact categoricals (for target encoding): ['sleep_quality', 'study_method', 'facility_rating']\n",
      "Low-impact categoricals (will be removed): ['gender', 'course', 'internet_access', 'exam_difficulty']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. OPTIMIZED FEATURE ENGINEERING (EDA-DRIVEN)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZED FEATURE ENGINEERING (EDA-DRIVEN)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_optimized_features(df):\n",
    "    \"\"\"\n",
    "    Create high-impact features based strictly on EDA findings.\n",
    "    Focus: Quality over Quantity - Only features with theoretical justification.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    eps = 1e-5\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 1: CRITICAL FEATURES (Core predictors from EDA)\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 1: Critical features...\")\n",
    "    \n",
    "    # Polynomial features (degree 2 only)\n",
    "    df['study_sq'] = df['study_hours'] ** 2\n",
    "    df['attendance_sq'] = df['class_attendance'] ** 2\n",
    "    \n",
    "    # Key interactions\n",
    "    df['study_x_attendance'] = df['study_hours'] * df['class_attendance']\n",
    "    df['study_x_sleep'] = df['study_hours'] * df['sleep_hours']\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    df['efficiency'] = (df['study_hours'] * df['class_attendance']) / (df['sleep_hours'] + 1)\n",
    "    df['efficiency_sq'] = df['efficiency'] ** 2\n",
    "    \n",
    "    # Weighted effort (EDA-informed weights)\n",
    "    df['weighted_effort'] = (0.06 * df['class_attendance'] + \n",
    "                             2.0 * df['study_hours'] + \n",
    "                             1.2 * df['sleep_hours'])\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 2: HIGH-IMPACT CATEGORICAL FEATURES (η² > 2% from EDA)\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 2: High-impact categorical features...\")\n",
    "    \n",
    "    # Ordinal encoding for features with clear monotonic relationships\n",
    "    sleep_quality_map = {'poor': 0, 'average': 1, 'good': 2}\n",
    "    df['sleep_quality_ord'] = df['sleep_quality'].map(sleep_quality_map).fillna(1)\n",
    "    \n",
    "    study_method_map = {'self-study': 0, 'online videos': 1, 'group study': 2, \n",
    "                        'mixed': 3, 'coaching': 4}\n",
    "    df['study_method_ord'] = df['study_method'].map(study_method_map).fillna(2)\n",
    "    \n",
    "    facility_map = {'low': 0, 'medium': 1, 'high': 2}\n",
    "    df['facility_ord'] = df['facility_rating'].map(facility_map).fillna(1)\n",
    "    \n",
    "    difficulty_map = {'easy': 0, 'moderate': 1, 'hard': 2}\n",
    "    df['difficulty_ord'] = df['exam_difficulty'].map(difficulty_map).fillna(1)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 3: CATEGORICAL × NUMERIC INTERACTIONS\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 3: Categorical × numeric interactions...\")\n",
    "    \n",
    "    df['sleep_quality_x_study'] = df['sleep_quality_ord'] * df['study_hours']\n",
    "    df['facility_x_attendance'] = df['facility_ord'] * df['class_attendance']\n",
    "    df['study_method_x_hours'] = df['study_method_ord'] * df['study_hours']\n",
    "    df['difficulty_x_efficiency'] = df['difficulty_ord'] * df['efficiency']\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 4: DOMAIN-SPECIFIC FLAGS\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 4: Domain-specific flags...\")\n",
    "    \n",
    "    df['ideal_sleep'] = ((df['sleep_hours'] >= 7) & (df['sleep_hours'] <= 9)).astype(int)\n",
    "    df['sleep_deprived'] = (df['sleep_hours'] <= 5.5).astype(int)\n",
    "    df['high_performer'] = ((df['study_hours'] >= 6) & (df['class_attendance'] >= 85)).astype(int)\n",
    "    df['at_risk'] = ((df['study_hours'] <= 3) | (df['class_attendance'] <= 60)).astype(int)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 5: BINNING FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 5: Intelligent binning...\")\n",
    "    \n",
    "    df['study_bin'] = pd.cut(df['study_hours'], bins=[-0.1, 2, 4, 6, 8], labels=False).astype(float)\n",
    "    df['attendance_bin'] = pd.cut(df['class_attendance'], bins=[40, 60, 75, 85, 95, 100], labels=False).astype(float)\n",
    "    df['sleep_bin'] = pd.cut(df['sleep_hours'], bins=[4, 5.5, 7, 8.5, 10], labels=False).astype(float)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 6: SELECTIVE TRANSFORMATIONS\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 6: Selective transformations...\")\n",
    "    \n",
    "    df['log_study_hours'] = np.log1p(df['study_hours'])\n",
    "    df['sqrt_attendance'] = np.sqrt(df['class_attendance'])\n",
    "    df['age_rank'] = df['age'].rank(pct=True)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 7: DISTANCE FROM OPTIMAL\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 7: Distance from optimal...\")\n",
    "    \n",
    "    df['sleep_from_optimal'] = np.abs(df['sleep_hours'] - 8)\n",
    "    df['study_from_optimal'] = np.abs(df['study_hours'] - 6)\n",
    "    df['attendance_from_optimal'] = np.abs(df['class_attendance'] - 95)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 8: RATIOS\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 8: Meaningful ratios...\")\n",
    "    \n",
    "    df['study_per_sleep'] = df['study_hours'] / (df['sleep_hours'] + eps)\n",
    "    df['geometric_mean'] = (\n",
    "        (df['study_hours'] + 1) *\n",
    "        (df['class_attendance'] + 1) *\n",
    "        (df['sleep_hours'] + 1)\n",
    "    ) ** (1/3)\n",
    "    \n",
    "    print(\"✓ Feature engineering complete!\")\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# Apply feature engineering\n",
    "# =============================================================================\n",
    "print(\"\\nApplying optimized feature engineering...\")\n",
    "train_fe = create_optimized_features(train_df)\n",
    "test_fe = create_optimized_features(test_df)\n",
    "original_fe = create_optimized_features(original_df)\n",
    "\n",
    "y = train_df[TARGET].clip(0, 100).values\n",
    "y_orig = original_df[TARGET].clip(0, 100).values\n",
    "\n",
    "# Categorical features for target encoding (only high-impact from EDA)\n",
    "cat_features = [\n",
    "    'sleep_quality',      # η² = 5.6%\n",
    "    'study_method',       # η² = 5.0%\n",
    "    'facility_rating',    # η² = 3.6%\n",
    "]\n",
    "\n",
    "# Low-impact categoricals to remove\n",
    "low_impact_cats = [\n",
    "    'gender',           # η² < 0.2%\n",
    "    'course',           # η² < 0.2%\n",
    "    'internet_access',  # η² < 0.2%\n",
    "    'exam_difficulty',  # We have difficulty_ord instead\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"High-impact categoricals (for target encoding): {cat_features}\")\n",
    "print(f\"Low-impact categoricals (will be removed): {low_impact_cats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1a16bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TARGET ENCODING ON CATEGORICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      "Target encoding TRAIN and TEST (with CV)...\n",
      "  Encoding sleep_quality...\n",
      "  Encoding study_method...\n",
      "  Encoding facility_rating...\n",
      "\n",
      "Target encoding ORIGINAL (simple, no CV)...\n",
      "  Encoding sleep_quality (simple)...\n",
      "  Encoding study_method (simple)...\n",
      "  Encoding facility_rating (simple)...\n",
      "\n",
      "✓ Target encoding complete. Shape: (500, 37)\n",
      "✓ Features after cleanup: 37\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "Training quick LightGBM for feature importance...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's rmse: 9.60017\n",
      "  Fold 1/3 done\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's rmse: 8.98488\n",
      "  Fold 2/3 done\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's rmse: 9.06568\n",
      "  Fold 3/3 done\n",
      "\n",
      "Total features: 37\n",
      "\n",
      "Top 20 most important features:\n",
      "                    feature     importance  cumulative_importance\n",
      "6        study_x_attendance  551241.990926               0.516000\n",
      "10          weighted_effort  108871.269278               0.617910\n",
      "33           geometric_mean   95789.586806               0.707576\n",
      "15    sleep_quality_x_study   91854.583913               0.793558\n",
      "17     study_method_x_hours   34872.578581               0.826201\n",
      "16    facility_x_attendance   31239.794084               0.855444\n",
      "2          class_attendance   21685.611624               0.875743\n",
      "1               study_hours   20626.282483               0.895050\n",
      "7             study_x_sleep   13790.038386               0.907959\n",
      "32          study_per_sleep   13711.356734               0.920794\n",
      "35      study_method_target   12364.453988               0.932368\n",
      "29       sleep_from_optimal    9979.785039               0.941709\n",
      "11        sleep_quality_ord    8966.018661               0.950102\n",
      "18  difficulty_x_efficiency    7959.767935               0.957553\n",
      "3               sleep_hours    7327.602745               0.964412\n",
      "8                efficiency    7108.541622               0.971066\n",
      "0                       age    6714.273413               0.977351\n",
      "36   facility_rating_target    6048.992664               0.983013\n",
      "12         study_method_ord    5069.285114               0.987759\n",
      "34     sleep_quality_target    4844.798439               0.992294\n",
      "\n",
      "============================================================\n",
      "FEATURE SELECTION STRATEGY\n",
      "============================================================\n",
      "Given we have 37 well-designed features,\n",
      "we'll use a CONSERVATIVE threshold to keep most features.\n",
      "\n",
      "\n",
      "============================================================\n",
      "FEATURE SELECTION RESULTS\n",
      "============================================================\n",
      "Original features: 37\n",
      "Selected features: 17\n",
      "Reduction: 54.1%\n",
      "\n",
      "✓ Feature selection complete!\n",
      "✓ Final dataset shape: (500, 17)\n",
      "✓ Feature importance saved to feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. TARGET ENCODING (BEFORE feature selection)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TARGET ENCODING ON CATEGORICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def target_encode_cv(X_train, X_test, y_train, cat_cols, n_splits=5, alpha=10):\n",
    "    \"\"\"\n",
    "    Target encoding with CV to prevent leakage.\n",
    "    Alpha = smoothing parameter (higher = more regularization)\n",
    "    \"\"\"\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    global_mean = y_train.mean()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        print(f\"  Encoding {col}...\")\n",
    "        \n",
    "        # For train: use CV to prevent leakage\n",
    "        X_train_enc[f'{col}_target'] = 0.0\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_tr = X_train.iloc[train_idx]\n",
    "            y_tr = y_train[train_idx]\n",
    "            \n",
    "            # Create temporary DataFrame to align X and y\n",
    "            df_fold = pd.DataFrame({\n",
    "                'category': X_tr[col].values,\n",
    "                'target': y_tr\n",
    "            })\n",
    "            \n",
    "            # Calculate mean encoding on training fold\n",
    "            target_means = df_fold.groupby('category')['target'].agg(['mean', 'count'])\n",
    "            target_means['target_enc'] = (\n",
    "                (target_means['mean'] * target_means['count'] + global_mean * alpha) / \n",
    "                (target_means['count'] + alpha)\n",
    "            )\n",
    "            \n",
    "            # Apply to validation fold\n",
    "            X_train_enc.loc[val_idx, f'{col}_target'] = (\n",
    "                X_train.iloc[val_idx][col]\n",
    "                .map(target_means['target_enc'])\n",
    "                .fillna(global_mean)\n",
    "            )\n",
    "        \n",
    "        # For test: use all training data to calculate means\n",
    "        df_full = pd.DataFrame({\n",
    "            'category': X_train[col].values,\n",
    "            'target': y_train\n",
    "        })\n",
    "        \n",
    "        target_means_full = df_full.groupby('category')['target'].agg(['mean', 'count'])\n",
    "        target_means_full['target_enc'] = (\n",
    "            (target_means_full['mean'] * target_means_full['count'] + global_mean * alpha) / \n",
    "            (target_means_full['count'] + alpha)\n",
    "        )\n",
    "        \n",
    "        X_test_enc[f'{col}_target'] = (\n",
    "            X_test[col]\n",
    "            .map(target_means_full['target_enc'])\n",
    "            .fillna(global_mean)\n",
    "        )\n",
    "        \n",
    "        # Remove original column (now we have the encoded version)\n",
    "        X_train_enc = X_train_enc.drop(columns=[col])\n",
    "        X_test_enc = X_test_enc.drop(columns=[col])\n",
    "    \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "\n",
    "def target_encode_simple(X, y, cat_cols, alpha=10):\n",
    "    \"\"\"Simple target encoding (no CV) for small datasets\"\"\"\n",
    "    X_enc = X.copy()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        print(f\"  Encoding {col} (simple)...\")\n",
    "        \n",
    "        df_temp = pd.DataFrame({\n",
    "            'category': X[col].values,\n",
    "            'target': y\n",
    "        })\n",
    "        \n",
    "        target_means = df_temp.groupby('category')['target'].agg(['mean', 'count'])\n",
    "        target_means['target_enc'] = (\n",
    "            (target_means['mean'] * target_means['count'] + global_mean * alpha) / \n",
    "            (target_means['count'] + alpha)\n",
    "        )\n",
    "        \n",
    "        X_enc[f'{col}_target'] = X[col].map(target_means['target_enc']).fillna(global_mean)\n",
    "        X_enc = X_enc.drop(columns=[col])\n",
    "    \n",
    "    return X_enc\n",
    "\n",
    "\n",
    "# ✅ Apply target encoding - PASS COMPLETE DATAFRAMES\n",
    "print(\"\\nTarget encoding TRAIN and TEST (with CV)...\")\n",
    "X_train_init, X_test_init = target_encode_cv(\n",
    "    train_fe,  # ✅ Pass complete DataFrame (includes categoricals)\n",
    "    test_fe,   # ✅ Pass complete DataFrame\n",
    "    pd.Series(y, name='exam_score'),\n",
    "    cat_features,\n",
    "    n_splits=5,\n",
    "    alpha=10\n",
    ")\n",
    "\n",
    "print(\"\\nTarget encoding ORIGINAL (simple, no CV)...\")\n",
    "X_orig_init = target_encode_simple(\n",
    "    original_fe,  # ✅ Pass complete DataFrame\n",
    "    pd.Series(y_orig, name='exam_score'),\n",
    "    cat_features,\n",
    "    alpha=10\n",
    ")\n",
    "\n",
    "# ✅ NOW remove low-impact categoricals + ID + TARGET\n",
    "cols_to_remove = [ID_COL, TARGET] + low_impact_cats\n",
    "feature_cols = [c for c in X_train_init.columns if c not in cols_to_remove]\n",
    "\n",
    "X_train_init = X_train_init[feature_cols]\n",
    "X_test_init = X_test_init[feature_cols]\n",
    "X_orig_init = X_orig_init[feature_cols]\n",
    "\n",
    "# Convert to float32\n",
    "X_train_init = X_train_init.astype(np.float32)\n",
    "X_test_init = X_test_init.astype(np.float32)\n",
    "X_orig_init = X_orig_init.astype(np.float32)\n",
    "\n",
    "print(f\"\\n✓ Target encoding complete. Shape: {X_train_init.shape}\")\n",
    "print(f\"✓ Features after cleanup: {len(feature_cols)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4.5. FEATURE SELECTION (OPTIONAL with ~40 features)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Training quick LightGBM for feature importance...\")\n",
    "\n",
    "X_temp = X_train_init.copy()\n",
    "y_temp = y.copy()\n",
    "\n",
    "kf_temp = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "feature_importance = np.zeros(X_temp.shape[1])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf_temp.split(X_temp)):\n",
    "    X_tr, X_val = X_temp.iloc[train_idx], X_temp.iloc[val_idx]\n",
    "    y_tr, y_val = y_temp[train_idx], y_temp[val_idx]\n",
    "\n",
    "    lgb_params_quick = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'device': 'gpu' if USE_GPU else 'cpu',\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "        lgb_params_quick, train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    "    )\n",
    "\n",
    "    feature_importance += model.feature_importance(importance_type='gain')\n",
    "    print(f\"  Fold {fold+1}/3 done\")\n",
    "\n",
    "# Average importance\n",
    "feature_importance /= 3\n",
    "\n",
    "# Create DataFrame with importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_temp.columns,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Calculate cumulative importance\n",
    "importance_df['cumulative_importance'] = importance_df['importance'].cumsum() / importance_df['importance'].sum()\n",
    "\n",
    "print(f\"\\nTotal features: {len(importance_df)}\")\n",
    "print(f\"\\nTop 20 most important features:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Feature selection strategy\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SELECTION STRATEGY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Given we have {len(feature_cols)} well-designed features,\")\n",
    "print(f\"we'll use a CONSERVATIVE threshold to keep most features.\")\n",
    "print()\n",
    "\n",
    "# Choose strategy\n",
    "USE_FEATURE_SELECTION = True  # Set to False to keep all features\n",
    "\n",
    "if USE_FEATURE_SELECTION:\n",
    "    # Keep features covering 98% of importance\n",
    "    threshold = 0.98\n",
    "    selected_features = importance_df[importance_df['cumulative_importance'] <= threshold]['feature'].tolist()\n",
    "    \n",
    "    # Ensure at least 3 features\n",
    "    if len(selected_features) < 3:\n",
    "        selected_features = importance_df.head(3)['feature'].tolist()\n",
    "else:\n",
    "    selected_features = feature_cols\n",
    "    print(\"Keeping ALL features (no selection)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FEATURE SELECTION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "print(f\"Reduction: {100*(1 - len(selected_features)/len(feature_cols)):.1f}%\")\n",
    "\n",
    "# Update datasets\n",
    "X_train_init = X_train_init[selected_features]\n",
    "X_test_init = X_test_init[selected_features]\n",
    "X_orig_init = X_orig_init[selected_features]\n",
    "feature_cols = selected_features\n",
    "\n",
    "print(f\"\\n✓ Feature selection complete!\")\n",
    "print(f\"✓ Final dataset shape: {X_train_init.shape}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.to_csv('../results/feature_importance.csv', index=False)\n",
    "print(\"✓ Feature importance saved to feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2140e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BAYESIAN HYPERPARAMETER OPTIMIZATION - ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "[1/8] Optimizing LightGBM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87726f43e08b4b978e286b4be8dfc21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's rmse: 8.98253\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[223]\tvalid_0's rmse: 9.96857\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid_0's rmse: 8.32107\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[160]\tvalid_0's rmse: 9.40645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[338]\tvalid_0's rmse: 9.00805\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[408]\tvalid_0's rmse: 9.88616\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[313]\tvalid_0's rmse: 8.23923\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[333]\tvalid_0's rmse: 9.50445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's rmse: 9.2423\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's rmse: 9.94467\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's rmse: 8.21376\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's rmse: 9.47721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's rmse: 9.05176\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's rmse: 10.0134\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's rmse: 8.23223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's rmse: 9.38609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's rmse: 9.17008\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[231]\tvalid_0's rmse: 10.0114\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[299]\tvalid_0's rmse: 8.26601\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[344]\tvalid_0's rmse: 9.42431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's rmse: 9.29083\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's rmse: 9.99669\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's rmse: 8.37574\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's rmse: 9.62772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's rmse: 9.15659\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[110]\tvalid_0's rmse: 9.98535\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's rmse: 8.1857\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's rmse: 9.56786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's rmse: 9.06141\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid_0's rmse: 9.87331\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's rmse: 8.38651\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's rmse: 9.35901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's rmse: 9.2807\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's rmse: 9.89368\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's rmse: 8.63345\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's rmse: 9.60982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[228]\tvalid_0's rmse: 8.98457\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[308]\tvalid_0's rmse: 9.88538\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[241]\tvalid_0's rmse: 8.10912\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[263]\tvalid_0's rmse: 9.57178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid_0's rmse: 9.1375\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's rmse: 10.1233\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's rmse: 8.46356\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[256]\tvalid_0's rmse: 9.38502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[332]\tvalid_0's rmse: 8.996\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[287]\tvalid_0's rmse: 9.99356\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[353]\tvalid_0's rmse: 8.25193\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[373]\tvalid_0's rmse: 9.43241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's rmse: 9.02059\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[133]\tvalid_0's rmse: 9.94194\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[266]\tvalid_0's rmse: 8.17917\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's rmse: 9.46193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's rmse: 9.12269\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[165]\tvalid_0's rmse: 9.94514\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's rmse: 8.23101\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's rmse: 9.31829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid_0's rmse: 9.06484\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's rmse: 9.98803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[160]\tvalid_0's rmse: 8.51307\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid_0's rmse: 9.41905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[139]\tvalid_0's rmse: 9.19816\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[151]\tvalid_0's rmse: 9.91738\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[253]\tvalid_0's rmse: 8.08714\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[212]\tvalid_0's rmse: 9.35018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's rmse: 9.20022\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's rmse: 10.2279\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's rmse: 8.43553\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's rmse: 9.34916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[190]\tvalid_0's rmse: 9.21228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid_0's rmse: 10.0163\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[157]\tvalid_0's rmse: 8.27148\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid_0's rmse: 9.57489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's rmse: 9.12376\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[138]\tvalid_0's rmse: 9.91018\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid_0's rmse: 8.38462\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's rmse: 9.35022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[192]\tvalid_0's rmse: 9.25249\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's rmse: 10.043\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[187]\tvalid_0's rmse: 8.51522\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[227]\tvalid_0's rmse: 9.61603\n",
      "✓ Best LightGBM RMSE: 9.137712\n",
      "\n",
      "[2/8] Optimizing XGBoost...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e523ca0acd4c0797c48fcabcc89583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best XGBoost RMSE: 9.228942\n",
      "\n",
      "[3/8] Optimizing CatBoost...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5699298bd4e34564bd74ead6fc364ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best CatBoost RMSE: 9.116181\n",
      "\n",
      "[4/8] Optimizing ExtraTrees...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345d0c783bb74eba89fdb03973d5a53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best ExtraTrees RMSE: 9.509916\n",
      "\n",
      "[5/8] Optimizing GradientBoosting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc56012a3a2b44e9be1dc1905a58a07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best GradientBoosting RMSE: 9.218248\n",
      "\n",
      "[6/8] Optimizing Ridge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a42a435a174c6cb3c67743e00652ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best Ridge RMSE: 8.935643\n",
      "\n",
      "[7/8] Optimizing ElasticNet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1284ed7284c44ce2ba219f3b017c918f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best ElasticNet RMSE: 8.945330\n",
      "\n",
      "[8/8] Optimizing SVR (using subsampling for speed)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643ae46f89c14785a87efbf39936f62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best SVR RMSE: 9.529466\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION SUMMARY\n",
      "================================================================================\n",
      "           Model Best RMSE  Trials\n",
      "        LightGBM  9.137712      20\n",
      "         XGBoost  9.228942      25\n",
      "        CatBoost  9.116181      10\n",
      "      ExtraTrees  9.509916       5\n",
      "GradientBoosting  9.218248       5\n",
      "           Ridge  8.935643      15\n",
      "      ElasticNet  8.945330      15\n",
      "             SVR  9.529466       5\n",
      "\n",
      "✓ All hyperparameters saved in best_params_dict\n",
      "✓ Total optimization time: 78.7s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. BAYESIAN HYPERPARAMETER OPTIMIZATION - ALL MODELS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BAYESIAN HYPERPARAMETER OPTIMIZATION - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# VELOCITÀ vs PRECISIONE trade-off\n",
    "FAST_MODE = True\n",
    "\n",
    "if FAST_MODE:\n",
    "    trials_config = {\n",
    "        'lgb': 20,      # era 40\n",
    "        'xgb': 25,      # era 50\n",
    "        'cat': 10,      # era 10\n",
    "        'et': 5,        # era 5\n",
    "        'gbr': 5,       # era 10\n",
    "        'ridge': 15,    # era 30\n",
    "        'elastic': 15,  # era 30\n",
    "        'svr': 5        # era 10\n",
    "    }\n",
    "else:\n",
    "    trials_config = {\n",
    "        'lgb': 50, 'xgb': 50, 'cat': 20,\n",
    "        'et': 20, 'gbr': 20, 'ridge': 40,\n",
    "        'elastic': 40, 'svr': 20\n",
    "    }\n",
    "\n",
    "# Numero di folds\n",
    "FOLDS = 4\n",
    "\n",
    "# Dictionary to store all best parameters\n",
    "best_params_dict = {}\n",
    "\n",
    "# --- LIGHTGBM ---\n",
    "def objective_lgb(trial, X, y):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'device': 'gpu' if USE_GPU else 'cpu',\n",
    "        'gpu_use_dp': False if USE_GPU else None,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0, 1.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0, 1.0),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 1.0),\n",
    "    }\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    # Progress bar per i fold\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), \n",
    "                                                           total=FOLDS, \n",
    "                                                           desc=f\"Trial {trial.number}\", \n",
    "                                                           leave=False)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params, train_data, num_boost_round=1000,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "\n",
    "        preds = np.clip(model.predict(X_val), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\n[1/8] Optimizing LightGBM...\")\n",
    "study_lgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_lgb.optimize(lambda trial: objective_lgb(trial, X_train_init, y), \n",
    "                   n_trials=trials_config['lgb'], \n",
    "                   show_progress_bar=True, \n",
    "                   n_jobs=1)\n",
    "best_params_dict['lightgbm'] = study_lgb.best_params\n",
    "print(f\"✓ Best LightGBM RMSE: {study_lgb.best_value:.6f}\")\n",
    "\n",
    "# --- XGBOOST ---\n",
    "def objective_xgb(trial, X, y):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda:0' if USE_GPU else 'cpu',\n",
    "        'predictor': 'gpu_predictor' if USE_GPU else 'cpu_predictor',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0, 2.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0, 2.0),\n",
    "        'seed': 42\n",
    "    }\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), \n",
    "                                                           total=FOLDS, \n",
    "                                                           desc=f\"Trial {trial.number}\", \n",
    "                                                           leave=False)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        model = xgb.train(\n",
    "            params, dtrain, num_boost_round=1000,\n",
    "            evals=[(dval, 'eval')],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        preds = np.clip(model.predict(dval), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\n[2/8] Optimizing XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_xgb.optimize(lambda trial: objective_xgb(trial, X_train_init, y), \n",
    "                   n_trials=trials_config['xgb'], \n",
    "                   show_progress_bar=True, \n",
    "                   n_jobs=1)\n",
    "best_params_dict['xgboost'] = study_xgb.best_params\n",
    "print(f\"✓ Best XGBoost RMSE: {study_xgb.best_value:.6f}\")\n",
    "\n",
    "# --- CATBOOST ---\n",
    "def objective_cat(trial, X, y):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 1000, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 2),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'task_type': 'GPU' if USE_GPU else 'CPU',\n",
    "        'devices': '0' if USE_GPU else None,\n",
    "        'verbose': False,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), \n",
    "                                                           total=FOLDS, \n",
    "                                                           desc=f\"Trial {trial.number}\", \n",
    "                                                           leave=False)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "        preds = np.clip(model.predict(X_val), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\n[3/8] Optimizing CatBoost...\")\n",
    "study_cat = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_cat.optimize(lambda trial: objective_cat(trial, X_train_init, y), \n",
    "                   n_trials=trials_config['cat'], \n",
    "                   show_progress_bar=True, \n",
    "                   n_jobs=1)\n",
    "best_params_dict['catboost'] = study_cat.best_params\n",
    "print(f\"✓ Best CatBoost RMSE: {study_cat.best_value:.6f}\")\n",
    "\n",
    "# --- EXTRATREES ---\n",
    "def objective_et(trial, X, y):\n",
    "    # Subsample per velocità\n",
    "    sample_size = int(len(X) * 0.3)\n",
    "    idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "    X_sample = X.iloc[idx]\n",
    "    y_sample = y[idx]\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 10, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 4, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.6, 0.9),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X_sample), \n",
    "                                                           total=FOLDS, \n",
    "                                                           desc=f\"Trial {trial.number}\", \n",
    "                                                           leave=False)):\n",
    "        X_tr, X_val = X_sample.iloc[train_idx], X_sample.iloc[val_idx]\n",
    "        y_tr, y_val = y_sample[train_idx], y_sample[val_idx]\n",
    "\n",
    "        model = ExtraTreesRegressor(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        preds = np.clip(model.predict(X_val), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\n[4/8] Optimizing ExtraTrees...\")\n",
    "study_et = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_et.optimize(lambda trial: objective_et(trial, X_train_init, y), \n",
    "                  n_trials=trials_config['et'], \n",
    "                  show_progress_bar=True, \n",
    "                  n_jobs=1)\n",
    "best_params_dict['extratrees'] = study_et.best_params\n",
    "print(f\"✓ Best ExtraTrees RMSE: {study_et.best_value:.6f}\")\n",
    "\n",
    "# --- GRADIENT BOOSTING ---\n",
    "def objective_gbr(trial, X, y):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 10, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "        'max_features': trial.suggest_float('max_features', 0.6, 0.9),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), \n",
    "                                                           total=FOLDS, \n",
    "                                                           desc=f\"Trial {trial.number}\", \n",
    "                                                           leave=False)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        preds = np.clip(model.predict(X_val), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\n[5/8] Optimizing GradientBoosting...\")\n",
    "study_gbr = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_gbr.optimize(lambda trial: objective_gbr(trial, X_train_init, y), \n",
    "                   n_trials=trials_config['gbr'], \n",
    "                   show_progress_bar=True, \n",
    "                   n_jobs=4)\n",
    "best_params_dict['gradientboosting'] = study_gbr.best_params\n",
    "print(f\"✓ Best GradientBoosting RMSE: {study_gbr.best_value:.6f}\")\n",
    "\n",
    "# --- RIDGE ---\n",
    "def objective_ridge(trial, X, y):\n",
    "    alpha = trial.suggest_float('alpha', 0.1, 100, log=True)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), \n",
    "                                                           total=FOLDS, \n",
    "                                                           desc=f\"Trial {trial.number}\", \n",
    "                                                           leave=False)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        model = Ridge(alpha=alpha, random_state=42)\n",
    "        model.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "        preds = np.clip(model.predict(X_val_scaled), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\n[6/8] Optimizing Ridge...\")\n",
    "study_ridge = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_ridge.optimize(lambda trial: objective_ridge(trial, X_train_init, y), \n",
    "                     n_trials=trials_config['ridge'], \n",
    "                     show_progress_bar=True, \n",
    "                     n_jobs=4)\n",
    "best_params_dict['ridge'] = {'alpha': study_ridge.best_params['alpha']}\n",
    "print(f\"✓ Best Ridge RMSE: {study_ridge.best_value:.6f}\")\n",
    "\n",
    "# --- ELASTICNET ---\n",
    "def objective_elastic(trial, X, y):\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 10, log=True)\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), \n",
    "                                                           total=FOLDS, \n",
    "                                                           desc=f\"Trial {trial.number}\", \n",
    "                                                           leave=False)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=5000, random_state=42)\n",
    "        model.fit(X_tr_scaled, y_tr)\n",
    "\n",
    "        preds = np.clip(model.predict(X_val_scaled), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\n[7/8] Optimizing ElasticNet...\")\n",
    "study_elastic = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_elastic.optimize(lambda trial: objective_elastic(trial, X_train_init, y), \n",
    "                       n_trials=trials_config['elastic'], \n",
    "                       show_progress_bar=True, \n",
    "                       n_jobs=4)\n",
    "best_params_dict['elasticnet'] = study_elastic.best_params\n",
    "print(f\"✓ Best ElasticNet RMSE: {study_elastic.best_value:.6f}\")\n",
    "\n",
    "# --- SVR ---\n",
    "def objective_svr(trial, X, y):\n",
    "    C = trial.suggest_float('C', 1, 100, log=True)\n",
    "    epsilon = trial.suggest_float('epsilon', 0.01, 1.0)\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), \n",
    "                                                           total=FOLDS, \n",
    "                                                           desc=f\"Trial {trial.number}\", \n",
    "                                                           leave=False)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # Subsample per velocità (SVR è lento)\n",
    "        sample_size = min(50000, len(X_tr))\n",
    "        idx = np.random.choice(len(X_tr), sample_size, replace=False)\n",
    "\n",
    "        model = SVR(C=C, epsilon=epsilon, gamma=gamma, kernel='rbf')\n",
    "        model.fit(X_tr_scaled[idx], y_tr[idx])\n",
    "\n",
    "        preds = np.clip(model.predict(X_val_scaled), 0, 100)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        scores.append(rmse)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\n[8/8] Optimizing SVR (using subsampling for speed)...\")\n",
    "study_svr = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_svr.optimize(lambda trial: objective_svr(trial, X_train_init, y), \n",
    "                   n_trials=trials_config['svr'], \n",
    "                   show_progress_bar=True, \n",
    "                   n_jobs=2)\n",
    "best_params_dict['svr'] = study_svr.best_params\n",
    "print(f\"✓ Best SVR RMSE: {study_svr.best_value:.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for model_name, study in [\n",
    "    ('LightGBM', study_lgb),\n",
    "    ('XGBoost', study_xgb),\n",
    "    ('CatBoost', study_cat),\n",
    "    ('ExtraTrees', study_et),\n",
    "    ('GradientBoosting', study_gbr),\n",
    "    ('Ridge', study_ridge),\n",
    "    ('ElasticNet', study_elastic),\n",
    "    ('SVR', study_svr)\n",
    "]:\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Best RMSE': f\"{study.best_value:.6f}\",\n",
    "        'Trials': len(study.trials)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n✓ All hyperparameters saved in best_params_dict\")\n",
    "print(f\"✓ Total optimization time: {time.time() - start_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb143ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING BEST PARAMETERS\n",
      "================================================================================\n",
      "✓ Saved to: results/best_hyperparameters.json\n",
      "✓ Saved to: best_hyperparameters.csv\n",
      "\n",
      "Best Parameters Summary:\n",
      "\n",
      "LIGHTGBM:\n",
      "  learning_rate: 0.020763482134447155\n",
      "  num_leaves: 49\n",
      "  max_depth: 9\n",
      "  min_child_samples: 33\n",
      "  feature_fraction: 0.8698385129840963\n",
      "  bagging_fraction: 0.7728284587275367\n",
      "  bagging_freq: 4\n",
      "  lambda_l1: 0.42754101835854963\n",
      "  lambda_l2: 0.02541912674409519\n",
      "  min_gain_to_split: 0.10789142699330445\n",
      "\n",
      "XGBOOST:\n",
      "  learning_rate: 0.025691100161474374\n",
      "  max_depth: 4\n",
      "  min_child_weight: 5\n",
      "  subsample: 0.6750300724595839\n",
      "  colsample_bytree: 0.7080589128703798\n",
      "  gamma: 0.34569374524826657\n",
      "  lambda: 1.4740680461408981\n",
      "  alpha: 1.197143151579027\n",
      "\n",
      "CATBOOST:\n",
      "  iterations: 2617\n",
      "  learning_rate: 0.037415239225603365\n",
      "  depth: 4\n",
      "  l2_leaf_reg: 7.158097238609412\n",
      "  bagging_temperature: 0.4401524937396013\n",
      "  random_strength: 0.24407646968955765\n",
      "  border_count: 142\n",
      "\n",
      "EXTRATREES:\n",
      "  n_estimators: 142\n",
      "  max_depth: 10\n",
      "  min_samples_split: 13\n",
      "  min_samples_leaf: 6\n",
      "  max_features: 0.7368209952651108\n",
      "\n",
      "GRADIENTBOOSTING:\n",
      "  n_estimators: 127\n",
      "  learning_rate: 0.056327369591044354\n",
      "  max_depth: 3\n",
      "  min_samples_split: 14\n",
      "  min_samples_leaf: 8\n",
      "  subsample: 0.8974311531059256\n",
      "  max_features: 0.7358600104346266\n",
      "\n",
      "RIDGE:\n",
      "  alpha: 0.3391998205549459\n",
      "\n",
      "ELASTICNET:\n",
      "  alpha: 0.010399510317816697\n",
      "  l1_ratio: 0.6767848682861811\n",
      "\n",
      "SVR:\n",
      "  C: 11.782477479554705\n",
      "  epsilon: 0.48400693985987375\n",
      "  gamma: scale\n",
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER OPTIMIZATION COMPLETE\n",
      "================================================================================\n",
      "✓ All 8 models optimized\n",
      "✓ Parameters saved to JSON and CSV\n",
      "\n",
      "You can now proceed to Part 2 for training and ensemble creation.\n",
      "================================================================================\n",
      "PART 2: MODEL TRAINING & ENSEMBLE\n",
      "================================================================================\n",
      "✓ Best parameters loaded from results/best_hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. SAVE BEST PARAMETERS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING BEST PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save as JSON\n",
    "with open('../results/best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_params_dict, f, indent=4)\n",
    "print(\"✓ Saved to: results/best_hyperparameters.json\")\n",
    "\n",
    "# Save as CSV for easy viewing\n",
    "params_df = pd.DataFrame([\n",
    "    {'model': model, 'parameter': param, 'value': value}\n",
    "    for model, params in best_params_dict.items()\n",
    "    for param, value in params.items()\n",
    "])\n",
    "params_df.to_csv('../results/best_hyperparameters.csv', index=False)\n",
    "print(\"✓ Saved to: best_hyperparameters.csv\")\n",
    "\n",
    "print(\"\\nBest Parameters Summary:\")\n",
    "for model, params in best_params_dict.items():\n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    for param, value in params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"HYPERPARAMETER OPTIMIZATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"✓ All 8 models optimized\")\n",
    "print(\"✓ Parameters saved to JSON and CSV\")\n",
    "print(\"\\nYou can now proceed to Part 2 for training and ensemble creation.\")\n",
    "\n",
    "# Load best parameters from Part 1\n",
    "with open('../results/best_hyperparameters.json', 'r') as f:\n",
    "    best_params_dict = json.load(f)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 2: MODEL TRAINING & ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Best parameters loaded from results/best_hyperparameters.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73072561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEVEL 1: BASE MODELS TRAINING (8 MODELS)\n",
      "================================================================================\n",
      "\n",
      "Dataset info:\n",
      "  Train (synthetic): 500 samples × 17 features\n",
      "  Original data:     200 samples × 17 features\n",
      "  Test:              100 samples × 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FOLD 1/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 8.58951\n",
      "ElasticNet... RMSE: 8.54865\n",
      "XGBoost... RMSE: 8.69551 (iter: 146)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's rmse: 8.86981\n",
      "RMSE: 8.86981 (iter: 171)\n",
      "CatBoost... RMSE: 8.28395 (iter: 201)\n",
      "ExtraTrees... RMSE: 8.29571\n",
      "GradientBoosting... RMSE: 8.51005\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  10%|█         | 1/10 [00:02<00:20,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 8.52794\n",
      "\n",
      "============================================================\n",
      "FOLD 2/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 9.13556\n",
      "ElasticNet... RMSE: 9.08940\n",
      "XGBoost... RMSE: 10.05524 (iter: 171)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's rmse: 9.8636\n",
      "RMSE: 9.86360 (iter: 166)\n",
      "CatBoost... RMSE: 9.42505 (iter: 142)\n",
      "ExtraTrees... RMSE: 9.84746\n",
      "GradientBoosting... RMSE: 10.08082\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  20%|██        | 2/10 [00:04<00:19,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.40667\n",
      "\n",
      "============================================================\n",
      "FOLD 3/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 8.99441\n",
      "ElasticNet... RMSE: 8.99422\n",
      "XGBoost... RMSE: 9.77154 (iter: 124)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[156]\tvalid_0's rmse: 9.47008\n",
      "RMSE: 9.47008 (iter: 156)\n",
      "CatBoost... RMSE: 9.03379 (iter: 190)\n",
      "ExtraTrees... RMSE: 9.48427\n",
      "GradientBoosting... RMSE: 9.36207\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  30%|███       | 3/10 [00:07<00:16,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.71736\n",
      "\n",
      "============================================================\n",
      "FOLD 4/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 9.06966\n",
      "ElasticNet... RMSE: 9.03164\n",
      "XGBoost... RMSE: 9.56348 (iter: 197)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[239]\tvalid_0's rmse: 9.36093\n",
      "RMSE: 9.36093 (iter: 239)\n",
      "CatBoost... RMSE: 9.00241 (iter: 144)\n",
      "ExtraTrees... RMSE: 9.32460\n",
      "GradientBoosting... RMSE: 9.46273\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  40%|████      | 4/10 [00:09<00:14,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.84722\n",
      "\n",
      "============================================================\n",
      "FOLD 5/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 8.93847\n",
      "ElasticNet... RMSE: 8.98362\n",
      "XGBoost... RMSE: 8.88608 (iter: 156)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's rmse: 8.90413\n",
      "RMSE: 8.90413 (iter: 220)\n",
      "CatBoost... RMSE: 8.80888 (iter: 264)\n",
      "ExtraTrees... RMSE: 9.27934\n",
      "GradientBoosting... RMSE: 8.49708\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  50%|█████     | 5/10 [00:11<00:11,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 8.39061\n",
      "\n",
      "============================================================\n",
      "FOLD 6/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 7.77975\n",
      "ElasticNet... RMSE: 7.86347\n",
      "XGBoost... RMSE: 8.35736 (iter: 177)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[276]\tvalid_0's rmse: 7.965\n",
      "RMSE: 7.96500 (iter: 276)\n",
      "CatBoost... RMSE: 8.27575 (iter: 172)\n",
      "ExtraTrees... RMSE: 8.85814\n",
      "GradientBoosting... RMSE: 8.46885\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  60%|██████    | 6/10 [00:14<00:09,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 8.10540\n",
      "\n",
      "============================================================\n",
      "FOLD 7/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 9.94333\n",
      "ElasticNet... RMSE: 9.89971\n",
      "XGBoost... RMSE: 9.65715 (iter: 124)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's rmse: 9.30108\n",
      "RMSE: 9.30108 (iter: 124)\n",
      "CatBoost... RMSE: 9.61642 (iter: 213)\n",
      "ExtraTrees... RMSE: 9.84408\n",
      "GradientBoosting... RMSE: 9.76726\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  70%|███████   | 7/10 [00:17<00:08,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.51907\n",
      "\n",
      "============================================================\n",
      "FOLD 8/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 9.26583\n",
      "ElasticNet... RMSE: 9.25004\n",
      "XGBoost... RMSE: 9.63488 (iter: 129)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's rmse: 9.64325\n",
      "RMSE: 9.64325 (iter: 178)\n",
      "CatBoost... RMSE: 9.44695 (iter: 131)\n",
      "ExtraTrees... RMSE: 9.25356\n",
      "GradientBoosting... RMSE: 9.75777\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  80%|████████  | 8/10 [00:20<00:05,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.04189\n",
      "\n",
      "============================================================\n",
      "FOLD 9/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 9.87988\n",
      "ElasticNet... RMSE: 9.88644\n",
      "XGBoost... RMSE: 10.47581 (iter: 250)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[284]\tvalid_0's rmse: 10.0702\n",
      "RMSE: 10.07015 (iter: 284)\n",
      "CatBoost... RMSE: 9.91978 (iter: 192)\n",
      "ExtraTrees... RMSE: 10.72676\n",
      "GradientBoosting... RMSE: 9.82333\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  90%|█████████ | 9/10 [00:23<00:02,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.91247\n",
      "\n",
      "============================================================\n",
      "FOLD 10/10\n",
      "============================================================\n",
      "Training size: 650 (450 synthetic + 200 original)\n",
      "Ridge... RMSE: 8.24177\n",
      "ElasticNet... RMSE: 8.23803\n",
      "XGBoost... RMSE: 8.87170 (iter: 263)\n",
      "LightGBM... Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[336]\tvalid_0's rmse: 8.77091\n",
      "RMSE: 8.77091 (iter: 336)\n",
      "CatBoost... RMSE: 8.72947 (iter: 249)\n",
      "ExtraTrees... RMSE: 9.38574\n",
      "GradientBoosting... RMSE: 8.90065\n",
      "SVR... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 10/10 [00:25<00:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 8.66836\n",
      "\n",
      "================================================================================\n",
      "LEVEL 1 OOF SCORES\n",
      "================================================================================\n",
      "RIDGE          : 9.006004\n",
      "ELASTIC        : 8.999318\n",
      "XGB            : 9.418131\n",
      "LGB            : 9.240334\n",
      "CAT            : 9.069280\n",
      "ET             : 9.449621\n",
      "GBR            : 9.281649\n",
      "SVR            : 9.346624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. MULTI-LEVEL STACKING WITH OPTIMIZED MODELS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LEVEL 1: BASE MODELS TRAINING (8 MODELS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "N_FOLDS = 10\n",
    "y_bins = pd.qcut(y, q=10, labels=False, duplicates='drop')\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for OOF and test predictions\n",
    "oof_predictions = {\n",
    "    'ridge': np.zeros(len(y)),\n",
    "    'elastic': np.zeros(len(y)),\n",
    "    'xgb': np.zeros(len(y)),\n",
    "    'lgb': np.zeros(len(y)),\n",
    "    'cat': np.zeros(len(y)),\n",
    "    'et': np.zeros(len(y)),\n",
    "    'gbr': np.zeros(len(y)),\n",
    "    'svr': np.zeros(len(y))\n",
    "}\n",
    "\n",
    "test_predictions = {\n",
    "    'ridge': np.zeros((len(X_test_init), N_FOLDS)),\n",
    "    'elastic': np.zeros((len(X_test_init), N_FOLDS)),\n",
    "    'xgb': np.zeros((len(X_test_init), N_FOLDS)),\n",
    "    'lgb': np.zeros((len(X_test_init), N_FOLDS)),\n",
    "    'cat': np.zeros((len(X_test_init), N_FOLDS)),\n",
    "    'et': np.zeros((len(X_test_init), N_FOLDS)),\n",
    "    'gbr': np.zeros((len(X_test_init), N_FOLDS)),\n",
    "    'svr': np.zeros((len(X_test_init), N_FOLDS))\n",
    "}\n",
    "\n",
    "best_iterations = {\n",
    "    'xgb': [],\n",
    "    'lgb': [],\n",
    "    'cat': []\n",
    "}\n",
    "\n",
    "# Update parameters with fixed settings\n",
    "best_params_dict['lightgbm'].update({\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1,\n",
    "    'device': 'gpu' if USE_GPU else 'cpu',\n",
    "    'gpu_use_dp': False if USE_GPU else None,\n",
    "    'seed': 42\n",
    "})\n",
    "\n",
    "best_params_dict['xgboost'].update({\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda:0' if USE_GPU else 'cpu',\n",
    "    'predictor': 'gpu_predictor' if USE_GPU else 'cpu_predictor',\n",
    "    'seed': 42\n",
    "})\n",
    "\n",
    "best_params_dict['catboost'].update({\n",
    "    'task_type': 'GPU' if USE_GPU else 'CPU',\n",
    "    'devices': '0' if USE_GPU else None,\n",
    "    'verbose': False,\n",
    "    'random_seed': 42\n",
    "})\n",
    "\n",
    "# Remove None values\n",
    "for model in ['lightgbm', 'xgboost', 'catboost']:\n",
    "    best_params_dict[model] = {k: v for k, v in best_params_dict[model].items() if v is not None}\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Train (synthetic): {len(X_train_init):,} samples × {X_train_init.shape[1]} features\")\n",
    "print(f\"  Original data:     {len(X_orig_init):,} samples × {X_orig_init.shape[1]} features\")\n",
    "print(f\"  Test:              {len(X_test_init):,} samples × {X_test_init.shape[1]} features\")\n",
    "\n",
    "# Training loop\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X_train_init, y_bins), \n",
    "                                                   total=N_FOLDS, \n",
    "                                                   desc=\"Training Folds\"), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold}/{N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # ✅ Split usando i dati già target-encodati\n",
    "    X_tr, X_val = X_train_init.iloc[train_idx], X_train_init.iloc[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # ✅ Augment with original data (già target-encodato)\n",
    "    X_tr_full = pd.concat([X_tr, X_orig_init], axis=0, ignore_index=True)\n",
    "    y_tr_full = np.concatenate([y_tr, y_orig])\n",
    "\n",
    "    print(f\"Training size: {len(X_tr_full):,} ({len(X_tr):,} synthetic + {len(X_orig_init):,} original)\")\n",
    "\n",
    "    # --- RIDGE ---\n",
    "    print(\"Ridge...\", end=\" \", flush=True)\n",
    "    scaler_ridge = RobustScaler()\n",
    "    X_tr_scaled = scaler_ridge.fit_transform(X_tr_full)\n",
    "    X_val_scaled = scaler_ridge.transform(X_val)\n",
    "    X_test_scaled = scaler_ridge.transform(X_test_init)\n",
    "\n",
    "    ridge = Ridge(**best_params_dict['ridge'], random_state=42)\n",
    "    ridge.fit(X_tr_scaled, y_tr_full)\n",
    "    oof_predictions['ridge'][val_idx] = np.clip(ridge.predict(X_val_scaled), 0, 100)\n",
    "    test_predictions['ridge'][:, fold-1] = np.clip(ridge.predict(X_test_scaled), 0, 100)\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, oof_predictions['ridge'][val_idx])):.5f}\")\n",
    "\n",
    "    # --- ELASTICNET ---\n",
    "    print(\"ElasticNet...\", end=\" \", flush=True)\n",
    "    elastic = ElasticNet(**best_params_dict['elasticnet'], max_iter=5000, random_state=42)\n",
    "    elastic.fit(X_tr_scaled, y_tr_full)\n",
    "    oof_predictions['elastic'][val_idx] = np.clip(elastic.predict(X_val_scaled), 0, 100)\n",
    "    test_predictions['elastic'][:, fold-1] = np.clip(elastic.predict(X_test_scaled), 0, 100)\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, oof_predictions['elastic'][val_idx])):.5f}\")\n",
    "\n",
    "    # --- XGBOOST ---\n",
    "    print(\"XGBoost...\", end=\" \", flush=True)\n",
    "    dtrain = xgb.DMatrix(X_tr_full, label=y_tr_full)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test_init)\n",
    "\n",
    "    xgb_model = xgb.train(\n",
    "        best_params_dict['xgboost'],\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=[(dval, 'eval')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    best_iterations['xgb'].append(xgb_model.best_iteration)\n",
    "    oof_predictions['xgb'][val_idx] = np.clip(xgb_model.predict(dval), 0, 100)\n",
    "    test_predictions['xgb'][:, fold-1] = np.clip(xgb_model.predict(dtest), 0, 100)\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, oof_predictions['xgb'][val_idx])):.5f} (iter: {xgb_model.best_iteration})\")\n",
    "\n",
    "    # --- LIGHTGBM ---\n",
    "    print(\"LightGBM...\", end=\" \", flush=True)\n",
    "    train_data = lgb.Dataset(X_tr_full, label=y_tr_full)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "    lgb_model = lgb.train(\n",
    "        best_params_dict['lightgbm'],\n",
    "        train_data,\n",
    "        num_boost_round=3000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "    )\n",
    "\n",
    "    best_iterations['lgb'].append(lgb_model.best_iteration)\n",
    "    oof_predictions['lgb'][val_idx] = np.clip(lgb_model.predict(X_val), 0, 100)\n",
    "    test_predictions['lgb'][:, fold-1] = np.clip(lgb_model.predict(X_test_init), 0, 100)\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, oof_predictions['lgb'][val_idx])):.5f} (iter: {lgb_model.best_iteration})\")\n",
    "\n",
    "    # --- CATBOOST ---\n",
    "    print(\"CatBoost...\", end=\" \", flush=True)\n",
    "    cat_model = CatBoostRegressor(**best_params_dict['catboost'])\n",
    "    cat_model.fit(X_tr_full, y_tr_full, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "    best_iterations['cat'].append(cat_model.get_best_iteration())\n",
    "    oof_predictions['cat'][val_idx] = np.clip(cat_model.predict(X_val), 0, 100)\n",
    "    test_predictions['cat'][:, fold-1] = np.clip(cat_model.predict(X_test_init), 0, 100)\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, oof_predictions['cat'][val_idx])):.5f} (iter: {cat_model.get_best_iteration()})\")\n",
    "\n",
    "    # --- EXTRATREES ---\n",
    "    print(\"ExtraTrees...\", end=\" \", flush=True)\n",
    "    et_model = ExtraTreesRegressor(**best_params_dict['extratrees'])\n",
    "    et_model.fit(X_tr_full, y_tr_full)\n",
    "    oof_predictions['et'][val_idx] = np.clip(et_model.predict(X_val), 0, 100)\n",
    "    test_predictions['et'][:, fold-1] = np.clip(et_model.predict(X_test_init), 0, 100)\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, oof_predictions['et'][val_idx])):.5f}\")\n",
    "\n",
    "    # --- GRADIENT BOOSTING ---\n",
    "    print(\"GradientBoosting...\", end=\" \", flush=True)\n",
    "    gbr_model = GradientBoostingRegressor(**best_params_dict['gradientboosting'])\n",
    "    gbr_model.fit(X_tr_full, y_tr_full)\n",
    "    oof_predictions['gbr'][val_idx] = np.clip(gbr_model.predict(X_val), 0, 100)\n",
    "    test_predictions['gbr'][:, fold-1] = np.clip(gbr_model.predict(X_test_init), 0, 100)\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, oof_predictions['gbr'][val_idx])):.5f}\")\n",
    "\n",
    "    # --- SVR (with subsampling for speed) ---\n",
    "    print(\"SVR...\", end=\" \", flush=True)\n",
    "    sample_size = min(100000, len(X_tr_full))\n",
    "    if len(X_tr_full) > sample_size:\n",
    "        idx = np.random.choice(len(X_tr_full), sample_size, replace=False)\n",
    "        X_tr_sample = X_tr_full.iloc[idx]\n",
    "        y_tr_sample = y_tr_full[idx]\n",
    "    else:\n",
    "        X_tr_sample = X_tr_full\n",
    "        y_tr_sample = y_tr_full\n",
    "\n",
    "    scaler_svr = StandardScaler()\n",
    "    X_tr_svr = scaler_svr.fit_transform(X_tr_sample)\n",
    "    X_val_svr = scaler_svr.transform(X_val)\n",
    "    X_test_svr = scaler_svr.transform(X_test_init)\n",
    "\n",
    "    svr_model = SVR(**best_params_dict['svr'], kernel='rbf')\n",
    "    svr_model.fit(X_tr_svr, y_tr_sample)\n",
    "    oof_predictions['svr'][val_idx] = np.clip(svr_model.predict(X_val_svr), 0, 100)\n",
    "    test_predictions['svr'][:, fold-1] = np.clip(svr_model.predict(X_test_svr), 0, 100)\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_val, oof_predictions['svr'][val_idx])):.5f}\")\n",
    "\n",
    "# Level 1 OOF scores\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LEVEL 1 OOF SCORES\")\n",
    "print(f\"{'='*80}\")\n",
    "for model_name, oof in oof_predictions.items():\n",
    "    rmse = np.sqrt(mean_squared_error(y, oof))\n",
    "    print(f\"{model_name.upper():15s}: {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "037843b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FULL REFIT: TRAINING ON 100% OF DATA\n",
      "================================================================================\n",
      "\n",
      "Average best iterations from CV:\n",
      "  XGBoost:  173\n",
      "  LightGBM: 215\n",
      "  CatBoost: 189\n",
      "\n",
      "Full training set size: 700 samples\n",
      "\n",
      "Training final models on 100% data...\n",
      "  Ridge... ✓\n",
      "  ElasticNet... ✓\n",
      "  XGBoost... ✓\n",
      "  LightGBM... ✓\n",
      "  CatBoost... ✓\n",
      "  ExtraTrees... ✓\n",
      "  GradientBoosting... ✓\n",
      "  SVR... ✓\n",
      "\n",
      "✓ All models trained on full data!\n",
      "\n",
      "================================================================================\n",
      "SAVING FULL MODELS\n",
      "================================================================================\n",
      "\n",
      "Saving models...\n",
      "  ✓ ridge           → ../models/ridge_full.pkl\n",
      "  ✓ elastic         → ../models/elastic_full.pkl\n",
      "  ✓ xgb             → ../models/xgb_full.pkl\n",
      "  ✓ lgb             → ../models/lgb_full.pkl\n",
      "  ✓ cat             → ../models/cat_full.pkl\n",
      "  ✓ et              → ../models/et_full.pkl\n",
      "  ✓ gbr             → ../models/gbr_full.pkl\n",
      "  ✓ svr             → ../models/svr_full.pkl\n",
      "\n",
      "✓ All models saved successfully in '../models/' directory!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. FULL REFIT ON ALL DATA\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FULL REFIT: TRAINING ON 100% OF DATA\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "avg_best_iter_xgb = int(np.mean(best_iterations['xgb']))\n",
    "avg_best_iter_lgb = int(np.mean(best_iterations['lgb']))\n",
    "avg_best_iter_cat = int(np.mean(best_iterations['cat']))\n",
    "\n",
    "print(f\"\\nAverage best iterations from CV:\")\n",
    "print(f\"  XGBoost:  {avg_best_iter_xgb}\")\n",
    "print(f\"  LightGBM: {avg_best_iter_lgb}\")\n",
    "print(f\"  CatBoost: {avg_best_iter_cat}\")\n",
    "\n",
    "# ✅ Prepare full dataset (già target-encodato)\n",
    "X_full = pd.concat([X_train_init, X_orig_init], axis=0, ignore_index=True)\n",
    "y_full = np.concatenate([y, y_orig])\n",
    "\n",
    "print(f\"\\nFull training set size: {len(X_full):,} samples\")\n",
    "\n",
    "# Storage for full model predictions\n",
    "test_pred_full = {}\n",
    "\n",
    "# Storage for full models\n",
    "full_models = {}\n",
    "\n",
    "print(\"\\nTraining final models on 100% data...\")\n",
    "\n",
    "# Ridge\n",
    "print(\"  Ridge...\", end=\" \", flush=True)\n",
    "scaler_full = RobustScaler()\n",
    "X_full_scaled = scaler_full.fit_transform(X_full)\n",
    "X_test_full_scaled = scaler_full.transform(X_test_init)\n",
    "\n",
    "ridge_full = Ridge(**best_params_dict['ridge'], random_state=42)\n",
    "ridge_full.fit(X_full_scaled, y_full)\n",
    "test_pred_full['ridge'] = np.clip(ridge_full.predict(X_test_full_scaled), 0, 100)\n",
    "full_models['ridge'] = {'model': ridge_full, 'scaler': scaler_full}\n",
    "print(\"✓\")\n",
    "\n",
    "# ElasticNet\n",
    "print(\"  ElasticNet...\", end=\" \", flush=True)\n",
    "elastic_full = ElasticNet(**best_params_dict['elasticnet'], max_iter=5000, random_state=42)\n",
    "elastic_full.fit(X_full_scaled, y_full)\n",
    "test_pred_full['elastic'] = np.clip(elastic_full.predict(X_test_full_scaled), 0, 100)\n",
    "full_models['elastic'] = {'model': elastic_full, 'scaler': scaler_full}\n",
    "print(\"✓\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"  XGBoost...\", end=\" \", flush=True)\n",
    "dtrain_full = xgb.DMatrix(X_full, label=y_full)\n",
    "dtest_full = xgb.DMatrix(X_test_init)\n",
    "xgb_full = xgb.train(best_params_dict['xgboost'], dtrain_full, num_boost_round=avg_best_iter_xgb, verbose_eval=False)\n",
    "test_pred_full['xgb'] = np.clip(xgb_full.predict(dtest_full), 0, 100)\n",
    "full_models['xgb'] = xgb_full\n",
    "print(\"✓\")\n",
    "\n",
    "# LightGBM\n",
    "print(\"  LightGBM...\", end=\" \", flush=True)\n",
    "train_full_lgb = lgb.Dataset(X_full, label=y_full)\n",
    "lgb_full = lgb.train(best_params_dict['lightgbm'], train_full_lgb, num_boost_round=avg_best_iter_lgb, callbacks=[lgb.log_evaluation(0)])\n",
    "test_pred_full['lgb'] = np.clip(lgb_full.predict(X_test_init), 0, 100)\n",
    "full_models['lgb'] = lgb_full\n",
    "print(\"✓\")\n",
    "\n",
    "# CatBoost\n",
    "print(\"  CatBoost...\", end=\" \", flush=True)\n",
    "cat_params_full = best_params_dict['catboost'].copy()\n",
    "cat_params_full['iterations'] = avg_best_iter_cat\n",
    "cat_full = CatBoostRegressor(**cat_params_full)\n",
    "cat_full.fit(X_full, y_full)\n",
    "test_pred_full['cat'] = np.clip(cat_full.predict(X_test_init), 0, 100)\n",
    "full_models['cat'] = cat_full\n",
    "print(\"✓\")\n",
    "\n",
    "# ExtraTrees\n",
    "print(\"  ExtraTrees...\", end=\" \", flush=True)\n",
    "et_full = ExtraTreesRegressor(**best_params_dict['extratrees'])\n",
    "et_full.fit(X_full, y_full)\n",
    "test_pred_full['et'] = np.clip(et_full.predict(X_test_init), 0, 100)\n",
    "full_models['et'] = et_full\n",
    "print(\"✓\")\n",
    "\n",
    "# GradientBoosting\n",
    "print(\"  GradientBoosting...\", end=\" \", flush=True)\n",
    "gbr_full = GradientBoostingRegressor(**best_params_dict['gradientboosting'])\n",
    "gbr_full.fit(X_full, y_full)\n",
    "test_pred_full['gbr'] = np.clip(gbr_full.predict(X_test_init), 0, 100)\n",
    "full_models['gbr'] = gbr_full\n",
    "print(\"✓\")\n",
    "\n",
    "# SVR\n",
    "print(\"  SVR...\", end=\" \", flush=True)\n",
    "sample_size = min(100000, len(X_full))\n",
    "if len(X_full) > sample_size:\n",
    "    idx = np.random.choice(len(X_full), sample_size, replace=False)\n",
    "    X_full_sample = X_full.iloc[idx]\n",
    "    y_full_sample = y_full[idx]\n",
    "else:\n",
    "    X_full_sample = X_full\n",
    "    y_full_sample = y_full\n",
    "\n",
    "scaler_svr_full = StandardScaler()\n",
    "X_full_svr = scaler_svr_full.fit_transform(X_full_sample)\n",
    "X_test_svr = scaler_svr_full.transform(X_test_init)\n",
    "\n",
    "svr_full = SVR(**best_params_dict['svr'], kernel='rbf')\n",
    "svr_full.fit(X_full_svr, y_full_sample)\n",
    "test_pred_full['svr'] = np.clip(svr_full.predict(X_test_svr), 0, 100)\n",
    "full_models['svr'] = {'model': svr_full, 'scaler': scaler_svr_full}\n",
    "print(\"✓\")\n",
    "\n",
    "print(\"\\n✓ All models trained on full data!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 💾 SAVE FULL MODELS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVING FULL MODELS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Create directory for models\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "print(\"\\nSaving models...\")\n",
    "for model_name, model_obj in full_models.items():\n",
    "    filepath = f'../models/{model_name}_full.pkl'\n",
    "    joblib.dump(model_obj, filepath)\n",
    "    print(f\"  ✓ {model_name:15s} → {filepath}\")\n",
    "\n",
    "print(\"\\n✓ All models saved successfully in '../models/' directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4ddde6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LEVEL 2: META-LEARNER STACKING\n",
      "================================================================================\n",
      "Meta-features shape: (500, 12)\n",
      "Meta-learner Fold 1/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[315]\tvalid_0's rmse: 8.55687\n",
      "✓\n",
      "Meta-learner Fold 2/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[302]\tvalid_0's rmse: 9.13258\n",
      "✓\n",
      "Meta-learner Fold 3/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid_0's rmse: 9.11603\n",
      "✓\n",
      "Meta-learner Fold 4/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[460]\tvalid_0's rmse: 9.82911\n",
      "✓\n",
      "Meta-learner Fold 5/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[384]\tvalid_0's rmse: 9.00089\n",
      "✓\n",
      "Meta-learner Fold 6/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[335]\tvalid_0's rmse: 8.10021\n",
      "✓\n",
      "Meta-learner Fold 7/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[230]\tvalid_0's rmse: 9.89052\n",
      "✓\n",
      "Meta-learner Fold 8/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[302]\tvalid_0's rmse: 10.1825\n",
      "✓\n",
      "Meta-learner Fold 9/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's rmse: 9.726\n",
      "✓\n",
      "Meta-learner Fold 10/10... Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[315]\tvalid_0's rmse: 8.98702\n",
      "✓\n",
      "\n",
      "================================================================================\n",
      "LEVEL 2 META-LEARNER OOF SCORES\n",
      "================================================================================\n",
      "Ridge Meta:    9.043043\n",
      "LightGBM Meta: 9.272719\n",
      "XGBoost Meta:  9.248655\n",
      "\n",
      "================================================================================\n",
      "SAVING META-LEARNER MODELS\n",
      "================================================================================\n",
      "  ✓ All meta-learner models saved → models/meta_models.pkl\n",
      "  ✓ Meta-learner info saved → models/meta_info.pkl\n",
      "\n",
      "✓ All meta-learner models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. LEVEL 2: META-LEARNER STACKING\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LEVEL 2: META-LEARNER STACKING\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create meta-features\n",
    "meta_train = np.column_stack(list(oof_predictions.values()))\n",
    "meta_test = np.column_stack(list(test_pred_full.values()))\n",
    "\n",
    "# Add statistical features\n",
    "meta_train_enhanced = np.column_stack([\n",
    "    meta_train,\n",
    "    meta_train.mean(axis=1),\n",
    "    meta_train.std(axis=1),\n",
    "    meta_train.max(axis=1),\n",
    "    meta_train.min(axis=1)\n",
    "])\n",
    "\n",
    "meta_test_enhanced = np.column_stack([\n",
    "    meta_test,\n",
    "    meta_test.mean(axis=1),\n",
    "    meta_test.std(axis=1),\n",
    "    meta_test.max(axis=1),\n",
    "    meta_test.min(axis=1)\n",
    "])\n",
    "\n",
    "print(f\"Meta-features shape: {meta_train_enhanced.shape}\")\n",
    "\n",
    "# Train meta-learners\n",
    "oof_meta_ridge = np.zeros(len(y))\n",
    "oof_meta_lgb = np.zeros(len(y))\n",
    "oof_meta_xgb = np.zeros(len(y))\n",
    "\n",
    "test_meta_ridge = np.zeros((len(test_fe), N_FOLDS))\n",
    "test_meta_lgb = np.zeros((len(test_fe), N_FOLDS))\n",
    "test_meta_xgb = np.zeros((len(test_fe), N_FOLDS))\n",
    "\n",
    "# Storage for meta-learner models\n",
    "meta_models = {\n",
    "    'ridge': [],\n",
    "    'lgb': [],\n",
    "    'xgb': [],\n",
    "    'scalers': []\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(meta_train_enhanced, y_bins), 1):\n",
    "    print(f\"Meta-learner Fold {fold}/{N_FOLDS}...\", end=\" \")\n",
    "\n",
    "    X_tr_meta, X_val_meta = meta_train_enhanced[train_idx], meta_train_enhanced[val_idx]\n",
    "    y_tr_meta, y_val_meta = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Ridge meta\n",
    "    scaler_meta = StandardScaler()\n",
    "    X_tr_meta_scaled = scaler_meta.fit_transform(X_tr_meta)\n",
    "    X_val_meta_scaled = scaler_meta.transform(X_val_meta)\n",
    "    X_test_meta_scaled = scaler_meta.transform(meta_test_enhanced)\n",
    "\n",
    "    ridge_meta = Ridge(alpha=1.0, random_state=42)\n",
    "    ridge_meta.fit(X_tr_meta_scaled, y_tr_meta)\n",
    "    oof_meta_ridge[val_idx] = np.clip(ridge_meta.predict(X_val_meta_scaled), 0, 100)\n",
    "    test_meta_ridge[:, fold-1] = np.clip(ridge_meta.predict(X_test_meta_scaled), 0, 100)\n",
    "    \n",
    "    # Save Ridge meta model and scaler\n",
    "    meta_models['ridge'].append(ridge_meta)\n",
    "    meta_models['scalers'].append(scaler_meta)\n",
    "\n",
    "    # LightGBM meta\n",
    "    meta_lgb_params = {\n",
    "        'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.01,\n",
    "        'num_leaves': 15, 'max_depth': 4, 'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8, 'bagging_freq': 5, 'lambda_l1': 1.0,\n",
    "        'lambda_l2': 1.0, 'verbosity': -1, 'seed': 42\n",
    "    }\n",
    "\n",
    "    train_meta_data = lgb.Dataset(X_tr_meta, label=y_tr_meta)\n",
    "    val_meta_data = lgb.Dataset(X_val_meta, label=y_val_meta, reference=train_meta_data)\n",
    "\n",
    "    lgb_meta = lgb.train(\n",
    "        meta_lgb_params, train_meta_data, num_boost_round=1000,\n",
    "        valid_sets=[val_meta_data],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    oof_meta_lgb[val_idx] = np.clip(lgb_meta.predict(X_val_meta), 0, 100)\n",
    "    test_meta_lgb[:, fold-1] = np.clip(lgb_meta.predict(meta_test_enhanced), 0, 100)\n",
    "    \n",
    "    # Save LightGBM meta model\n",
    "    meta_models['lgb'].append(lgb_meta)\n",
    "\n",
    "    # XGBoost meta\n",
    "    dtrain_meta = xgb.DMatrix(X_tr_meta, label=y_tr_meta)\n",
    "    dval_meta = xgb.DMatrix(X_val_meta, label=y_val_meta)\n",
    "    dtest_meta = xgb.DMatrix(meta_test_enhanced)\n",
    "\n",
    "    meta_xgb_params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse',\n",
    "        'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8, 'lambda': 2.0, 'alpha': 1.0, 'seed': 42\n",
    "    }\n",
    "\n",
    "    xgb_meta = xgb.train(\n",
    "        meta_xgb_params, dtrain_meta, num_boost_round=1000,\n",
    "        evals=[(dval_meta, 'eval')],\n",
    "        early_stopping_rounds=50, verbose_eval=False\n",
    "    )\n",
    "    oof_meta_xgb[val_idx] = np.clip(xgb_meta.predict(dval_meta), 0, 100)\n",
    "    test_meta_xgb[:, fold-1] = np.clip(xgb_meta.predict(dtest_meta), 0, 100)\n",
    "    \n",
    "    # Save XGBoost meta model\n",
    "    meta_models['xgb'].append(xgb_meta)\n",
    "\n",
    "    print(f\"✓\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LEVEL 2 META-LEARNER OOF SCORES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Ridge Meta:    {np.sqrt(mean_squared_error(y, oof_meta_ridge)):.6f}\")\n",
    "print(f\"LightGBM Meta: {np.sqrt(mean_squared_error(y, oof_meta_lgb)):.6f}\")\n",
    "print(f\"XGBoost Meta:  {np.sqrt(mean_squared_error(y, oof_meta_xgb)):.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 💾 SAVE META-LEARNER MODELS\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVING META-LEARNER MODELS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save all meta-learner fold models\n",
    "joblib.dump(meta_models, '../models/meta_models.pkl')\n",
    "print(f\"  ✓ All meta-learner models saved → models/meta_models.pkl\")\n",
    "\n",
    "# Save meta-learner parameters for reference\n",
    "meta_info = {\n",
    "    'ridge_params': {'alpha': 1.0, 'random_state': 42},\n",
    "    'lgb_params': meta_lgb_params,\n",
    "    'xgb_params': meta_xgb_params,\n",
    "    'n_folds': N_FOLDS,\n",
    "    'meta_feature_names': list(oof_predictions.keys()) + ['mean', 'std', 'max', 'min']\n",
    "}\n",
    "joblib.dump(meta_info, '../models/meta_info.pkl')\n",
    "print(f\"  ✓ Meta-learner info saved → models/meta_info.pkl\")\n",
    "\n",
    "print(\"\\n✓ All meta-learner models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc7dd6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZING FINAL ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "Optimal weights:\n",
      "  ridge          : 0.2407\n",
      "  elastic        : 0.2249\n",
      "  lgb            : 0.0340\n",
      "  cat            : 0.3410\n",
      "  svr            : 0.1125\n",
      "  xgb_meta       : 0.0413\n",
      "\n",
      "================================================================================\n",
      "FINAL ENSEMBLE PERFORMANCE\n",
      "================================================================================\n",
      "Final OOF RMSE: 8.934629\n",
      "Best single model: 8.999318\n",
      "Improvement: 0.064689\n",
      "\n",
      "================================================================================\n",
      "SAVING ENSEMBLE CONFIGURATION\n",
      "================================================================================\n",
      "  ✓ Ensemble configuration saved → models/ensemble_config.pkl\n",
      "  ✓ Ensemble predictions saved → models/ensemble_predictions.pkl\n",
      "  ✓ Ensemble summary saved → models/ensemble_summary.pkl\n",
      "\n",
      "✓ All ensemble configurations saved successfully!\n",
      "\n",
      "================================================================================\n",
      "SAVED FILES SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Base models (8 models):\n",
      "  ✓ models/ridge_full.pkl\n",
      "  ✓ models/elastic_full.pkl\n",
      "  ✓ models/xgb_full.pkl\n",
      "  ✓ models/lgb_full.pkl\n",
      "  ✓ models/cat_full.pkl\n",
      "  ✓ models/et_full.pkl\n",
      "  ✓ models/gbr_full.pkl\n",
      "  ✓ models/svr_full.pkl\n",
      "\n",
      "Meta-learner models:\n",
      "  ✓ models/meta_models.pkl (Ridge, LightGBM, XGBoost × 10 folds)\n",
      "  ✓ models/meta_info.pkl\n",
      "\n",
      "Ensemble configuration:\n",
      "  ✓ models/ensemble_config.pkl\n",
      "  ✓ models/ensemble_predictions.pkl\n",
      "  ✓ models/ensemble_summary.pkl\n",
      "\n",
      "Parameters and metadata:\n",
      "  ✓ models/best_params.pkl\n",
      "  ✓ models/best_iterations.pkl\n",
      "\n",
      "================================================================================\n",
      "ALL MODELS AND CONFIGURATIONS SAVED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 10. OPTIMAL ENSEMBLE\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OPTIMIZING FINAL ENSEMBLE\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "def ensemble_rmse_final(weights, *args):\n",
    "    oof_preds, y_true = args\n",
    "    weights = np.abs(weights) / np.sum(np.abs(weights))\n",
    "    ensemble = sum(w * pred for w, pred in zip(weights, oof_preds))\n",
    "    ensemble = np.clip(ensemble, 0, 100)\n",
    "    return np.sqrt(mean_squared_error(y_true, ensemble))\n",
    "\n",
    "# Combine all predictions\n",
    "all_oof_preds = list(oof_predictions.values()) + [\n",
    "    oof_meta_ridge, oof_meta_lgb, oof_meta_xgb\n",
    "]\n",
    "\n",
    "all_test_preds = list(test_pred_full.values()) + [\n",
    "    test_meta_ridge.mean(axis=1),\n",
    "    test_meta_lgb.mean(axis=1),\n",
    "    test_meta_xgb.mean(axis=1)\n",
    "]\n",
    "\n",
    "# Optimize weights\n",
    "initial_weights = np.ones(len(all_oof_preds)) / len(all_oof_preds)\n",
    "\n",
    "result = minimize(\n",
    "    ensemble_rmse_final, initial_weights, args=(all_oof_preds, y),\n",
    "    method='Nelder-Mead',\n",
    "    options={'maxiter': 2000, 'xatol': 1e-8, 'fatol': 1e-8}\n",
    ")\n",
    "\n",
    "optimal_weights = np.abs(result.x) / np.sum(np.abs(result.x))\n",
    "\n",
    "print(\"\\nOptimal weights:\")\n",
    "model_names = list(oof_predictions.keys()) + ['ridge_meta', 'lgb_meta', 'xgb_meta']\n",
    "for name, weight in zip(model_names, optimal_weights):\n",
    "    if weight > 0.01:\n",
    "        print(f\"  {name:15s}: {weight:.4f}\")\n",
    "\n",
    "# Create final ensemble\n",
    "final_oof = sum(w * pred for w, pred in zip(optimal_weights, all_oof_preds))\n",
    "final_oof = np.clip(final_oof, 0, 100)\n",
    "\n",
    "final_test = sum(w * pred for w, pred in zip(optimal_weights, all_test_preds))\n",
    "final_test = np.clip(final_test, 0, 100)\n",
    "\n",
    "final_oof_rmse = np.sqrt(mean_squared_error(y, final_oof))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL ENSEMBLE PERFORMANCE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Final OOF RMSE: {final_oof_rmse:.6f}\")\n",
    "\n",
    "best_single = min([np.sqrt(mean_squared_error(y, pred)) for pred in all_oof_preds])\n",
    "print(f\"Best single model: {best_single:.6f}\")\n",
    "print(f\"Improvement: {best_single - final_oof_rmse:.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 💾 SAVE ENSEMBLE CONFIGURATION\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVING ENSEMBLE CONFIGURATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Prepare ensemble configuration\n",
    "ensemble_config = {\n",
    "    'weights': optimal_weights,\n",
    "    'model_names': model_names,\n",
    "    'weights_dict': {name: weight for name, weight in zip(model_names, optimal_weights)},\n",
    "    'performance': {\n",
    "        'final_oof_rmse': final_oof_rmse,\n",
    "        'best_single_model_rmse': best_single,\n",
    "        'improvement': best_single - final_oof_rmse\n",
    "    },\n",
    "    'optimization_result': {\n",
    "        'success': result.success,\n",
    "        'message': result.message,\n",
    "        'n_iterations': result.nit if hasattr(result, 'nit') else None,\n",
    "        'final_value': result.fun\n",
    "    },\n",
    "    'model_order': {\n",
    "        'base_models': list(oof_predictions.keys()),\n",
    "        'meta_models': ['ridge_meta', 'lgb_meta', 'xgb_meta']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save ensemble configuration\n",
    "joblib.dump(ensemble_config, '../models/ensemble_config.pkl')\n",
    "print(f\"  ✓ Ensemble configuration saved → models/ensemble_config.pkl\")\n",
    "\n",
    "# Save final predictions for validation\n",
    "ensemble_predictions = {\n",
    "    'oof_predictions': final_oof,\n",
    "    'test_predictions': final_test,\n",
    "    'y_true': y\n",
    "}\n",
    "joblib.dump(ensemble_predictions, '../models/ensemble_predictions.pkl')\n",
    "print(f\"  ✓ Ensemble predictions saved → models/ensemble_predictions.pkl\")\n",
    "\n",
    "# Create a summary report\n",
    "summary = {\n",
    "    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'n_base_models': len(oof_predictions),\n",
    "    'n_meta_models': 3,\n",
    "    'total_models': len(model_names),\n",
    "    'final_oof_rmse': final_oof_rmse,\n",
    "    'best_single_rmse': best_single,\n",
    "    'improvement': best_single - final_oof_rmse,\n",
    "    'top_5_models': sorted(\n",
    "        [(name, weight) for name, weight in zip(model_names, optimal_weights)],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "}\n",
    "joblib.dump(summary, '../models/ensemble_summary.pkl')\n",
    "print(f\"  ✓ Ensemble summary saved → models/ensemble_summary.pkl\")\n",
    "\n",
    "print(\"\\n✓ All ensemble configurations saved successfully!\")\n",
    "\n",
    "# Print summary of what was saved\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SAVED FILES SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nBase models (8 models):\")\n",
    "for name in oof_predictions.keys():\n",
    "    print(f\"  ✓ models/{name}_full.pkl\")\n",
    "\n",
    "print(\"\\nMeta-learner models:\")\n",
    "print(f\"  ✓ models/meta_models.pkl (Ridge, LightGBM, XGBoost × {N_FOLDS} folds)\")\n",
    "print(f\"  ✓ models/meta_info.pkl\")\n",
    "\n",
    "print(\"\\nEnsemble configuration:\")\n",
    "print(f\"  ✓ models/ensemble_config.pkl\")\n",
    "print(f\"  ✓ models/ensemble_predictions.pkl\")\n",
    "print(f\"  ✓ models/ensemble_summary.pkl\")\n",
    "\n",
    "print(\"\\nParameters and metadata:\")\n",
    "print(f\"  ✓ models/best_params.pkl\")\n",
    "print(f\"  ✓ models/best_iterations.pkl\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL MODELS AND CONFIGURATIONS SAVED!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3706a464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "POST-PROCESSING & CALIBRATION\n",
      "================================================================================\n",
      "Calibrated OOF RMSE: 9.262065\n",
      "✓ Using original predictions\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 11. POST-PROCESSING & CALIBRATION\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"POST-PROCESSING & CALIBRATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "def calibrate_predictions(train_preds, train_true, test_preds):\n",
    "    iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso_reg.fit(train_preds, train_true)\n",
    "    test_calibrated = iso_reg.predict(test_preds)\n",
    "    return np.clip(test_calibrated, 0, 100)\n",
    "\n",
    "# Apply calibration\n",
    "final_test_calibrated = calibrate_predictions(final_oof, y, final_test)\n",
    "\n",
    "# Check if calibration improves\n",
    "final_oof_calibrated = calibrate_predictions(\n",
    "    final_oof[::2], y[::2],\n",
    "    final_oof[1::2]\n",
    ")\n",
    "calibrated_oof = final_oof.copy()\n",
    "calibrated_oof[1::2] = final_oof_calibrated\n",
    "\n",
    "calibrated_rmse = np.sqrt(mean_squared_error(y, calibrated_oof))\n",
    "print(f\"Calibrated OOF RMSE: {calibrated_rmse:.6f}\")\n",
    "\n",
    "if calibrated_rmse < final_oof_rmse:\n",
    "    print(\"✓ Using calibrated predictions\")\n",
    "    final_submission = final_test_calibrated\n",
    "    final_rmse = calibrated_rmse\n",
    "else:\n",
    "    print(\"✓ Using original predictions\")\n",
    "    final_submission = final_test\n",
    "    final_rmse = final_oof_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3006b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Prediction Statistics:\n",
      "  Train target - Mean: 63.27, Std: 18.56, Min: 19.60, Max: 100.00\n",
      "  OOF preds    - Mean: 63.36, Std: 16.19, Min: 23.69, Max: 97.39\n",
      "  Test preds   - Mean: 64.61, Std: 16.92, Min: 32.12, Max: 95.89\n",
      "\n",
      "================================================================================\n",
      "FINAL PERFORMANCE\n",
      "================================================================================\n",
      "✓ Final OOF RMSE: 8.934629\n",
      "✓ Expected LB Score: ~8.9346 (±0.002)\n",
      "\n",
      "🎯 Target beaten: False\n",
      "\n",
      "✓ Submission saved to submission.csv\n",
      "✓ OOF predictions saved to oof_predictions.csv\n",
      "✓ Detailed OOF predictions saved to oof_predictions_detailed.csv\n",
      "✓ Model comparison saved to model_comparison.csv\n",
      "\n",
      "Final RMSE: 8.934629\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 12. FINAL STATISTICS & SUBMISSION\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"  Train target - Mean: {y.mean():.2f}, Std: {y.std():.2f}, Min: {y.min():.2f}, Max: {y.max():.2f}\")\n",
    "print(f\"  OOF preds    - Mean: {final_oof.mean():.2f}, Std: {final_oof.std():.2f}, Min: {final_oof.min():.2f}, Max: {final_oof.max():.2f}\")\n",
    "print(f\"  Test preds   - Mean: {final_submission.mean():.2f}, Std: {final_submission.std():.2f}, Min: {final_submission.min():.2f}, Max: {final_submission.max():.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL PERFORMANCE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ Final OOF RMSE: {final_rmse:.6f}\")\n",
    "print(f\"✓ Expected LB Score: ~{final_rmse:.4f} (±0.002)\")\n",
    "print(f\"\\n🎯 Target beaten: {final_rmse < 8.54414}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'exam_score': final_submission\n",
    "})\n",
    "\n",
    "submission.to_csv('../results/submission.csv', index=False)\n",
    "print(f\"\\n✓ Submission saved to submission.csv\")\n",
    "\n",
    "# Save OOF predictions\n",
    "oof_df = pd.DataFrame({\n",
    "    'id': train_df['id'],\n",
    "    'exam_score': y,\n",
    "    'prediction': final_oof\n",
    "})\n",
    "oof_df.to_csv('../results/oof_predictions.csv', index=False)\n",
    "print(\"✓ OOF predictions saved to oof_predictions.csv\")\n",
    "\n",
    "# Save individual model OOF predictions for analysis\n",
    "oof_detailed = pd.DataFrame({\n",
    "    'id': train_df['id'],\n",
    "    'exam_score': y\n",
    "})\n",
    "for model_name, oof in oof_predictions.items():\n",
    "    oof_detailed[f'oof_{model_name}'] = oof\n",
    "\n",
    "oof_detailed['oof_ridge_meta'] = oof_meta_ridge\n",
    "oof_detailed['oof_lgb_meta'] = oof_meta_lgb\n",
    "oof_detailed['oof_xgb_meta'] = oof_meta_xgb\n",
    "oof_detailed['oof_final_ensemble'] = final_oof\n",
    "\n",
    "oof_detailed.to_csv('../results/oof_predictions_detailed.csv', index=False)\n",
    "print(\"✓ Detailed OOF predictions saved to oof_predictions_detailed.csv\")\n",
    "\n",
    "# Save model comparison\n",
    "model_comparison = pd.DataFrame({\n",
    "    'model': model_names,\n",
    "    'oof_rmse': [np.sqrt(mean_squared_error(y, pred)) for pred in all_oof_preds],\n",
    "    'ensemble_weight': optimal_weights\n",
    "}).sort_values('oof_rmse')\n",
    "\n",
    "model_comparison.to_csv('../results/model_comparison.csv', index=False)\n",
    "print(\"✓ Model comparison saved to model_comparison.csv\")\n",
    "print(f\"\\nFinal RMSE: {final_rmse:.6f}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "students-scores",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
