{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b143982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPU CONFIGURATION\n",
      "================================================================================\n",
      "⚠ No GPU detected, using CPU\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "Train: (630000, 13), Test: (270000, 12), Original: (20000, 13)\n",
      "\n",
      "================================================================================\n",
      "ADVERSARIAL VALIDATION - Detecting Distribution Shift\n",
      "================================================================================\n",
      "Adversarial AUC: 0.4996\n",
      "✓ Distribution shift is minimal\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZED FEATURE ENGINEERING (EDA-DRIVEN)\n",
      "================================================================================\n",
      "\n",
      "Applying optimized feature engineering...\n",
      "  Creating Tier 1: Critical features...\n",
      "  Creating Tier 2: High-impact categorical features...\n",
      "  Creating Tier 3: Categorical × numeric interactions...\n",
      "  Creating Tier 4: Domain-specific flags...\n",
      "  Creating Tier 5: Intelligent binning...\n",
      "  Creating Tier 6: Selective transformations...\n",
      "  Creating Tier 7: Distance from optimal...\n",
      "  Creating Tier 8: Meaningful ratios...\n",
      "  Creating Tier 9: Formula-based features...\n",
      "✓ Feature engineering complete!\n",
      "  Creating Tier 1: Critical features...\n",
      "  Creating Tier 2: High-impact categorical features...\n",
      "  Creating Tier 3: Categorical × numeric interactions...\n",
      "  Creating Tier 4: Domain-specific flags...\n",
      "  Creating Tier 5: Intelligent binning...\n",
      "  Creating Tier 6: Selective transformations...\n",
      "  Creating Tier 7: Distance from optimal...\n",
      "  Creating Tier 8: Meaningful ratios...\n",
      "  Creating Tier 9: Formula-based features...\n",
      "✓ Feature engineering complete!\n",
      "  Creating Tier 1: Critical features...\n",
      "  Creating Tier 2: High-impact categorical features...\n",
      "  Creating Tier 3: Categorical × numeric interactions...\n",
      "  Creating Tier 4: Domain-specific flags...\n",
      "  Creating Tier 5: Intelligent binning...\n",
      "  Creating Tier 6: Selective transformations...\n",
      "  Creating Tier 7: Distance from optimal...\n",
      "  Creating Tier 8: Meaningful ratios...\n",
      "  Creating Tier 9: Formula-based features...\n",
      "✓ Feature engineering complete!\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "High-impact categoricals (for target encoding): ['sleep_quality', 'study_method', 'facility_rating']\n",
      "Low-impact categoricals (will be removed): ['gender', 'course', 'internet_access', 'exam_difficulty']\n",
      "\n",
      "================================================================================\n",
      "TARGET ENCODING ON CATEGORICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      "Target encoding TRAIN and TEST (with CV)...\n",
      "  Encoding sleep_quality...\n",
      "  Encoding study_method...\n",
      "  Encoding facility_rating...\n",
      "\n",
      "Target encoding ORIGINAL (simple, no CV)...\n",
      "  Encoding sleep_quality (simple)...\n",
      "  Encoding study_method (simple)...\n",
      "  Encoding facility_rating (simple)...\n",
      "\n",
      "✓ Target encoding complete. Shape: (630000, 46)\n",
      "✓ Features after cleanup: 46\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "Training quick LightGBM for feature importance...\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 8.81153\n",
      "  Fold 1/3 done\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 8.82093\n",
      "  Fold 2/3 done\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 8.83689\n",
      "  Fold 3/3 done\n",
      "\n",
      "Total features: 46\n",
      "\n",
      "Top 20 most important features:\n",
      "                    feature    importance  cumulative_importance\n",
      "34                  formula  1.198565e+09               0.992622\n",
      "1               study_hours  1.751341e+06               0.994072\n",
      "39          formula_x_study  1.196372e+06               0.995063\n",
      "40         formula_residual  1.156164e+06               0.996020\n",
      "2          class_attendance  1.139073e+06               0.996964\n",
      "30       study_from_optimal  8.537036e+05               0.997671\n",
      "31  attendance_from_optimal  4.837181e+05               0.998071\n",
      "3               sleep_hours  3.438393e+05               0.998356\n",
      "0                       age  2.680490e+05               0.998578\n",
      "6        study_x_attendance  2.372759e+05               0.998774\n",
      "17     study_method_x_hours  2.207950e+05               0.998957\n",
      "15    sleep_quality_x_study  2.142033e+05               0.999135\n",
      "16    facility_x_attendance  1.434039e+05               0.999254\n",
      "41        formula_per_study  1.001976e+05               0.999336\n",
      "42   formula_per_attendance  9.578267e+04               0.999416\n",
      "12         study_method_ord  9.231674e+04               0.999492\n",
      "29       sleep_from_optimal  7.128659e+04               0.999551\n",
      "33           geometric_mean  6.798893e+04               0.999608\n",
      "38     formula_x_efficiency  6.591231e+04               0.999662\n",
      "32          study_per_sleep  5.819355e+04               0.999710\n",
      "\n",
      "============================================================\n",
      "FEATURE SELECTION STRATEGY\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FEATURE SELECTION RESULTS\n",
      "============================================================\n",
      "Original features: 46\n",
      "Selected features: 8\n",
      "Reduction: 82.6%\n",
      "\n",
      "✓ Feature selection complete!\n",
      "✓ Final dataset shape: (630000, 8)\n",
      "\n",
      "================================================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "================================================================================\n",
      "✓ X_train.pkl saved\n",
      "✓ X_test.pkl saved\n",
      "✓ X_orig.pkl saved\n",
      "✓ y_train.pkl saved\n",
      "✓ y_orig.pkl saved\n",
      "✓ feature_names.pkl saved\n",
      "✓ feature_importance.csv saved\n",
      "✓ metadata.pkl saved\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING COMPLETE!\n",
      "================================================================================\n",
      "✓ Train shape: (630000, 8)\n",
      "✓ Test shape: (270000, 8)\n",
      "✓ Original shape: (20000, 8)\n",
      "✓ All files saved in '../data/preprocessed/'\n",
      "\n",
      "You can now proceed to hyperparameter optimization notebooks (02-09).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# =============================================================================\n",
    "# GPU CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"✓ CUDA version: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "    print(f\"✓ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    USE_GPU = True\n",
    "else:\n",
    "    print(\"⚠ No GPU detected, using CPU\")\n",
    "    USE_GPU = False\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "original_df = pd.read_csv(\"../data/Exam_Score_Prediction.csv\")\n",
    "\n",
    "TARGET = \"exam_score\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}, Original: {original_df.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ADVERSARIAL VALIDATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVERSARIAL VALIDATION - Detecting Distribution Shift\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def adversarial_validation(train, test, features):\n",
    "    \"\"\"Identify train-test distribution differences\"\"\"\n",
    "    train_adv = train[features].copy()\n",
    "    test_adv = test[features].copy()\n",
    "\n",
    "    train_adv['is_test'] = 0\n",
    "    test_adv['is_test'] = 1\n",
    "\n",
    "    combined = pd.concat([train_adv, test_adv], axis=0, ignore_index=True)\n",
    "\n",
    "    for col in combined.select_dtypes(include=['object']).columns:\n",
    "        if col != 'is_test':\n",
    "            combined[col] = combined[col].astype('category').cat.codes\n",
    "\n",
    "    X = combined.drop('is_test', axis=1)\n",
    "    y = combined['is_test']\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y, oof_preds)\n",
    "    print(f\"Adversarial AUC: {auc:.4f}\")\n",
    "\n",
    "    if auc > 0.6:\n",
    "        print(\"⚠️  Significant distribution shift detected!\")\n",
    "        model = lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "        model.fit(X, y)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"\\nTop features causing shift:\")\n",
    "        print(importance_df.head(10))\n",
    "    else:\n",
    "        print(\"✓ Distribution shift is minimal\")\n",
    "\n",
    "    return auc\n",
    "\n",
    "base_features = [c for c in train_df.columns if c not in [ID_COL, TARGET]]\n",
    "adv_auc = adversarial_validation(train_df, test_df, base_features)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. OPTIMIZED FEATURE ENGINEERING (EDA-DRIVEN)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZED FEATURE ENGINEERING (EDA-DRIVEN)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_optimized_features(df):\n",
    "    \"\"\"\n",
    "    Create high-impact features based strictly on EDA findings.\n",
    "    Focus: Quality over Quantity - Only features with theoretical justification.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    eps = 1e-5\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 1: CRITICAL FEATURES (Core predictors from EDA)\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 1: Critical features...\")\n",
    "    \n",
    "    # Polynomial features (degree 2 only)\n",
    "    df['study_sq'] = df['study_hours'] ** 2\n",
    "    df['attendance_sq'] = df['class_attendance'] ** 2\n",
    "    \n",
    "    # Key interactions\n",
    "    df['study_x_attendance'] = df['study_hours'] * df['class_attendance']\n",
    "    df['study_x_sleep'] = df['study_hours'] * df['sleep_hours']\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    df['efficiency'] = (df['study_hours'] * df['class_attendance']) / (df['sleep_hours'] + 1)\n",
    "    df['efficiency_sq'] = df['efficiency'] ** 2\n",
    "    \n",
    "    # Weighted effort (EDA-informed weights)\n",
    "    df['weighted_effort'] = (0.06 * df['class_attendance'] + \n",
    "                             2.0 * df['study_hours'] + \n",
    "                             1.2 * df['sleep_hours'])\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 2: HIGH-IMPACT CATEGORICAL FEATURES (η² > 2% from EDA)\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 2: High-impact categorical features...\")\n",
    "    \n",
    "    # Ordinal encoding for features with clear monotonic relationships\n",
    "    sleep_quality_map = {'poor': 0, 'average': 1, 'good': 2}\n",
    "    df['sleep_quality_ord'] = df['sleep_quality'].map(sleep_quality_map).fillna(1)\n",
    "    \n",
    "    study_method_map = {'self-study': 0, 'online videos': 1, 'group study': 2, \n",
    "                        'mixed': 3, 'coaching': 4}\n",
    "    df['study_method_ord'] = df['study_method'].map(study_method_map).fillna(2)\n",
    "    \n",
    "    facility_map = {'low': 0, 'medium': 1, 'high': 2}\n",
    "    df['facility_ord'] = df['facility_rating'].map(facility_map).fillna(1)\n",
    "    \n",
    "    difficulty_map = {'easy': 0, 'moderate': 1, 'hard': 2}\n",
    "    df['difficulty_ord'] = df['exam_difficulty'].map(difficulty_map).fillna(1)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 3: CATEGORICAL × NUMERIC INTERACTIONS\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 3: Categorical × numeric interactions...\")\n",
    "    \n",
    "    df['sleep_quality_x_study'] = df['sleep_quality_ord'] * df['study_hours']\n",
    "    df['facility_x_attendance'] = df['facility_ord'] * df['class_attendance']\n",
    "    df['study_method_x_hours'] = df['study_method_ord'] * df['study_hours']\n",
    "    df['difficulty_x_efficiency'] = df['difficulty_ord'] * df['efficiency']\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 4: DOMAIN-SPECIFIC FLAGS\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 4: Domain-specific flags...\")\n",
    "    \n",
    "    df['ideal_sleep'] = ((df['sleep_hours'] >= 7) & (df['sleep_hours'] <= 9)).astype(int)\n",
    "    df['sleep_deprived'] = (df['sleep_hours'] <= 5.5).astype(int)\n",
    "    df['high_performer'] = ((df['study_hours'] >= 6) & (df['class_attendance'] >= 85)).astype(int)\n",
    "    df['at_risk'] = ((df['study_hours'] <= 3) | (df['class_attendance'] <= 60)).astype(int)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 5: BINNING FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 5: Intelligent binning...\")\n",
    "    \n",
    "    df['study_bin'] = pd.cut(df['study_hours'], bins=[-0.1, 2, 4, 6, 8], labels=False).astype(float)\n",
    "    df['attendance_bin'] = pd.cut(df['class_attendance'], bins=[40, 60, 75, 85, 95, 100], labels=False).astype(float)\n",
    "    df['sleep_bin'] = pd.cut(df['sleep_hours'], bins=[4, 5.5, 7, 8.5, 10], labels=False).astype(float)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 6: SELECTIVE TRANSFORMATIONS\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 6: Selective transformations...\")\n",
    "    \n",
    "    df['log_study_hours'] = np.log1p(df['study_hours'])\n",
    "    df['sqrt_attendance'] = np.sqrt(df['class_attendance'])\n",
    "    df['age_rank'] = df['age'].rank(pct=True)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 7: DISTANCE FROM OPTIMAL\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 7: Distance from optimal...\")\n",
    "    \n",
    "    df['sleep_from_optimal'] = np.abs(df['sleep_hours'] - 8)\n",
    "    df['study_from_optimal'] = np.abs(df['study_hours'] - 6)\n",
    "    df['attendance_from_optimal'] = np.abs(df['class_attendance'] - 95)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 8: RATIOS\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 8: Meaningful ratios...\")\n",
    "    \n",
    "    df['study_per_sleep'] = df['study_hours'] / (df['sleep_hours'] + eps)\n",
    "    df['geometric_mean'] = (\n",
    "        (df['study_hours'] + 1) *\n",
    "        (df['class_attendance'] + 1) *\n",
    "        (df['sleep_hours'] + 1)\n",
    "    ) ** (1/3)\n",
    "\n",
    "    # =========================================================================\n",
    "    # TIER 9: FORMULA-BASED FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"  Creating Tier 9: Formula-based features...\")\n",
    "\n",
    "    # Core formula from domain analysis\n",
    "    df['formula'] = (\n",
    "        6 * df['study_hours'] + \n",
    "        0.35 * df['class_attendance'] + \n",
    "        1.5 * df['sleep_hours'] +\n",
    "        5 * (df['sleep_quality'] == 'good') + \n",
    "        -5 * (df['sleep_quality'] == 'poor') +\n",
    "        10 * (df['study_method'] == 'coaching') + \n",
    "        5 * (df['study_method'] == 'mixed') + \n",
    "        2 * (df['study_method'] == 'group study') + \n",
    "        1 * (df['study_method'] == 'online videos') +\n",
    "        4 * (df['facility_rating'] == 'high') + \n",
    "        -4 * (df['facility_rating'] == 'low')\n",
    "    )\n",
    "\n",
    "    # Formula derivatives\n",
    "    df['formula_sq'] = df['formula'] ** 2\n",
    "    df['formula_sqrt'] = np.sqrt(np.abs(df['formula']))\n",
    "    df['formula_log'] = np.log1p(df['formula'] - df['formula'].min() + 1)\n",
    "\n",
    "    # Formula interactions with key features\n",
    "    df['formula_x_efficiency'] = df['formula'] * df['efficiency']\n",
    "    df['formula_x_study'] = df['formula'] * df['study_hours']\n",
    "    df['formula_residual'] = df['formula'] - df['weighted_effort']\n",
    "\n",
    "    # Formula-based ratios\n",
    "    df['formula_per_study'] = df['formula'] / (df['study_hours'] + eps)\n",
    "    df['formula_per_attendance'] = df['formula'] / (df['class_attendance'] + eps)\n",
    "   \n",
    "    \n",
    "    print(\"✓ Feature engineering complete!\")\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# Apply feature engineering\n",
    "# =============================================================================\n",
    "print(\"\\nApplying optimized feature engineering...\")\n",
    "train_fe = create_optimized_features(train_df)\n",
    "test_fe = create_optimized_features(test_df)\n",
    "original_fe = create_optimized_features(original_df)\n",
    "\n",
    "y = train_df[TARGET].clip(0, 100).values\n",
    "y_orig = original_df[TARGET].clip(0, 100).values\n",
    "\n",
    "# Categorical features for target encoding (only high-impact from EDA)\n",
    "cat_features = [\n",
    "    'sleep_quality',      # η² = 5.6%\n",
    "    'study_method',       # η² = 5.0%\n",
    "    'facility_rating',    # η² = 3.6%\n",
    "]\n",
    "\n",
    "# Low-impact categoricals to remove\n",
    "low_impact_cats = [\n",
    "    'gender',           # η² < 0.2%\n",
    "    'course',           # η² < 0.2%\n",
    "    'internet_access',  # η² < 0.2%\n",
    "    'exam_difficulty',  # We have difficulty_ord instead\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"High-impact categoricals (for target encoding): {cat_features}\")\n",
    "print(f\"Low-impact categoricals (will be removed): {low_impact_cats}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TARGET ENCODING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TARGET ENCODING ON CATEGORICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def target_encode_cv(X_train, X_test, y_train, cat_cols, n_splits=5, alpha=10):\n",
    "    \"\"\"\n",
    "    Target encoding with CV to prevent leakage.\n",
    "    Alpha = smoothing parameter (higher = more regularization)\n",
    "    \"\"\"\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    global_mean = y_train.mean()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        print(f\"  Encoding {col}...\")\n",
    "        \n",
    "        # For train: use CV to prevent leakage\n",
    "        X_train_enc[f'{col}_target'] = 0.0\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            X_tr = X_train.iloc[train_idx]\n",
    "            y_tr = y_train[train_idx]\n",
    "            \n",
    "            # Create temporary DataFrame to align X and y\n",
    "            df_fold = pd.DataFrame({\n",
    "                'category': X_tr[col].values,\n",
    "                'target': y_tr\n",
    "            })\n",
    "            \n",
    "            # Calculate mean encoding on training fold\n",
    "            target_means = df_fold.groupby('category')['target'].agg(['mean', 'count'])\n",
    "            target_means['target_enc'] = (\n",
    "                (target_means['mean'] * target_means['count'] + global_mean * alpha) / \n",
    "                (target_means['count'] + alpha)\n",
    "            )\n",
    "            \n",
    "            # Apply to validation fold\n",
    "            X_train_enc.loc[val_idx, f'{col}_target'] = (\n",
    "                X_train.iloc[val_idx][col]\n",
    "                .map(target_means['target_enc'])\n",
    "                .fillna(global_mean)\n",
    "            )\n",
    "        \n",
    "        # For test: use all training data to calculate means\n",
    "        df_full = pd.DataFrame({\n",
    "            'category': X_train[col].values,\n",
    "            'target': y_train\n",
    "        })\n",
    "        \n",
    "        target_means_full = df_full.groupby('category')['target'].agg(['mean', 'count'])\n",
    "        target_means_full['target_enc'] = (\n",
    "            (target_means_full['mean'] * target_means_full['count'] + global_mean * alpha) / \n",
    "            (target_means_full['count'] + alpha)\n",
    "        )\n",
    "        \n",
    "        X_test_enc[f'{col}_target'] = (\n",
    "            X_test[col]\n",
    "            .map(target_means_full['target_enc'])\n",
    "            .fillna(global_mean)\n",
    "        )\n",
    "        \n",
    "        # Remove original column (now we have the encoded version)\n",
    "        X_train_enc = X_train_enc.drop(columns=[col])\n",
    "        X_test_enc = X_test_enc.drop(columns=[col])\n",
    "    \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "\n",
    "def target_encode_simple(X, y, cat_cols, alpha=10):\n",
    "    \"\"\"Simple target encoding (no CV) for small datasets\"\"\"\n",
    "    X_enc = X.copy()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        print(f\"  Encoding {col} (simple)...\")\n",
    "        \n",
    "        df_temp = pd.DataFrame({\n",
    "            'category': X[col].values,\n",
    "            'target': y\n",
    "        })\n",
    "        \n",
    "        target_means = df_temp.groupby('category')['target'].agg(['mean', 'count'])\n",
    "        target_means['target_enc'] = (\n",
    "            (target_means['mean'] * target_means['count'] + global_mean * alpha) / \n",
    "            (target_means['count'] + alpha)\n",
    "        )\n",
    "        \n",
    "        X_enc[f'{col}_target'] = X[col].map(target_means['target_enc']).fillna(global_mean)\n",
    "        X_enc = X_enc.drop(columns=[col])\n",
    "    \n",
    "    return X_enc\n",
    "\n",
    "\n",
    "# Apply target encoding\n",
    "print(\"\\nTarget encoding TRAIN and TEST (with CV)...\")\n",
    "X_train_init, X_test_init = target_encode_cv(\n",
    "    train_fe,\n",
    "    test_fe,\n",
    "    pd.Series(y, name='exam_score'),\n",
    "    cat_features,\n",
    "    n_splits=5,\n",
    "    alpha=10\n",
    ")\n",
    "\n",
    "print(\"\\nTarget encoding ORIGINAL (simple, no CV)...\")\n",
    "X_orig_init = target_encode_simple(\n",
    "    original_fe,\n",
    "    pd.Series(y_orig, name='exam_score'),\n",
    "    cat_features,\n",
    "    alpha=10\n",
    ")\n",
    "\n",
    "# Remove low-impact categoricals + ID + TARGET\n",
    "cols_to_remove = [ID_COL, TARGET] + low_impact_cats\n",
    "feature_cols = [c for c in X_train_init.columns if c not in cols_to_remove]\n",
    "\n",
    "X_train_init = X_train_init[feature_cols]\n",
    "X_test_init = X_test_init[feature_cols]\n",
    "X_orig_init = X_orig_init[feature_cols]\n",
    "\n",
    "# Convert to float32\n",
    "X_train_init = X_train_init.astype(np.float32)\n",
    "X_test_init = X_test_init.astype(np.float32)\n",
    "X_orig_init = X_orig_init.astype(np.float32)\n",
    "\n",
    "print(f\"\\n✓ Target encoding complete. Shape: {X_train_init.shape}\")\n",
    "print(f\"✓ Features after cleanup: {len(feature_cols)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. FEATURE IMPORTANCE ANALYSIS & SELECTION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Training quick LightGBM for feature importance...\")\n",
    "\n",
    "X_temp = X_train_init.copy()\n",
    "y_temp = y.copy()\n",
    "\n",
    "kf_temp = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "feature_importance = np.zeros(X_temp.shape[1])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf_temp.split(X_temp)):\n",
    "    X_tr, X_val = X_temp.iloc[train_idx], X_temp.iloc[val_idx]\n",
    "    y_tr, y_val = y_temp[train_idx], y_temp[val_idx]\n",
    "\n",
    "    lgb_params_quick = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'device': 'gpu' if USE_GPU else 'cpu',\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "        lgb_params_quick, train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]\n",
    "    )\n",
    "\n",
    "    feature_importance += model.feature_importance(importance_type='gain')\n",
    "    print(f\"  Fold {fold+1}/3 done\")\n",
    "\n",
    "# Average importance\n",
    "feature_importance /= 3\n",
    "\n",
    "# Create DataFrame with importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_temp.columns,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Calculate cumulative importance\n",
    "importance_df['cumulative_importance'] = importance_df['importance'].cumsum() / importance_df['importance'].sum()\n",
    "\n",
    "print(f\"\\nTotal features: {len(importance_df)}\")\n",
    "print(f\"\\nTop 20 most important features:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Feature selection strategy\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SELECTION STRATEGY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "USE_FEATURE_SELECTION = True\n",
    "MIN_FEATURES = 8\n",
    "\n",
    "if USE_FEATURE_SELECTION:\n",
    "    # Keep features covering 98.5% of importance\n",
    "    threshold = 0.995\n",
    "    selected_features = importance_df[importance_df['cumulative_importance'] <= threshold]['feature'].tolist()\n",
    "    \n",
    "    # Ensure at least MIN_FEATURES features\n",
    "    if len(selected_features) < MIN_FEATURES:\n",
    "        selected_features = importance_df.head(MIN_FEATURES)['feature'].tolist()\n",
    "else:\n",
    "    selected_features = feature_cols\n",
    "    print(\"Keeping ALL features (no selection)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FEATURE SELECTION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "print(f\"Reduction: {100*(1 - len(selected_features)/len(feature_cols)):.1f}%\")\n",
    "\n",
    "# Update datasets\n",
    "X_train_init = X_train_init[selected_features]\n",
    "X_test_init = X_test_init[selected_features]\n",
    "X_orig_init = X_orig_init[selected_features]\n",
    "feature_cols = selected_features\n",
    "\n",
    "print(f\"\\n✓ Feature selection complete!\")\n",
    "print(f\"✓ Final dataset shape: {X_train_init.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. SAVE PREPROCESSED DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../data/preprocessed', exist_ok=True)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save preprocessed datasets\n",
    "joblib.dump(X_train_init, '../data/preprocessed/X_train.pkl')\n",
    "joblib.dump(X_test_init, '../data/preprocessed/X_test.pkl')\n",
    "joblib.dump(X_orig_init, '../data/preprocessed/X_orig.pkl')\n",
    "joblib.dump(y, '../data/preprocessed/y_train.pkl')\n",
    "joblib.dump(y_orig, '../data/preprocessed/y_orig.pkl')\n",
    "\n",
    "print(\"✓ X_train.pkl saved\")\n",
    "print(\"✓ X_test.pkl saved\")\n",
    "print(\"✓ X_orig.pkl saved\")\n",
    "print(\"✓ y_train.pkl saved\")\n",
    "print(\"✓ y_orig.pkl saved\")\n",
    "\n",
    "# Save feature names\n",
    "joblib.dump(selected_features, '../data/preprocessed/feature_names.pkl')\n",
    "print(\"✓ feature_names.pkl saved\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.to_csv('../results/feature_importance.csv', index=False)\n",
    "print(\"✓ feature_importance.csv saved\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'n_train': len(X_train_init),\n",
    "    'n_test': len(X_test_init),\n",
    "    'n_orig': len(X_orig_init),\n",
    "    'n_features': len(selected_features),\n",
    "    'cat_features': cat_features,\n",
    "    'low_impact_cats': low_impact_cats,\n",
    "    'adv_auc': adv_auc,\n",
    "    'use_gpu': USE_GPU\n",
    "}\n",
    "joblib.dump(metadata, '../data/preprocessed/metadata.pkl')\n",
    "print(\"✓ metadata.pkl saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Train shape: {X_train_init.shape}\")\n",
    "print(f\"✓ Test shape: {X_test_init.shape}\")\n",
    "print(f\"✓ Original shape: {X_orig_init.shape}\")\n",
    "print(f\"✓ All files saved in '../data/preprocessed/'\")\n",
    "print(\"\\nYou can now proceed to hyperparameter optimization notebooks (02-09).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978abb11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "students-scores",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
