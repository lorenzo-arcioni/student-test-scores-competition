{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DEPENDENCIES & SETUP\n",
        "# ============================================================================\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OrdinalEncoder, RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pytabkit import TabM_D_Regressor\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration management\"\"\"\n",
        "    \n",
        "    # Experiment Settings\n",
        "    EXPERIMENT_NAME: str = \"TabM_Production_v2\"\n",
        "    SEED: int = 42\n",
        "    N_FOLDS: int = 10\n",
        "    TARGET: str = 'exam_score'\n",
        "    \n",
        "    # Hardware\n",
        "    DEVICE: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    N_WORKERS: int = 4\n",
        "    \n",
        "    # TabM Hyperparameters (Optimized)\n",
        "    TABM_CONFIG: Dict = {\n",
        "        'device': DEVICE,\n",
        "        'random_state': 42,\n",
        "        'verbosity': 2,\n",
        "        'arch_type': 'tabm-mini-normal',\n",
        "        'tabm_k': 24,  # Increased ensemble diversity\n",
        "        'num_emb_type': 'pwl',\n",
        "        'd_embedding': 16,  # Richer embeddings\n",
        "        'batch_size': 256,\n",
        "        'lr': 1e-3,  # Slightly lower for stability\n",
        "        'n_epochs': 100,  # More training budget\n",
        "        'dropout': 0.11,  # Increased regularization\n",
        "        'd_block': 256,  # Wider architecture\n",
        "        'n_blocks': 5,  # Deeper network\n",
        "        'patience': 4,  # More patience for convergence\n",
        "        'weight_decay': 1e-2,\n",
        "    }\n",
        "    \n",
        "    # Feature Engineering\n",
        "    POLY_DEGREE: int = 2\n",
        "    INTERACTION_PAIRS: List[Tuple[str, str]] = [\n",
        "        ('study_hours', 'class_attendance'),\n",
        "        ('study_hours', 'sleep_hours'),\n",
        "        ('sleep_hours', 'sleep_quality'),\n",
        "    ]\n",
        "    \n",
        "    # Blending Strategy\n",
        "    ENSEMBLE_WEIGHT: float = 0.5  # Weight for CV ensemble (0.5 = equal blend)\n",
        "    REFIT_WEIGHT: float = 0.5     # Weight for full refit (0.5 = equal blend)\n",
        "    \n",
        "    # Output Settings\n",
        "    OUTPUT_DIR: Path = Path('./outputs')\n",
        "    SAVE_PLOTS: bool = True\n",
        "    SAVE_MODEL: bool = True  # Save final model\n",
        "    CLIP_MIN: float = 19.6\n",
        "    CLIP_MAX: float = 100.0\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "def seed_everything(seed: int = 42) -> None:\n",
        "    \"\"\"Ensure reproducibility across all libraries\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def setup_directories(config: Config) -> None:\n",
        "    \"\"\"Create output directory structure\"\"\"\n",
        "    config.OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "    (config.OUTPUT_DIR / 'plots').mkdir(exist_ok=True)\n",
        "    (config.OUTPUT_DIR / 'models').mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "def print_section(title: str, char: str = '=') -> None:\n",
        "    \"\"\"Pretty print section headers\"\"\"\n",
        "    print(f\"\\n{char * 80}\")\n",
        "    print(f\"{title.center(80)}\")\n",
        "    print(f\"{char * 80}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE ENGINEERING ‚Äî KAGGLE / TABM EMBEDDING HACK\n",
        "# ============================================================================\n",
        "class FeatureEngineer:\n",
        "    \"\"\"\n",
        "    Kaggle-optimized Feature Engineering for TabM\n",
        "    (Intentionally exploits categorical embeddings on numeric features)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.eps = 1e-6\n",
        "\n",
        "        # ============================\n",
        "        # TRUE CATEGORICAL FEATURES\n",
        "        # ============================\n",
        "        self.base_categorical = [\n",
        "            'gender',\n",
        "            'course',\n",
        "            'study_method'\n",
        "        ]\n",
        "\n",
        "        # ============================\n",
        "        # NUMERICS TO DUPLICATE AS CATS\n",
        "        # ============================\n",
        "        self.numeric_to_cat = [\n",
        "            'age',\n",
        "            'study_hours',\n",
        "            'class_attendance',\n",
        "            'sleep_hours',\n",
        "            'sleep_quality',\n",
        "            'facility_rating',\n",
        "            'exam_difficulty',\n",
        "            'internet_access'\n",
        "        ]\n",
        "\n",
        "    # =========================================================================\n",
        "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "\n",
        "        # =====================================================\n",
        "        # 1. FORCE BASE NUMERICS\n",
        "        # =====================================================\n",
        "        numeric_cols = [\n",
        "            'age',\n",
        "            'study_hours',\n",
        "            'class_attendance',\n",
        "            'sleep_hours'\n",
        "        ]\n",
        "        for col in numeric_cols:\n",
        "            df[col] = df[col].astype(float)\n",
        "\n",
        "        # =====================================================\n",
        "        # 2. MAP SEMI-CATEGORICALS TO NUMERIC (FOR FORMULAS)\n",
        "        # =====================================================\n",
        "        df['internet_access_num'] = df['internet_access'].map({'no': 0, 'yes': 1}).fillna(0)\n",
        "        df['sleep_quality_num'] = df['sleep_quality'].map(\n",
        "            {'poor': 1, 'average': 2, 'good': 3, 'excellent': 4}\n",
        "        ).fillna(2)\n",
        "\n",
        "        # =====================================================\n",
        "        # 3. NUMERIC FEATURE ENGINEERING\n",
        "        # =====================================================\n",
        "        # Cyclic\n",
        "        df['study_hours_sin'] = np.sin(2 * np.pi * df['study_hours'] / 12)\n",
        "        df['class_attendance_sin'] = np.sin(2 * np.pi * df['class_attendance'] / 12)\n",
        "\n",
        "        # Polynomials\n",
        "        for col in ['study_hours', 'class_attendance', 'sleep_hours']:\n",
        "            df[f'{col}_sq'] = df[col] ** 2\n",
        "            df[f'log_{col}'] = np.log1p(df[col].clip(lower=0))\n",
        "\n",
        "        # Interactions\n",
        "        df['study_x_attendance'] = df['study_hours'] * df['class_attendance']\n",
        "        df['study_x_sleep'] = df['study_hours'] * df['sleep_hours']\n",
        "        df['efficiency'] = (\n",
        "            df['study_hours'] * df['class_attendance']\n",
        "        ) / (df['sleep_hours'] + self.eps)\n",
        "\n",
        "        # =====================================================\n",
        "        # 4. MAGIC LINEAR FORMULA (CORE SIGNAL)\n",
        "        # =====================================================\n",
        "        df['feature_formula'] = (\n",
        "            5.9051154511950499 * df['study_hours'] +\n",
        "            0.34540967058057986 * df['class_attendance'] +\n",
        "            1.423461171860262 * df['sleep_hours'] +\n",
        "            4.7819\n",
        "        )\n",
        "\n",
        "        df['formula_sq'] = df['feature_formula'] ** 2\n",
        "        df['formula_log'] = np.log1p(df['feature_formula'] - df['feature_formula'].min() + 1)\n",
        "\n",
        "        # =====================================================\n",
        "        # 5. BINNING ‚Üí EXTRA TOKENS\n",
        "        # =====================================================\n",
        "        df['study_intensity'] = pd.cut(\n",
        "            df['study_hours'],\n",
        "            bins=4,\n",
        "            labels=['low', 'medium', 'high', 'very_high']\n",
        "        )\n",
        "\n",
        "        df['attendance_level'] = pd.cut(\n",
        "            df['class_attendance'],\n",
        "            bins=4,\n",
        "            labels=['poor', 'fair', 'good', 'excellent']\n",
        "        )\n",
        "\n",
        "        # =====================================================\n",
        "        # 6. DUPLICATE NUMERICS AS CATEGORICAL TOKENS (THE HACK)\n",
        "        # =====================================================\n",
        "        for col in self.numeric_to_cat:\n",
        "            df[f'{col}_cat'] = df[col].astype(str)\n",
        "\n",
        "        # =====================================================\n",
        "        # 7. CAST TRUE CATEGORICALS\n",
        "        # =====================================================\n",
        "        for col in self.base_categorical + ['study_intensity', 'attendance_level']:\n",
        "            df[col] = df[col].astype(str)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # =========================================================================\n",
        "    def get_feature_groups(self, df: pd.DataFrame):\n",
        "        # ============================\n",
        "        # CATEGORICAL FEATURES\n",
        "        # ============================\n",
        "        categorical = (\n",
        "            self.base_categorical +\n",
        "            ['study_intensity', 'attendance_level'] +\n",
        "            [f'{c}_cat' for c in self.numeric_to_cat]\n",
        "        )\n",
        "    \n",
        "        # ============================\n",
        "        # NUMERICAL FEATURES\n",
        "        # ============================\n",
        "        numerical = []\n",
        "    \n",
        "        for col in df.columns:\n",
        "            if col in categorical:\n",
        "                continue\n",
        "            if col in [self.config.TARGET, 'id', 'student_id']:\n",
        "                continue\n",
        "    \n",
        "            # üîí SAFETY CHECK: only real numerics\n",
        "            if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                numerical.append(col)\n",
        "    \n",
        "        return categorical, numerical\n",
        "\n",
        "        \n",
        "# ============================================================================\n",
        "# DATA PREPROCESSING\n",
        "# ============================================================================\n",
        "class DataPreprocessor:\n",
        "    \"\"\"Robust data preprocessing pipeline\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.encoder = OrdinalEncoder(\n",
        "            handle_unknown='use_encoded_value',\n",
        "            unknown_value=-1,\n",
        "            encoded_missing_value=-2\n",
        "        )\n",
        "        self.scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
        "        self.categorical_cols = None\n",
        "        self.numerical_cols = None\n",
        "        \n",
        "    def fit(self, df: pd.DataFrame, categorical: List[str], numerical: List[str]) -> 'DataPreprocessor':\n",
        "        \"\"\"Fit encoders and scalers\"\"\"\n",
        "        self.categorical_cols = categorical\n",
        "        self.numerical_cols = numerical\n",
        "        \n",
        "        if categorical:\n",
        "            self.encoder.fit(df[categorical])\n",
        "        if numerical:\n",
        "            self.scaler.fit(df[numerical])\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Apply preprocessing transformations\"\"\"\n",
        "        result_dfs = []\n",
        "        \n",
        "        if self.categorical_cols:\n",
        "            cats_encoded = pd.DataFrame(\n",
        "                self.encoder.transform(df[self.categorical_cols]),\n",
        "                columns=self.categorical_cols,\n",
        "                index=df.index\n",
        "            )\n",
        "            result_dfs.append(cats_encoded)\n",
        "        \n",
        "        if self.numerical_cols:\n",
        "            nums_scaled = pd.DataFrame(\n",
        "                self.scaler.transform(df[self.numerical_cols]),\n",
        "                columns=self.numerical_cols,\n",
        "                index=df.index\n",
        "            )\n",
        "            result_dfs.append(nums_scaled)\n",
        "        \n",
        "        return pd.concat(result_dfs, axis=1)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL TRAINING & VALIDATION\n",
        "# ============================================================================\n",
        "class TabMTrainer:\n",
        "    \"\"\"Orchestrates TabM model training with cross-validation\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.models = []\n",
        "        self.oof_predictions = None\n",
        "        self.test_predictions = []\n",
        "        self.fold_scores = []\n",
        "        self.final_model = None  # For full refit\n",
        "        \n",
        "    def train_cv(\n",
        "        self,\n",
        "        X: pd.DataFrame,\n",
        "        y: np.ndarray,\n",
        "        X_test: pd.DataFrame,\n",
        "        categorical_cols: List[str],\n",
        "        X_augment: Optional[pd.DataFrame] = None,\n",
        "        y_augment: Optional[np.ndarray] = None\n",
        "    ) -> None:\n",
        "        \"\"\"Execute cross-validated training\"\"\"\n",
        "        \n",
        "        print_section(\"CROSS-VALIDATION TRAINING\")\n",
        "        \n",
        "        self.oof_predictions = np.zeros(len(X))\n",
        "        kfold = KFold(n_splits=self.config.N_FOLDS, shuffle=True, random_state=self.config.SEED)\n",
        "        \n",
        "        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
        "            print(f\"\\n{'‚îÄ' * 80}\")\n",
        "            print(f\"Fold {fold + 1}/{self.config.N_FOLDS}\")\n",
        "            print(f\"{'‚îÄ' * 80}\")\n",
        "            \n",
        "            # Split data\n",
        "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "            y_train, y_val = y[train_idx], y[val_idx]\n",
        "            \n",
        "            # Augment with original data if available\n",
        "            if X_augment is not None and y_augment is not None:\n",
        "                X_train = pd.concat([X_train, X_augment], axis=0)\n",
        "                y_train = np.concatenate([y_train, y_augment])\n",
        "                print(f\"Training samples (augmented): {len(X_train):,}\")\n",
        "            else:\n",
        "                print(f\"Training samples: {len(X_train):,}\")\n",
        "            \n",
        "            print(f\"Validation samples: {len(X_val):,}\")\n",
        "            \n",
        "            # Initialize and train model\n",
        "            model = TabM_D_Regressor(**self.config.TABM_CONFIG)\n",
        "            \n",
        "            model.fit(\n",
        "                X_train, y_train,\n",
        "                X_val, y_val,\n",
        "                cat_col_names=categorical_cols\n",
        "            )\n",
        "            \n",
        "            # Generate predictions\n",
        "            val_preds = model.predict(X_val)\n",
        "            test_preds = model.predict(X_test)\n",
        "            \n",
        "            # Store results\n",
        "            self.oof_predictions[val_idx] = val_preds\n",
        "            self.test_predictions.append(test_preds)\n",
        "            \n",
        "            # Calculate metrics\n",
        "            fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "            self.fold_scores.append(fold_rmse)\n",
        "            \n",
        "            print(f\"\\n‚úì Fold {fold + 1} RMSE: {fold_rmse:.5f}\")\n",
        "            \n",
        "            # Memory cleanup\n",
        "            del model\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        # Final OOF score\n",
        "        overall_rmse = np.sqrt(mean_squared_error(y, self.oof_predictions))\n",
        "        print(f\"\\n{'=' * 80}\")\n",
        "        print(f\"OVERALL OOF RMSE: {overall_rmse:.5f}\")\n",
        "        print(f\"Mean Fold RMSE: {np.mean(self.fold_scores):.5f} ¬± {np.std(self.fold_scores):.5f}\")\n",
        "        print(f\"{'=' * 80}\\n\")\n",
        "    \n",
        "    def train_full_refit(\n",
        "        self,\n",
        "        X: pd.DataFrame,\n",
        "        y: np.ndarray,\n",
        "        X_test: pd.DataFrame,\n",
        "        categorical_cols: List[str],\n",
        "        X_augment: Optional[pd.DataFrame] = None,\n",
        "        y_augment: Optional[np.ndarray] = None\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Train final model on full dataset\"\"\"\n",
        "        \n",
        "        print_section(\"FULL DATASET REFIT\")\n",
        "        \n",
        "        # Combine all training data\n",
        "        X_full = X.copy()\n",
        "        y_full = y.copy()\n",
        "        \n",
        "        if X_augment is not None and y_augment is not None:\n",
        "            X_full = pd.concat([X_full, X_augment], axis=0)\n",
        "            y_full = np.concatenate([y_full, y_augment])\n",
        "        \n",
        "        print(f\"Total training samples: {len(X_full):,}\")\n",
        "        \n",
        "        # Train on full dataset (use 10% validation split for early stopping)\n",
        "        val_size = int(len(X_full) * 0.1)\n",
        "        indices = np.arange(len(X_full))\n",
        "        np.random.shuffle(indices)\n",
        "        \n",
        "        train_idx = indices[val_size:]\n",
        "        val_idx = indices[:val_size]\n",
        "        \n",
        "        X_train_full = X_full.iloc[train_idx]\n",
        "        y_train_full = y_full[train_idx]\n",
        "        X_val_full = X_full.iloc[val_idx]\n",
        "        y_val_full = y_full[val_idx]\n",
        "        \n",
        "        print(f\"Training: {len(X_train_full):,} | Validation (for early stopping): {len(X_val_full):,}\")\n",
        "        \n",
        "        # Initialize and train final model\n",
        "        self.final_model = TabM_D_Regressor(**self.config.TABM_CONFIG)\n",
        "        \n",
        "        self.final_model.fit(\n",
        "            X_train_full, y_train_full,\n",
        "            X_val_full, y_val_full,\n",
        "            cat_col_names=categorical_cols\n",
        "        )\n",
        "        \n",
        "        # Generate predictions\n",
        "        refit_preds = self.final_model.predict(X_test)\n",
        "        \n",
        "        print(f\"\\n‚úì Full refit model trained successfully\")\n",
        "        print(f\"Prediction range: [{refit_preds.min():.2f}, {refit_preds.max():.2f}]\")\n",
        "        \n",
        "        return refit_preds\n",
        "    \n",
        "    def get_ensemble_predictions(self) -> np.ndarray:\n",
        "        \"\"\"Average test predictions across folds\"\"\"\n",
        "        return np.mean(self.test_predictions, axis=0)\n",
        "    \n",
        "    def save_final_model(self, path: Path) -> None:\n",
        "        \"\"\"Save the final refitted model\"\"\"\n",
        "        if self.final_model is None:\n",
        "            print(\"Warning: No final model to save. Run train_full_refit first.\")\n",
        "            return\n",
        "        \n",
        "        import joblib\n",
        "        model_path = path / 'final_model.pkl'\n",
        "        joblib.dump(self.final_model, model_path)\n",
        "        print(f\"‚úì Final model saved to: {model_path}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "class Visualizer:\n",
        "    \"\"\"Generate diagnostic plots\"\"\"\n",
        "    \n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        sns.set_style(\"whitegrid\")\n",
        "        plt.rcParams['figure.figsize'] = (12, 6)\n",
        "        plt.rcParams['font.size'] = 10\n",
        "        \n",
        "    def plot_target_distribution(self, train_df: pd.DataFrame, original_df: pd.DataFrame) -> None:\n",
        "        \"\"\"Compare train vs original target distribution\"\"\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "        \n",
        "        sns.kdeplot(data=train_df[self.config.TARGET], label='Train', fill=True, alpha=0.5, ax=ax)\n",
        "        if not original_df.empty:\n",
        "            sns.kdeplot(data=original_df[self.config.TARGET], label='Original', fill=True, alpha=0.5, ax=ax)\n",
        "        \n",
        "        ax.set_xlabel('Exam Score')\n",
        "        ax.set_ylabel('Density')\n",
        "        ax.set_title('Target Distribution: Train vs Original Dataset')\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if self.config.SAVE_PLOTS:\n",
        "            plt.savefig(self.config.OUTPUT_DIR / 'plots' / 'target_distribution.png', dpi=150)\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_oof_predictions(self, y_true: np.ndarray, y_pred: np.ndarray) -> None:\n",
        "        \"\"\"Scatter plot of OOF predictions vs actual\"\"\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "        \n",
        "        # Sample for performance\n",
        "        n_sample = min(10000, len(y_true))\n",
        "        idx = np.random.choice(len(y_true), n_sample, replace=False)\n",
        "        \n",
        "        ax.scatter(y_true[idx], y_pred[idx], alpha=0.3, s=10)\n",
        "        \n",
        "        # Perfect prediction line\n",
        "        min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
        "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "        \n",
        "        ax.set_xlabel('Actual Exam Score')\n",
        "        ax.set_ylabel('Predicted Exam Score')\n",
        "        ax.set_title('Out-of-Fold Predictions vs Actual')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if self.config.SAVE_PLOTS:\n",
        "            plt.savefig(self.config.OUTPUT_DIR / 'plots' / 'oof_predictions.png', dpi=150)\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_fold_scores(self, fold_scores: List[float]) -> None:\n",
        "        \"\"\"Bar plot of per-fold RMSE\"\"\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "        \n",
        "        folds = list(range(1, len(fold_scores) + 1))\n",
        "        ax.bar(folds, fold_scores, alpha=0.7, color='steelblue')\n",
        "        ax.axhline(np.mean(fold_scores), color='red', linestyle='--', label=f'Mean: {np.mean(fold_scores):.5f}')\n",
        "        \n",
        "        ax.set_xlabel('Fold')\n",
        "        ax.set_ylabel('RMSE')\n",
        "        ax.set_title('Per-Fold RMSE Performance')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if self.config.SAVE_PLOTS:\n",
        "            plt.savefig(self.config.OUTPUT_DIR / 'plots' / 'fold_scores.png', dpi=150)\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_residuals(self, y_true: np.ndarray, y_pred: np.ndarray) -> None:\n",
        "        \"\"\"Residual plot for error analysis\"\"\"\n",
        "        residuals = y_true - y_pred\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Residual scatter\n",
        "        axes[0].scatter(y_pred, residuals, alpha=0.3, s=10)\n",
        "        axes[0].axhline(0, color='red', linestyle='--', lw=2)\n",
        "        axes[0].set_xlabel('Predicted Values')\n",
        "        axes[0].set_ylabel('Residuals')\n",
        "        axes[0].set_title('Residual Plot')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Residual distribution\n",
        "        axes[1].hist(residuals, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "        axes[1].axvline(0, color='red', linestyle='--', lw=2)\n",
        "        axes[1].set_xlabel('Residuals')\n",
        "        axes[1].set_ylabel('Frequency')\n",
        "        axes[1].set_title('Residual Distribution')\n",
        "        axes[1].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if self.config.SAVE_PLOTS:\n",
        "            plt.savefig(self.config.OUTPUT_DIR / 'plots' / 'residuals.png', dpi=150)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\"\"\"Main execution pipeline\"\"\"\n",
        "\n",
        "# Initialize\n",
        "config = Config()\n",
        "seed_everything(config.SEED)\n",
        "setup_directories(config)\n",
        "\n",
        "print_section(f\"EXAM SCORE PREDICTION - {config.EXPERIMENT_NAME}\", char='#')\n",
        "print(f\"Device: {config.DEVICE}\")\n",
        "print(f\"Cross-Validation: {config.N_FOLDS}-Fold\")\n",
        "print(f\"Random Seed: {config.SEED}\")\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 1: DATA LOADING\n",
        "# ========================================================================\n",
        "print_section(\"STEP 1: DATA LOADING\")\n",
        "\n",
        "train_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/train.csv\")\n",
        "test_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/test.csv\")\n",
        "original_df = pd.read_csv(\"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\")\n",
        "submission_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "print(f\"Original shape: {original_df.shape}\")\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 2: FEATURE ENGINEERING\n",
        "# ========================================================================\n",
        "print_section(\"STEP 2: FEATURE ENGINEERING\")\n",
        "\n",
        "fe = FeatureEngineer(config)\n",
        "\n",
        "train_eng = fe.engineer_features(train_df)\n",
        "test_eng = fe.engineer_features(test_df)\n",
        "original_eng = fe.engineer_features(original_df) if not original_df.empty else pd.DataFrame()\n",
        "\n",
        "categorical_cols, numerical_cols = fe.get_feature_groups(train_eng)\n",
        "\n",
        "print(f\"Categorical features: {len(categorical_cols)}\")\n",
        "print(f\"Numerical features: {len(numerical_cols)}\")\n",
        "print(f\"Total features: {len(categorical_cols) + len(numerical_cols)}\")\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 3: DATA PREPROCESSING\n",
        "# ========================================================================\n",
        "print_section(\"STEP 3: DATA PREPROCESSING\")\n",
        "\n",
        "preprocessor = DataPreprocessor()\n",
        "preprocessor.fit(train_eng, categorical_cols, numerical_cols)\n",
        "\n",
        "X = preprocessor.transform(train_eng)\n",
        "X_test = preprocessor.transform(test_eng)\n",
        "X_original = preprocessor.transform(original_eng) if not original_eng.empty else None\n",
        "\n",
        "y = train_df[config.TARGET].values\n",
        "y_original = original_df[config.TARGET].values if not original_df.empty else None\n",
        "\n",
        "print(f\"Preprocessed train shape: {X.shape}\")\n",
        "print(f\"Preprocessed test shape: {X_test.shape}\")\n",
        "if X_original is not None:\n",
        "    print(f\"Preprocessed original shape: {X_original.shape}\")\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 4: VISUALIZATION (PRE-TRAINING)\n",
        "# ========================================================================\n",
        "viz = Visualizer(config)\n",
        "viz.plot_target_distribution(train_df, original_df)\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 5: MODEL TRAINING\n",
        "# ========================================================================\n",
        "trainer = TabMTrainer(config)\n",
        "trainer.train_cv(\n",
        "    X=X,\n",
        "    y=y,\n",
        "    X_test=X_test,\n",
        "    categorical_cols=categorical_cols,\n",
        "    X_augment=X_original,\n",
        "    y_augment=y_original\n",
        ")\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 5B: FULL DATASET REFIT\n",
        "# ========================================================================\n",
        "refit_preds = trainer.train_full_refit(\n",
        "    X=X,\n",
        "    y=y,\n",
        "    X_test=X_test,\n",
        "    categorical_cols=categorical_cols,\n",
        "    X_augment=X_original,\n",
        "    y_augment=y_original\n",
        ")\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 6: VISUALIZATION (POST-TRAINING)\n",
        "# ========================================================================\n",
        "print_section(\"STEP 6: GENERATING DIAGNOSTIC PLOTS\")\n",
        "\n",
        "viz.plot_oof_predictions(y, trainer.oof_predictions)\n",
        "viz.plot_fold_scores(trainer.fold_scores)\n",
        "viz.plot_residuals(y, trainer.oof_predictions)\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 7: BLENDING PREDICTIONS\n",
        "# ========================================================================\n",
        "print_section(\"STEP 7: BLENDING ENSEMBLE + REFIT PREDICTIONS\")\n",
        "\n",
        "# Get CV ensemble predictions\n",
        "ensemble_preds = trainer.get_ensemble_predictions()\n",
        "\n",
        "# Blend predictions\n",
        "blended_preds = (\n",
        "    config.ENSEMBLE_WEIGHT * ensemble_preds +\n",
        "    config.REFIT_WEIGHT * refit_preds\n",
        ")\n",
        "\n",
        "print(f\"Ensemble weight: {config.ENSEMBLE_WEIGHT}\")\n",
        "print(f\"Refit weight: {config.REFIT_WEIGHT}\")\n",
        "print(f\"\\nPrediction Statistics:\")\n",
        "print(f\"  Ensemble range: [{ensemble_preds.min():.2f}, {ensemble_preds.max():.2f}]\")\n",
        "print(f\"  Refit range:    [{refit_preds.min():.2f}, {refit_preds.max():.2f}]\")\n",
        "print(f\"  Blended range:  [{blended_preds.min():.2f}, {blended_preds.max():.2f}]\")\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 8: GENERATE SUBMISSIONS\n",
        "# ========================================================================\n",
        "print_section(\"STEP 8: GENERATING SUBMISSIONS\")\n",
        "\n",
        "# Save OOF predictions\n",
        "oof_df = pd.DataFrame({\n",
        "    'id': train_df['id'],\n",
        "    config.TARGET: trainer.oof_predictions\n",
        "})\n",
        "oof_df.to_csv(config.OUTPUT_DIR / 'oof_predictions.csv', index=False)\n",
        "print(f\"‚úì Saved: {config.OUTPUT_DIR / 'oof_predictions.csv'}\")\n",
        "\n",
        "# Generate final test predictions (clipped)\n",
        "final_preds_clipped = np.clip(blended_preds, config.CLIP_MIN, config.CLIP_MAX)\n",
        "\n",
        "# Save main submission (blended)\n",
        "submission_df[config.TARGET] = final_preds_clipped\n",
        "submission_df.to_csv(config.OUTPUT_DIR / 'submission_blended.csv', index=False)\n",
        "print(f\"‚úì Saved: {config.OUTPUT_DIR / 'submission_blended.csv'}\")\n",
        "\n",
        "# Save alternative submissions for comparison\n",
        "submission_ensemble = submission_df.copy()\n",
        "submission_ensemble[config.TARGET] = np.clip(ensemble_preds, config.CLIP_MIN, config.CLIP_MAX)\n",
        "submission_ensemble.to_csv(config.OUTPUT_DIR / 'submission_ensemble.csv', index=False)\n",
        "print(f\"‚úì Saved: {config.OUTPUT_DIR / 'submission_ensemble.csv'}\")\n",
        "\n",
        "submission_refit = submission_df.copy()\n",
        "submission_refit[config.TARGET] = np.clip(refit_preds, config.CLIP_MIN, config.CLIP_MAX)\n",
        "submission_refit.to_csv(config.OUTPUT_DIR / 'submission_refit.csv', index=False)\n",
        "print(f\"‚úì Saved: {config.OUTPUT_DIR / 'submission_refit.csv'}\")\n",
        "\n",
        "# Save final model\n",
        "if config.SAVE_MODEL:\n",
        "    trainer.save_final_model(config.OUTPUT_DIR / 'models')\n",
        "\n",
        "# ========================================================================\n",
        "# STEP 9: FINAL SUMMARY\n",
        "# ========================================================================\n",
        "print_section(\"FINAL SUMMARY\", char='#')\n",
        "\n",
        "print(f\"Cross-Validation Results:\")\n",
        "print(f\"  Overall OOF RMSE: {np.sqrt(mean_squared_error(y, trainer.oof_predictions)):.5f}\")\n",
        "print(f\"  Mean Fold RMSE: {np.mean(trainer.fold_scores):.5f}\")\n",
        "print(f\"  Std Fold RMSE: {np.std(trainer.fold_scores):.5f}\")\n",
        "print(f\"  Min Fold RMSE: {np.min(trainer.fold_scores):.5f}\")\n",
        "print(f\"  Max Fold RMSE: {np.max(trainer.fold_scores):.5f}\")\n",
        "\n",
        "print(f\"\\nBlended Submission:\")\n",
        "print(f\"  Prediction Range: [{final_preds_clipped.min():.2f}, {final_preds_clipped.max():.2f}]\")\n",
        "print(f\"  Predictions clipped: {(blended_preds != final_preds_clipped).sum()}\")\n",
        "\n",
        "print(f\"\\nSubmissions Generated:\")\n",
        "print(f\"  1. submission_blended.csv  ‚Üê RECOMMENDED (ensemble + refit blend)\")\n",
        "print(f\"  2. submission_ensemble.csv (CV ensemble only)\")\n",
        "print(f\"  3. submission_refit.csv    (full refit only)\")\n",
        "\n",
        "print(f\"\\n{'#' * 80}\")\n",
        "print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "print(f\"{'#' * 80}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebooke888a22c2d",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 14993753,
          "sourceId": 119082,
          "sourceType": "competition"
        },
        {
          "datasetId": 8762382,
          "sourceId": 13904981,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31236,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "students-scores",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
