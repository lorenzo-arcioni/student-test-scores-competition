{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "PURE DEEP LEARNING APPROACH FOR TABULAR DATA - SOTA TECHNIQUES\n",
        "================================================================\n",
        "\n",
        "Questo codice implementa le tecniche piÃ¹ avanzate di deep learning\n",
        "per dati tabulari, senza usare gradient boosting o ensemble ibridi.\n",
        "\n",
        "Obiettivo: Battere XGBoost/LightGBM usando SOLO deep learning.\n",
        "\n",
        "Tecniche implementate:\n",
        "1. Feature Tokenizer + Transformer (FT-Transformer)\n",
        "2. TabNet con attenzione sparsa\n",
        "3. SAINT (Self-Attention and Intersample Attention Transformer)\n",
        "4. NODE (Neural Oblivious Decision Ensembles)\n",
        "5. Advanced preprocessing e augmentation\n",
        "6. Self-supervised pre-training\n",
        "7. Multi-task auxiliary learning\n",
        "8. Knowledge distillation tra modelli DL\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from sklearn.preprocessing import QuantileTransformer, PowerTransformer, RobustScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURAZIONE GPU\n",
        "# =============================================================================\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 1. PREPROCESSING AVANZATO PER DEEP LEARNING\n",
        "# =============================================================================\n",
        "\n",
        "class DeepLearningPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing ottimizzato per neural networks su tabular data.\n",
        "    \n",
        "    Tecniche:\n",
        "    - Multi-strategy normalization\n",
        "    - Outlier handling\n",
        "    - Feature-specific transformations\n",
        "    - Noise injection per regolarizzazione\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, numeric_strategy='quantile', add_noise=True):\n",
        "        self.numeric_strategy = numeric_strategy\n",
        "        self.add_noise = add_noise\n",
        "        \n",
        "        if numeric_strategy == 'quantile':\n",
        "            self.numeric_scaler = QuantileTransformer(\n",
        "                n_quantiles=2000, \n",
        "                output_distribution='normal',\n",
        "                random_state=42\n",
        "            )\n",
        "        elif numeric_strategy == 'power':\n",
        "            self.numeric_scaler = PowerTransformer(method='yeo-johnson')\n",
        "        else:\n",
        "            self.numeric_scaler = RobustScaler()\n",
        "        \n",
        "        self.numeric_cols = None\n",
        "        self.categorical_cols = None\n",
        "        self.cat_encoders = {}\n",
        "        \n",
        "    def fit(self, X, categorical_cols=None):\n",
        "        \"\"\"Fit preprocessor\"\"\"\n",
        "        self.categorical_cols = categorical_cols if categorical_cols else []\n",
        "        self.numeric_cols = [c for c in X.columns if c not in self.categorical_cols]\n",
        "        \n",
        "        # Fit numeric scaler\n",
        "        X_numeric = X[self.numeric_cols].copy()\n",
        "        \n",
        "        # Clip extreme outliers (> 5 IQR)\n",
        "        for col in self.numeric_cols:\n",
        "            Q1, Q3 = X_numeric[col].quantile([0.01, 0.99])\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 5 * IQR\n",
        "            upper = Q3 + 5 * IQR\n",
        "            X_numeric[col] = X_numeric[col].clip(lower, upper)\n",
        "        \n",
        "        self.numeric_scaler.fit(X_numeric)\n",
        "        \n",
        "        # Encode categoricals\n",
        "        for col in self.categorical_cols:\n",
        "            unique_vals = X[col].unique()\n",
        "            self.cat_encoders[col] = {val: idx for idx, val in enumerate(unique_vals)}\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, add_noise=None):\n",
        "        \"\"\"Transform data\"\"\"\n",
        "        if add_noise is None:\n",
        "            add_noise = self.add_noise\n",
        "            \n",
        "        # Numeric features\n",
        "        X_numeric = X[self.numeric_cols].copy()\n",
        "        \n",
        "        # Clip outliers\n",
        "        for col in self.numeric_cols:\n",
        "            Q1, Q3 = X_numeric[col].quantile([0.01, 0.99])\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 5 * IQR\n",
        "            upper = Q3 + 5 * IQR\n",
        "            X_numeric[col] = X_numeric[col].clip(lower, upper)\n",
        "        \n",
        "        X_numeric_transformed = self.numeric_scaler.transform(X_numeric)\n",
        "        \n",
        "        # Add small noise for regularization (only during training)\n",
        "        if add_noise:\n",
        "            noise = np.random.normal(0, 0.01, X_numeric_transformed.shape)\n",
        "            X_numeric_transformed += noise\n",
        "        \n",
        "        # Categorical features\n",
        "        X_cat_transformed = np.zeros((len(X), len(self.categorical_cols)), dtype=np.int64)\n",
        "        for i, col in enumerate(self.categorical_cols):\n",
        "            X_cat_transformed[:, i] = X[col].map(self.cat_encoders[col]).fillna(0).astype(np.int64)\n",
        "        \n",
        "        return X_numeric_transformed, X_cat_transformed\n",
        "    \n",
        "    def fit_transform(self, X, categorical_cols=None):\n",
        "        \"\"\"Fit and transform\"\"\"\n",
        "        self.fit(X, categorical_cols)\n",
        "        return self.transform(X)\n",
        "\n",
        "# =============================================================================\n",
        "# 2. FT-TRANSFORMER (FEATURE TOKENIZER + TRANSFORMER)\n",
        "# =============================================================================\n",
        "\n",
        "class FTTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature Tokenizer + Transformer\n",
        "    \n",
        "    Paper: \"Revisiting Deep Learning Models for Tabular Data\" (NeurIPS 2021)\n",
        "    \n",
        "    Key idea: Treat each feature as a token and apply transformer attention.\n",
        "    This allows the model to learn complex feature interactions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_numeric_features, categorical_cardinalities, \n",
        "                 d_token=192, n_blocks=3, attention_n_heads=8, \n",
        "                 attention_dropout=0.2, ffn_dropout=0.1,\n",
        "                 residual_dropout=0.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_numeric = n_numeric_features\n",
        "        self.n_categorical = len(categorical_cardinalities)\n",
        "        \n",
        "        # Feature tokenization\n",
        "        # Each numeric feature -> d_token dimensional embedding\n",
        "        self.numeric_tokenizer = nn.Linear(1, d_token)\n",
        "        \n",
        "        # Each categorical feature -> embedding -> d_token projection\n",
        "        self.category_embeddings = nn.ModuleList()\n",
        "        for cardinality in categorical_cardinalities:\n",
        "            # Embedding dimension: min(50, cardinality // 2)\n",
        "            embed_dim = min(50, max(cardinality // 2, 8))\n",
        "            embedding = nn.Sequential(\n",
        "                nn.Embedding(cardinality, embed_dim),\n",
        "                nn.Linear(embed_dim, d_token)\n",
        "            )\n",
        "            self.category_embeddings.append(embedding)\n",
        "        \n",
        "        # CLS token for aggregation (learnable)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                d_token=d_token,\n",
        "                n_heads=attention_n_heads,\n",
        "                attention_dropout=attention_dropout,\n",
        "                ffn_dropout=ffn_dropout,\n",
        "                residual_dropout=residual_dropout\n",
        "            )\n",
        "            for _ in range(n_blocks)\n",
        "        ])\n",
        "        \n",
        "        # Output head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(d_token),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_token, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x_numeric, x_categorical):\n",
        "        # Tokenize numeric features\n",
        "        # x_numeric: (batch, n_numeric)\n",
        "        x_numeric = x_numeric.unsqueeze(-1)  # (batch, n_numeric, 1)\n",
        "        numeric_tokens = self.numeric_tokenizer(x_numeric)  # (batch, n_numeric, d_token)\n",
        "        \n",
        "        # Tokenize categorical features\n",
        "        categorical_tokens = []\n",
        "        for i, embedding in enumerate(self.category_embeddings):\n",
        "            cat_indices = x_categorical[:, i]  # (batch,)\n",
        "            token = embedding(cat_indices)  # (batch, d_token)\n",
        "            categorical_tokens.append(token)\n",
        "        \n",
        "        if categorical_tokens:\n",
        "            categorical_tokens = torch.stack(categorical_tokens, dim=1)  # (batch, n_cat, d_token)\n",
        "            # Combine all tokens\n",
        "            tokens = torch.cat([numeric_tokens, categorical_tokens], dim=1)\n",
        "        else:\n",
        "            tokens = numeric_tokens\n",
        "        \n",
        "        # Add CLS token\n",
        "        batch_size = tokens.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch, 1, d_token)\n",
        "        tokens = torch.cat([cls_tokens, tokens], dim=1)  # (batch, 1 + n_features, d_token)\n",
        "        \n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            tokens = block(tokens)\n",
        "        \n",
        "        # Use CLS token for prediction\n",
        "        cls_output = tokens[:, 0, :]  # (batch, d_token)\n",
        "        \n",
        "        return self.head(cls_output).squeeze(-1)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Single transformer block with pre-norm architecture\"\"\"\n",
        "    \n",
        "    def __init__(self, d_token, n_heads, attention_dropout, ffn_dropout, residual_dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attention_norm = nn.LayerNorm(d_token)\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            d_token, n_heads, dropout=attention_dropout, batch_first=True\n",
        "        )\n",
        "        self.attention_dropout = nn.Dropout(residual_dropout)\n",
        "        \n",
        "        self.ffn_norm = nn.LayerNorm(d_token)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_token, d_token * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(ffn_dropout),\n",
        "            nn.Linear(d_token * 4, d_token),\n",
        "            nn.Dropout(ffn_dropout)\n",
        "        )\n",
        "        self.ffn_dropout = nn.Dropout(residual_dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pre-norm attention\n",
        "        x_norm = self.attention_norm(x)\n",
        "        attn_out, _ = self.attention(x_norm, x_norm, x_norm)\n",
        "        x = x + self.attention_dropout(attn_out)\n",
        "        \n",
        "        # Pre-norm FFN\n",
        "        x_norm = self.ffn_norm(x)\n",
        "        ffn_out = self.ffn(x_norm)\n",
        "        x = x + self.ffn_dropout(ffn_out)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# =============================================================================\n",
        "# 3. SAINT (SELF-ATTENTION AND INTERSAMPLE ATTENTION)\n",
        "# =============================================================================\n",
        "\n",
        "class SAINT(nn.Module):\n",
        "    \"\"\"\n",
        "    SAINT: Improved Neural Networks for Tabular Data via Row Attention\n",
        "    \n",
        "    Paper: \"SAINT: Improved Neural Networks for Tabular Data via \n",
        "            Row Attention and Contrastive Pre-Training\" (2021)\n",
        "    \n",
        "    Key innovations:\n",
        "    - Intersample attention (rows attend to other rows)\n",
        "    - Contrastive pre-training\n",
        "    - Hybrid attention mechanism\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_numeric_features, categorical_cardinalities,\n",
        "                 dim=32, depth=6, heads=8, dim_head=16, \n",
        "                 attn_dropout=0.1, ff_dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_numeric = n_numeric_features\n",
        "        self.n_categorical = len(categorical_cardinalities)\n",
        "        \n",
        "        # Embeddings\n",
        "        self.numeric_embedding = nn.Linear(1, dim)\n",
        "        \n",
        "        self.categorical_embeddings = nn.ModuleList()\n",
        "        for cardinality in categorical_cardinalities:\n",
        "            self.categorical_embeddings.append(\n",
        "                nn.Embedding(cardinality, dim)\n",
        "            )\n",
        "        \n",
        "        # Column embeddings (feature-wise positional encoding)\n",
        "        n_features = n_numeric_features + len(categorical_cardinalities)\n",
        "        self.column_embedding = nn.Parameter(torch.randn(1, n_features, dim))\n",
        "        \n",
        "        # Transformer layers with BOTH self-attention and intersample attention\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                # Self-attention (within sample, across features)\n",
        "                PreNormResidual(dim, Attention(dim, heads, dim_head, attn_dropout)),\n",
        "                # Intersample attention (across samples, same feature)\n",
        "                PreNormResidual(dim, IntersampleAttention(dim, heads, dim_head, attn_dropout)),\n",
        "                # Feed-forward\n",
        "                PreNormResidual(dim, FeedForward(dim, dropout=ff_dropout))\n",
        "            ]))\n",
        "        \n",
        "        # Output\n",
        "        self.to_logits = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x_numeric, x_categorical):\n",
        "        batch_size = x_numeric.shape[0]\n",
        "        \n",
        "        # Embed numeric\n",
        "        x_numeric = x_numeric.unsqueeze(-1)  # (batch, n_numeric, 1)\n",
        "        numeric_embedded = self.numeric_embedding(x_numeric)  # (batch, n_numeric, dim)\n",
        "        \n",
        "        # Embed categorical\n",
        "        categorical_embedded = []\n",
        "        for i, embedding in enumerate(self.categorical_embeddings):\n",
        "            cat_emb = embedding(x_categorical[:, i])  # (batch, dim)\n",
        "            categorical_embedded.append(cat_emb)\n",
        "        \n",
        "        if categorical_embedded:\n",
        "            categorical_embedded = torch.stack(categorical_embedded, dim=1)  # (batch, n_cat, dim)\n",
        "            x = torch.cat([numeric_embedded, categorical_embedded], dim=1)\n",
        "        else:\n",
        "            x = numeric_embedded\n",
        "        \n",
        "        # Add column embeddings\n",
        "        x = x + self.column_embedding\n",
        "        \n",
        "        # Apply transformer layers\n",
        "        for self_attn, intersample_attn, ff in self.layers:\n",
        "            x = self_attn(x)\n",
        "            x = intersample_attn(x)\n",
        "            x = ff(x)\n",
        "        \n",
        "        # Pool across features (mean pooling)\n",
        "        x = x.mean(dim=1)  # (batch, dim)\n",
        "        \n",
        "        return self.to_logits(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Standard multi-head self-attention\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        \n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        b, n, _ = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: t.reshape(b, n, self.heads, -1).transpose(1, 2), qkv)\n",
        "        \n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = F.softmax(dots, dim=-1)\n",
        "        \n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).reshape(b, n, -1)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class IntersampleAttention(nn.Module):\n",
        "    \"\"\"Intersample attention - samples attend to each other\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        \n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x: (batch, n_features, dim)\n",
        "        # Transpose to (n_features, batch, dim) for intersample attention\n",
        "        x = x.transpose(0, 1)\n",
        "        b, n, _ = x.shape  # Now b=n_features, n=batch_size\n",
        "        \n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: t.reshape(b, n, self.heads, -1).transpose(1, 2), qkv)\n",
        "        \n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = F.softmax(dots, dim=-1)\n",
        "        \n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).reshape(b, n, -1)\n",
        "        out = self.to_out(out)\n",
        "        \n",
        "        # Transpose back\n",
        "        return out.transpose(0, 1)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Feed-forward network\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, mult=4, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class PreNormResidual(nn.Module):\n",
        "    \"\"\"Pre-norm residual connection\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.fn(self.norm(x)) + x\n",
        "\n",
        "# =============================================================================\n",
        "# 4. ADVANCED TABNET WITH IMPROVEMENTS\n",
        "# =============================================================================\n",
        "\n",
        "class ImprovedTabNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved TabNet with:\n",
        "    - Better initialization\n",
        "    - Ghost batch normalization\n",
        "    - Adaptive sparsity\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim=1,\n",
        "                 n_d=64, n_a=64, n_steps=5,\n",
        "                 gamma=1.5, n_independent=2, n_shared=2,\n",
        "                 epsilon=1e-15, virtual_batch_size=256,\n",
        "                 momentum=0.02):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_d = n_d\n",
        "        self.n_a = n_a\n",
        "        self.n_steps = n_steps\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        \n",
        "        # Batch normalization\n",
        "        self.initial_bn = nn.BatchNorm1d(input_dim, momentum=momentum)\n",
        "        \n",
        "        # Feature transformer (shared across steps)\n",
        "        self.initial_splitter = FeatureTransformer(\n",
        "            input_dim, n_d + n_a, n_shared, n_independent,\n",
        "            virtual_batch_size, momentum\n",
        "        )\n",
        "        \n",
        "        self.feat_transformers = nn.ModuleList()\n",
        "        self.att_transformers = nn.ModuleList()\n",
        "        \n",
        "        for step in range(n_steps):\n",
        "            transformer = FeatureTransformer(\n",
        "                input_dim, n_d + n_a, n_shared, n_independent,\n",
        "                virtual_batch_size, momentum\n",
        "            )\n",
        "            attention = AttentiveTransformer(\n",
        "                n_a, input_dim, virtual_batch_size, momentum\n",
        "            )\n",
        "            self.feat_transformers.append(transformer)\n",
        "            self.att_transformers.append(attention)\n",
        "        \n",
        "        self.final_mapping = nn.Linear(n_d, output_dim, bias=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.initial_bn(x)\n",
        "        \n",
        "        prior_scales = torch.ones(x.shape).to(x.device)\n",
        "        M_loss = 0\n",
        "        att_loss = 0\n",
        "        \n",
        "        steps_output = []\n",
        "        \n",
        "        for step in range(self.n_steps):\n",
        "            # Feature transformer\n",
        "            if step == 0:\n",
        "                x_transformed = self.initial_splitter(x)\n",
        "            else:\n",
        "                x_transformed = self.feat_transformers[step - 1](x)\n",
        "            \n",
        "            # Split\n",
        "            d_out = x_transformed[:, :self.n_d]\n",
        "            a_out = x_transformed[:, self.n_d:]\n",
        "            \n",
        "            steps_output.append(d_out)\n",
        "            \n",
        "            # Attention\n",
        "            if step < self.n_steps - 1:\n",
        "                # Mask\n",
        "                mask_values = self.att_transformers[step](a_out)\n",
        "                mask_values = mask_values * prior_scales\n",
        "                mask_values = torch.softmax(mask_values, dim=1)\n",
        "                \n",
        "                # Sparsity loss\n",
        "                M_loss += torch.mean(\n",
        "                    torch.sum(torch.mul(mask_values, torch.log(mask_values + self.epsilon)), dim=1)\n",
        "                )\n",
        "                \n",
        "                # Update prior\n",
        "                prior_scales = torch.mul(prior_scales, self.gamma - mask_values)\n",
        "                \n",
        "                # Apply mask\n",
        "                x = torch.mul(x, mask_values)\n",
        "        \n",
        "        # Aggregate\n",
        "        d_out = torch.sum(torch.stack(steps_output, dim=0), dim=0)\n",
        "        \n",
        "        out = self.final_mapping(d_out)\n",
        "        \n",
        "        return out.squeeze(-1), M_loss\n",
        "\n",
        "\n",
        "class FeatureTransformer(nn.Module):\n",
        "    \"\"\"Feature transformer block for TabNet\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, n_shared, n_independent,\n",
        "                 virtual_batch_size, momentum):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Shared layers\n",
        "        self.shared = nn.ModuleList()\n",
        "        if n_shared > 0:\n",
        "            self.shared.append(nn.Linear(input_dim, output_dim))\n",
        "            for _ in range(n_shared - 1):\n",
        "                self.shared.append(nn.Linear(output_dim, output_dim))\n",
        "        \n",
        "        # Independent layers\n",
        "        self.independent = nn.ModuleList()\n",
        "        if n_independent > 0:\n",
        "            if n_shared == 0:\n",
        "                self.independent.append(nn.Linear(input_dim, output_dim))\n",
        "            for _ in range(n_independent):\n",
        "                self.independent.append(nn.Linear(output_dim, output_dim))\n",
        "        \n",
        "        self.norm = GhostBatchNorm(output_dim, virtual_batch_size, momentum)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Shared layers\n",
        "        for layer in self.shared:\n",
        "            x = layer(x)\n",
        "            x = torch.relu(x)\n",
        "        \n",
        "        # Independent layers\n",
        "        for layer in self.independent:\n",
        "            x = layer(x)\n",
        "            x = torch.relu(x)\n",
        "        \n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class AttentiveTransformer(nn.Module):\n",
        "    \"\"\"Attentive transformer for mask generation\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, virtual_batch_size, momentum):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.bn = GhostBatchNorm(output_dim, virtual_batch_size, momentum)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x)\n",
        "        return torch.mul(x, torch.sigmoid(x))  # Sparsemax approximation\n",
        "\n",
        "\n",
        "class GhostBatchNorm(nn.Module):\n",
        "    \"\"\"Ghost Batch Normalization\"\"\"\n",
        "    \n",
        "    def __init__(self, num_features, virtual_batch_size, momentum):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.bn = nn.BatchNorm1d(num_features, momentum=momentum)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.training and x.shape[0] > self.virtual_batch_size:\n",
        "            # Split into virtual batches\n",
        "            chunks = x.chunk(max(1, x.shape[0] // self.virtual_batch_size), dim=0)\n",
        "            normalized_chunks = [self.bn(chunk) for chunk in chunks]\n",
        "            return torch.cat(normalized_chunks, dim=0)\n",
        "        else:\n",
        "            return self.bn(x)\n",
        "\n",
        "# =============================================================================\n",
        "# 5. SELF-SUPERVISED PRE-TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "class SelfSupervisedPretrainer:\n",
        "    \"\"\"\n",
        "    Self-supervised pre-training per tabular data\n",
        "    \n",
        "    Tecniche:\n",
        "    1. Masked Feature Prediction (simile a BERT)\n",
        "    2. Contrastive Learning\n",
        "    3. Denoising Autoencoder\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, device='cuda'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        \n",
        "    def masked_feature_prediction(self, X_train, epochs=50, mask_prob=0.15):\n",
        "        \"\"\"\n",
        "        Pre-training: predici features mascherate\n",
        "        \"\"\"\n",
        "        print(\"Pre-training: Masked Feature Prediction...\")\n",
        "        \n",
        "        X_tensor = torch.FloatTensor(X_train).to(self.device)\n",
        "        dataset = TensorDataset(X_tensor)\n",
        "        loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
        "        \n",
        "        # Decoder head temporaneo\n",
        "        decoder = nn.Linear(self.model.head[-1].in_features, X_train.shape[1]).to(self.device)\n",
        "        \n",
        "        optimizer = optim.AdamW(\n",
        "            list(self.model.parameters()) + list(decoder.parameters()),\n",
        "            lr=1e-3\n",
        "        )\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch in loader:\n",
        "                x = batch[0]\n",
        "                \n",
        "                # Masking random features\n",
        "                mask = torch.rand(x.shape) < mask_prob\n",
        "                mask = mask.to(self.device)\n",
        "                x_masked = x.clone()\n",
        "                x_masked[mask] = 0  # Zero out masked features\n",
        "                \n",
        "                # Forward pass (bypassing final prediction head)\n",
        "                # Extract representations before final layer\n",
        "                features = self.model.blocks[-1](\n",
        "                    self.model.cls_token.expand(x.shape[0], -1, -1)\n",
        "                )[:, 0, :]\n",
        "                \n",
        "                # Predict original features\n",
        "                reconstructed = decoder(features)\n",
        "                \n",
        "                # Loss only on masked features\n",
        "                loss = F.mse_loss(reconstructed[mask], x[mask])\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "            \n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.6f}\")\n",
        "        \n",
        "        print(\"Pre-training complete!\")\n",
        "\n",
        "# =============================================================================\n",
        "# 6. ADVANCED DATA AUGMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "class TabularAugmenter:\n",
        "    \"\"\"\n",
        "    Advanced augmentation for tabular data\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def mixup(x, y, alpha=0.4):\n",
        "        \"\"\"Mixup augmentation\"\"\"\n",
        "        if alpha > 0:\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        index = torch.randperm(batch_size).to(x.device)\n",
        "        \n",
        "        mixed_x = lam * x + (1 - lam) * x[index]\n",
        "        y_a, y_b = y, y[index]\n",
        "        \n",
        "        return mixed_x, y_a, y_b, lam\n",
        "    \n",
        "    @staticmethod\n",
        "    def cutmix(x, y, alpha=1.0):\n",
        "        \"\"\"CutMix for tabular data (mask random features)\"\"\"\n",
        "        if alpha > 0:\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        index = torch.randperm(batch_size).to(x.device)\n",
        "        \n",
        "        # Random feature mask\n",
        "        n_features = x.size(1)\n",
        "        n_cut = int(n_features * (1 - lam))\n",
        "        cut_indices = np.random.choice(n_features, n_cut, replace=False)\n",
        "        \n",
        "        mixed_x = x.clone()\n",
        "        mixed_x[:, cut_indices] = x[index][:, cut_indices]\n",
        "        \n",
        "        y_a, y_b = y, y[index]\n",
        "        \n",
        "        return mixed_x, y_a, y_b, lam\n",
        "    \n",
        "    @staticmethod\n",
        "    def feature_dropout(x, p=0.1):\n",
        "        \"\"\"Randomly drop features during training\"\"\"\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.FloatTensor(x)\n",
        "        \n",
        "        mask = torch.rand(x.shape) > p\n",
        "        return x * mask.float()\n",
        "\n",
        "# =============================================================================\n",
        "# 7. ADVANCED LOSS FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "class HuberLoss(nn.Module):\n",
        "    \"\"\"Huber loss - robust to outliers\"\"\"\n",
        "    \n",
        "    def __init__(self, delta=1.0):\n",
        "        super().__init__()\n",
        "        self.delta = delta\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        error = pred - target\n",
        "        abs_error = torch.abs(error)\n",
        "        \n",
        "        quadratic = torch.min(abs_error, torch.tensor(self.delta).to(pred.device))\n",
        "        linear = abs_error - quadratic\n",
        "        \n",
        "        loss = 0.5 * quadratic ** 2 + self.delta * linear\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class QuantileLoss(nn.Module):\n",
        "    \"\"\"Quantile loss for uncertainty estimation\"\"\"\n",
        "    \n",
        "    def __init__(self, quantiles=[0.1, 0.5, 0.9]):\n",
        "        super().__init__()\n",
        "        self.quantiles = quantiles\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        losses = []\n",
        "        for quantile in self.quantiles:\n",
        "            error = target - pred\n",
        "            loss = torch.max(quantile * error, (quantile - 1) * error)\n",
        "            losses.append(loss)\n",
        "        \n",
        "        return torch.stack(losses).mean()\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"Combined loss: Huber + Quantile\"\"\"\n",
        "    \n",
        "    def __init__(self, huber_weight=0.7, delta=1.0):\n",
        "        super().__init__()\n",
        "        self.huber = HuberLoss(delta)\n",
        "        self.quantile = QuantileLoss()\n",
        "        self.huber_weight = huber_weight\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        huber_loss = self.huber(pred, target)\n",
        "        quantile_loss = self.quantile(pred, target)\n",
        "        return self.huber_weight * huber_loss + (1 - self.huber_weight) * quantile_loss\n",
        "\n",
        "# =============================================================================\n",
        "# 8. TRAINING PIPELINE CON TUTTE LE TECNICHE\n",
        "# =============================================================================\n",
        "\n",
        "def train_pure_deep_learning(train_df, test_df, original_df, \n",
        "                             target_col='exam_score', \n",
        "                             categorical_cols=['course', 'study_method'],\n",
        "                             n_folds=10, epochs=300):\n",
        "    \"\"\"\n",
        "    Pipeline completo di deep learning puro\n",
        "    \n",
        "    Steps:\n",
        "    1. Preprocessing avanzato\n",
        "    2. Self-supervised pre-training\n",
        "    3. Training con augmentation\n",
        "    4. Multi-model ensemble (solo DL)\n",
        "    5. Test-time augmentation\n",
        "    \"\"\"\n",
        "    \n",
        "    # Prepare data\n",
        "    feature_cols = [c for c in train_df.columns if c not in ['id', target_col]]\n",
        "    \n",
        "    X_train = train_df[feature_cols]\n",
        "    y_train = train_df[target_col].values\n",
        "    X_test = test_df[feature_cols]\n",
        "    X_original = original_df[feature_cols]\n",
        "    y_original = original_df[target_col].values\n",
        "    \n",
        "    # Combine train + original\n",
        "    X_full = pd.concat([X_train, X_original], axis=0, ignore_index=True)\n",
        "    y_full = np.concatenate([y_train, y_original])\n",
        "    \n",
        "    print(f\"Training samples: {len(X_full)}\")\n",
        "    print(f\"Features: {len(feature_cols)}\")\n",
        "    \n",
        "    # Preprocessing\n",
        "    preprocessor = DeepLearningPreprocessor(numeric_strategy='quantile')\n",
        "    numeric_cols = [c for c in feature_cols if c not in categorical_cols]\n",
        "    \n",
        "    # Get categorical cardinalities\n",
        "    cat_cardinalities = [X_full[col].nunique() for col in categorical_cols]\n",
        "    \n",
        "    # Stratified K-Fold\n",
        "    y_bins = pd.qcut(y_train, q=10, labels=False, duplicates='drop')\n",
        "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    \n",
        "    # Storage for predictions\n",
        "    oof_preds = {\n",
        "        'fttransformer': np.zeros(len(y_train)),\n",
        "        'saint': np.zeros(len(y_train)),\n",
        "        'tabnet': np.zeros(len(y_train))\n",
        "    }\n",
        "    \n",
        "    test_preds = {\n",
        "        'fttransformer': [],\n",
        "        'saint': [],\n",
        "        'tabnet': []\n",
        "    }\n",
        "    \n",
        "    # Cross-validation training\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_bins), 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"FOLD {fold}/{n_folds}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Split data\n",
        "        X_tr = pd.concat([X_train.iloc[train_idx], X_original], axis=0, ignore_index=True)\n",
        "        y_tr = np.concatenate([y_train[train_idx], y_original])\n",
        "        X_val = X_train.iloc[val_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "        \n",
        "        # Preprocess\n",
        "        preprocessor.fit(X_tr, categorical_cols)\n",
        "        X_tr_num, X_tr_cat = preprocessor.transform(X_tr, add_noise=True)\n",
        "        X_val_num, X_val_cat = preprocessor.transform(X_val, add_noise=False)\n",
        "        X_test_num, X_test_cat = preprocessor.transform(X_test, add_noise=False)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        X_tr_num_t = torch.FloatTensor(X_tr_num).to(DEVICE)\n",
        "        X_tr_cat_t = torch.LongTensor(X_tr_cat).to(DEVICE)\n",
        "        y_tr_t = torch.FloatTensor(y_tr).to(DEVICE)\n",
        "        \n",
        "        X_val_num_t = torch.FloatTensor(X_val_num).to(DEVICE)\n",
        "        X_val_cat_t = torch.LongTensor(X_val_cat).to(DEVICE)\n",
        "        y_val_t = torch.FloatTensor(y_val).to(DEVICE)\n",
        "        \n",
        "        X_test_num_t = torch.FloatTensor(X_test_num).to(DEVICE)\n",
        "        X_test_cat_t = torch.LongTensor(X_test_cat).to(DEVICE)\n",
        "        \n",
        "        # =====================================================================\n",
        "        # MODEL 1: FT-TRANSFORMER\n",
        "        # =====================================================================\n",
        "        print(f\"\\nTraining FT-Transformer...\")\n",
        "        \n",
        "        model_ft = FTTransformer(\n",
        "            n_numeric_features=len(numeric_cols),\n",
        "            categorical_cardinalities=cat_cardinalities,\n",
        "            d_token=192,\n",
        "            n_blocks=4,\n",
        "            attention_n_heads=8,\n",
        "            attention_dropout=0.2,\n",
        "            ffn_dropout=0.1\n",
        "        ).to(DEVICE)\n",
        "        \n",
        "        val_pred_ft, test_pred_ft = train_model(\n",
        "            model_ft, X_tr_num_t, X_tr_cat_t, y_tr_t,\n",
        "            X_val_num_t, X_val_cat_t, y_val_t,\n",
        "            X_test_num_t, X_test_cat_t,\n",
        "            epochs=epochs, lr=3e-4, use_augmentation=True\n",
        "        )\n",
        "        \n",
        "        oof_preds['fttransformer'][val_idx] = val_pred_ft\n",
        "        test_preds['fttransformer'].append(test_pred_ft)\n",
        "        \n",
        "        # =====================================================================\n",
        "        # MODEL 2: SAINT\n",
        "        # =====================================================================\n",
        "        print(f\"\\nTraining SAINT...\")\n",
        "        \n",
        "        model_saint = SAINT(\n",
        "            n_numeric_features=len(numeric_cols),\n",
        "            categorical_cardinalities=cat_cardinalities,\n",
        "            dim=64,\n",
        "            depth=6,\n",
        "            heads=8,\n",
        "            dim_head=16,\n",
        "            attn_dropout=0.1,\n",
        "            ff_dropout=0.1\n",
        "        ).to(DEVICE)\n",
        "        \n",
        "        val_pred_saint, test_pred_saint = train_model(\n",
        "            model_saint, X_tr_num_t, X_tr_cat_t, y_tr_t,\n",
        "            X_val_num_t, X_val_cat_t, y_val_t,\n",
        "            X_test_num_t, X_test_cat_t,\n",
        "            epochs=epochs, lr=2e-4, use_augmentation=True\n",
        "        )\n",
        "        \n",
        "        oof_preds['saint'][val_idx] = val_pred_saint\n",
        "        test_preds['saint'].append(test_pred_saint)\n",
        "        \n",
        "        # =====================================================================\n",
        "        # MODEL 3: IMPROVED TABNET\n",
        "        # =====================================================================\n",
        "        print(f\"\\nTraining Improved TabNet...\")\n",
        "        \n",
        "        # Concatenate numeric and categorical for TabNet\n",
        "        X_tr_full = np.concatenate([X_tr_num, X_tr_cat], axis=1)\n",
        "        X_val_full = np.concatenate([X_val_num, X_val_cat], axis=1)\n",
        "        X_test_full = np.concatenate([X_test_num, X_test_cat], axis=1)\n",
        "        \n",
        "        X_tr_full_t = torch.FloatTensor(X_tr_full).to(DEVICE)\n",
        "        X_val_full_t = torch.FloatTensor(X_val_full).to(DEVICE)\n",
        "        X_test_full_t = torch.FloatTensor(X_test_full).to(DEVICE)\n",
        "        \n",
        "        model_tabnet = ImprovedTabNet(\n",
        "            input_dim=X_tr_full.shape[1],\n",
        "            n_d=128,\n",
        "            n_a=128,\n",
        "            n_steps=5,\n",
        "            gamma=1.5,\n",
        "            virtual_batch_size=256\n",
        "        ).to(DEVICE)\n",
        "        \n",
        "        val_pred_tabnet, test_pred_tabnet = train_tabnet(\n",
        "            model_tabnet, X_tr_full_t, y_tr_t,\n",
        "            X_val_full_t, y_val_t,\n",
        "            X_test_full_t,\n",
        "            epochs=epochs, lr=2e-3\n",
        "        )\n",
        "        \n",
        "        oof_preds['tabnet'][val_idx] = val_pred_tabnet\n",
        "        test_preds['tabnet'].append(test_pred_tabnet)\n",
        "        \n",
        "        # Print fold results\n",
        "        print(f\"\\nFold {fold} OOF RMSE:\")\n",
        "        for model_name in oof_preds.keys():\n",
        "            rmse = np.sqrt(mean_squared_error(y_val, oof_preds[model_name][val_idx]))\n",
        "            print(f\"  {model_name}: {rmse:.6f}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # FINAL ENSEMBLE (WEIGHTED AVERAGE OF DL MODELS)\n",
        "    # =========================================================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OPTIMIZING ENSEMBLE WEIGHTS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Calculate OOF scores\n",
        "    oof_scores = {}\n",
        "    for model_name, oof in oof_preds.items():\n",
        "        rmse = np.sqrt(mean_squared_error(y_train, oof))\n",
        "        oof_scores[model_name] = rmse\n",
        "        print(f\"{model_name} OOF RMSE: {rmse:.6f}\")\n",
        "    \n",
        "    # Optimize ensemble weights using Nelder-Mead\n",
        "    from scipy.optimize import minimize\n",
        "    \n",
        "    def ensemble_rmse(weights):\n",
        "        weights = np.abs(weights) / np.sum(np.abs(weights))\n",
        "        ensemble = sum(w * oof for w, oof in zip(weights, oof_preds.values()))\n",
        "        return np.sqrt(mean_squared_error(y_train, ensemble))\n",
        "    \n",
        "    initial_weights = np.array([1.0] * len(oof_preds))\n",
        "    result = minimize(\n",
        "        ensemble_rmse, initial_weights,\n",
        "        method='Nelder-Mead',\n",
        "        options={'maxiter': 2000}\n",
        "    )\n",
        "    \n",
        "    optimal_weights = np.abs(result.x) / np.sum(np.abs(result.x))\n",
        "    \n",
        "    print(f\"\\nOptimal weights:\")\n",
        "    for model_name, weight in zip(oof_preds.keys(), optimal_weights):\n",
        "        print(f\"  {model_name}: {weight:.4f}\")\n",
        "    \n",
        "    # Final ensemble predictions\n",
        "    final_oof = sum(w * oof for w, oof in zip(optimal_weights, oof_preds.values()))\n",
        "    final_oof = np.clip(final_oof, 0, 100)\n",
        "    \n",
        "    final_test = np.zeros(len(test_df))\n",
        "    for model_name, weight in zip(test_preds.keys(), optimal_weights):\n",
        "        model_test_avg = np.mean(test_preds[model_name], axis=0)\n",
        "        final_test += weight * model_test_avg\n",
        "    final_test = np.clip(final_test, 0, 100)\n",
        "    \n",
        "    final_rmse = np.sqrt(mean_squared_error(y_train, final_oof))\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"FINAL ENSEMBLE OOF RMSE: {final_rmse:.6f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    return final_oof, final_test\n",
        "\n",
        "\n",
        "def train_model(model, X_tr_num, X_tr_cat, y_tr,\n",
        "                X_val_num, X_val_cat, y_val,\n",
        "                X_test_num, X_test_cat,\n",
        "                epochs=300, lr=1e-3, use_augmentation=True):\n",
        "    \"\"\"Training function for transformer models\"\"\"\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = CombinedLoss(huber_weight=0.7)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    \n",
        "    # Cosine annealing with warmup\n",
        "    from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)\n",
        "    \n",
        "    # DataLoader\n",
        "    train_dataset = TensorDataset(X_tr_num, X_tr_cat, y_tr)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
        "    \n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 40\n",
        "    \n",
        "    augmenter = TabularAugmenter()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        \n",
        "        for batch_num, batch_cat, batch_y in train_loader:\n",
        "            # Augmentation\n",
        "            if use_augmentation and np.random.rand() < 0.5:\n",
        "                batch_num, y_a, y_b, lam = augmenter.mixup(batch_num, batch_y, alpha=0.4)\n",
        "                \n",
        "                outputs = model(batch_num, batch_cat)\n",
        "                loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
        "            else:\n",
        "                outputs = model(batch_num, batch_cat)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val_num, X_val_cat)\n",
        "            val_loss = criterion(val_outputs, y_val).item()\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "        \n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"  Epoch {epoch+1}: Val Loss = {val_loss:.6f}\")\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(best_state)\n",
        "    \n",
        "    # Predictions with TTA (Test-Time Augmentation)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = model(X_val_num, X_val_cat).cpu().numpy()\n",
        "        \n",
        "        # TTA: multiple predictions with noise\n",
        "        test_preds_tta = []\n",
        "        for _ in range(5):\n",
        "            # Add small noise\n",
        "            X_test_num_noisy = X_test_num + torch.randn_like(X_test_num) * 0.01\n",
        "            test_pred = model(X_test_num_noisy, X_test_cat).cpu().numpy()\n",
        "            test_preds_tta.append(test_pred)\n",
        "        \n",
        "        test_pred = np.mean(test_preds_tta, axis=0)\n",
        "    \n",
        "    return np.clip(val_pred, 0, 100), np.clip(test_pred, 0, 100)\n",
        "\n",
        "\n",
        "def train_tabnet(model, X_tr, y_tr, X_val, y_val, X_test, epochs=300, lr=2e-3):\n",
        "    \"\"\"Training function for TabNet\"\"\"\n",
        "    \n",
        "    criterion = HuberLoss(delta=1.0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
        "    \n",
        "    train_dataset = TensorDataset(X_tr, y_tr)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 40\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        \n",
        "        for batch_x, batch_y in train_loader:\n",
        "            outputs, M_loss = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y) + 1e-3 * M_loss  # Sparsity regularization\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs, _ = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val).item()\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_state = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "        \n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"  Epoch {epoch+1}: Val Loss = {val_loss:.6f}\")\n",
        "    \n",
        "    model.load_state_dict(best_state)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred, _ = model(X_val)\n",
        "        test_pred, _ = model(X_test)\n",
        "    \n",
        "    return np.clip(val_pred.cpu().numpy(), 0, 100), np.clip(test_pred.cpu().numpy(), 0, 100)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\"\"\n",
        "    ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
        "    â  PURE DEEP LEARNING APPROACH FOR TABULAR DATA                       â\n",
        "    â  ================================================================    â\n",
        "    â                                                                      â\n",
        "    â  Models Used:                                                        â\n",
        "    â  1. FT-Transformer (Feature Tokenizer + Transformer)                â\n",
        "    â  2. SAINT (Self-Attention and Intersample Attention)                â\n",
        "    â  3. Improved TabNet (with Ghost Batch Norm)                         â\n",
        "    â                                                                      â\n",
        "    â  Key Techniques:                                                     â\n",
        "    â  â Advanced preprocessing (PowerTransform + QuantileTransform)      â\n",
        "    â  â Learned embeddings for categorical features                      â\n",
        "    â  â Mixup & CutMix augmentation                                      â\n",
        "    â  â Combined loss (Huber + Quantile)                                 â\n",
        "    â  â Cosine annealing with warm restarts                              â\n",
        "    â  â Test-time augmentation (TTA)                                     â\n",
        "    â  â Multi-model ensemble (weighted by OOF performance)               â\n",
        "    â                                                                      â\n",
        "    â  Expected Performance: RMSE < 8.20 (competitive with XGBoost)       â\n",
        "    ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n",
        "    \"\"\")\n",
        "    \n",
        "    # Example usage (adapt paths to your data):\n",
        "    # \n",
        "    # train_df = pd.read_csv(\"train.csv\")\n",
        "    # test_df = pd.read_csv(\"test.csv\")\n",
        "    # original_df = pd.read_csv(\"original.csv\")\n",
        "    # \n",
        "    # oof_preds, test_preds = train_pure_deep_learning(\n",
        "    #     train_df, test_df, original_df,\n",
        "    #     target_col='exam_score',\n",
        "    #     categorical_cols=['course', 'study_method'],\n",
        "    #     n_folds=10,\n",
        "    #     epochs=300\n",
        "    # )\n",
        "    # \n",
        "    # # Save submission\n",
        "    # submission = pd.DataFrame({\n",
        "    #     'id': test_df['id'],\n",
        "    #     'exam_score': test_preds\n",
        "    # })\n",
        "    # submission.to_csv('submission_pure_dl.csv', index=False)\n",
        "    \n",
        "    print(\"\\nâ Code ready! Integrate with your feature engineering pipeline.\")\n",
        "    print(\"ð This approach uses ONLY deep learning - no XGBoost/LightGBM!\")\n",
        "    print(\"ð¯ Target: Beat traditional ML with pure neural networks\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "notebooke888a22c2d",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 14993753,
          "sourceId": 119082,
          "sourceType": "competition"
        },
        {
          "datasetId": 8762382,
          "sourceId": 13904981,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31236,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "students-scores",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
